{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80399a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, pathlib, gc\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import os, nltk\n",
    "import os, nltk, re\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "DB_PATH = '../data/duckdb/subs.duckdb'\n",
    "TABLE   = 'ptbrvarid'   # raw goes here, as you requested\n",
    "BATCH   = 20_000\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "if NLTK_USER_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_USER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb72c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: journalistic\n",
      "  Processing split: train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/liaad/PtBrVId-Raw/resolve/8e3a5b5d34fe40f166cedeaa5a892368f3a3dcef/PtBrVId-Raw.py",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEntryNotFoundError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/load.py:982\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/hf_api.py:5518\u001b[39m, in \u001b[36mHfApi.hf_hub_download\u001b[39m\u001b[34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   5516\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.token\n\u001b[32m-> \u001b[39m\u001b[32m5518\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5521\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5530\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5534\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:1073\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1097\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:420\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    419\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mGatedRepo\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-69135c59-4617f52a62c370271661923a;267f89fa-e947-4f9f-b9c5-695c9b8ae232)\n\nEntry Not Found for url: https://huggingface.co/datasets/liaad/PtBrVId-Raw/resolve/8e3a5b5d34fe40f166cedeaa5a892368f3a3dcef/PtBrVId-Raw.py.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m                     con.unregister(\u001b[33m'\u001b[39m\u001b[33mbuf\u001b[39m\u001b[33m'\u001b[39m); n_new += \u001b[38;5;28mlen\u001b[39m(buf); buf.clear(); gc.collect()\n\u001b[32m     46\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_in\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mingest_ptbrvid_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mingest_ptbrvid_raw\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mingest_ptbrvid_raw\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_dataset_config_names\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mliaad/PtBrVId-Raw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing domain: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdomain\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mingest_ptbrvid_raw\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m get_dataset_split_names(\u001b[33m'\u001b[39m\u001b[33mliaad/PtBrVId-Raw\u001b[39m\u001b[33m'\u001b[39m, domain):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processing split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     ds  = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mliaad/PtBrVId-Raw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     buf = []; n_in = n_new = \u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m ds:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/load.py:1004\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    994\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    995\u001b[39m         use_exported_dataset_infos = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_exported_dataset_infos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1006\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is a gated dataset on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/load.py:608\u001b[39m, in \u001b[36mHubDatasetModuleFactory.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.USE_PARQUET_EXPORT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_exported_dataset_infos:\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m         exported_dataset_infos = \u001b[43m_dataset_viewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_exported_dataset_infos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    611\u001b[39m         exported_dataset_infos = DatasetInfosDict(\n\u001b[32m    612\u001b[39m             {\n\u001b[32m    613\u001b[39m                 config_name: DatasetInfo.from_dict(exported_dataset_infos[config_name])\n\u001b[32m    614\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m config_name \u001b[38;5;129;01min\u001b[39;00m exported_dataset_infos\n\u001b[32m    615\u001b[39m             }\n\u001b[32m    616\u001b[39m         )\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _dataset_viewer.DatasetViewerError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/utils/_dataset_viewer.py:71\u001b[39m, in \u001b[36mget_exported_dataset_infos\u001b[39m\u001b[34m(dataset, commit_hash, token)\u001b[39m\n\u001b[32m     69\u001b[39m dataset_viewer_info_url = config.HF_ENDPOINT.replace(\u001b[33m\"\u001b[39m\u001b[33m://\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m://datasets-server.\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m/info?dataset=\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     info_response = \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_viewer_info_url\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_authentication_headers_for_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_ENDPOINT\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatasets/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     info_response.raise_for_status()\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mX-Revision\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info_response.headers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, pathlib, gc\n",
    "\n",
    "DB_PATH = '../data/duckdb/subs.duckdb'\n",
    "TABLE   = 'ptbrvarid'   # raw goes here, as you requested\n",
    "BATCH   = 20_000\n",
    "\n",
    "with duckdb.connect(DB_PATH) as con:\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ptbrvarid (\n",
    "        dataset     TEXT,\n",
    "        domain      TEXT,\n",
    "        split       TEXT,\n",
    "        label       TEXT,           -- 'pt-BR' | 'pt-PT'\n",
    "        text_pt_br  TEXT,\n",
    "        text_pt_pt  TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    # keep only RAW rows scoped by dataset tag so we can also store processed later\n",
    "    con.execute(\"DELETE FROM ptbrvarid WHERE dataset='PtBrVId-Raw'\")\n",
    "\n",
    "def ingest_ptbrvid_raw():\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        for domain in get_dataset_config_names('liaad/PtBrVId-Raw'):\n",
    "            print(f\"Processing domain: {domain}\")\n",
    "            for split in get_dataset_split_names('liaad/PtBrVId-Raw', domain):\n",
    "                print(f\"  Processing split: {split}\")\n",
    "                ds  = load_dataset('liaad/PtBrVId-Raw', domain, split=split, streaming=True)\n",
    "                buf = []; n_in = n_new = 0\n",
    "                for ex in ds:\n",
    "                    lbl = 'pt-BR' if ex['label'] == 1 else 'pt-PT'\n",
    "                    br  = ex['text'] if lbl == 'pt-BR' else None\n",
    "                    pt  = ex['text'] if lbl == 'pt-PT' else None\n",
    "                    buf.append(('PtBrVId-Raw', domain, split, lbl, br, pt))\n",
    "                    n_in += 1\n",
    "                    if len(buf) >= BATCH:\n",
    "                        df = pd.DataFrame(buf, columns=['dataset','domain','split','label','text_pt_br','text_pt_pt'])\n",
    "                        con.register('buf', df)\n",
    "                        con.execute(\"INSERT INTO ptbrvarid SELECT * FROM buf\")\n",
    "                        con.unregister('buf'); n_new += len(buf); buf.clear(); gc.collect()\n",
    "                if buf:\n",
    "                    df = pd.DataFrame(buf, columns=['dataset','domain','split','label','text_pt_br','text_pt_pt'])\n",
    "                    con.register('buf', df)\n",
    "                    con.execute(\"INSERT INTO ptbrvarid SELECT * FROM buf\")\n",
    "                    con.unregister('buf'); n_new += len(buf); buf.clear(); gc.collect()\n",
    "                print(f\"âœ“ {domain}/{split}: read {n_in:,}, wrote {n_new:,}\")\n",
    "\n",
    "ingest_ptbrvid_raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b17d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_punkt():\n",
    "    \"\"\"\n",
    "    Ensure Portuguese punkt is available and visible to this kernel.\n",
    "    Newer NLTK may also require 'punkt_tab'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "    except LookupError:\n",
    "        # download into the same directory we added to nltk.data.path\n",
    "        nltk.download(\"punkt\", download_dir=NLTK_USER_DIR, quiet=True)\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt_tab\", download_dir=NLTK_USER_DIR, quiet=True)\n",
    "            nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "\n",
    "# ----------------------------\n",
    "# Author's regex & helpers (verbatim)\n",
    "# ----------------------------\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "\n",
    "HTML_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#â€¦])*\")\n",
    "HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
    "QUOTE_SPACE_START_RE = re.compile(r\"^\\\"\\s\")\n",
    "QUOTE_SPACE_END_RE = re.compile(r\"\\s\\\"$\")\n",
    "MENTION_RE = re.compile(r\"@(\\w+)\")\n",
    "RETWEET_RE = re.compile(r\"RT @(\\w+):\")\n",
    "COD_RE = re.compile(r\"COD _ (\\w+) \")\n",
    "BULLET_RE = re.compile(r\"^(\\d)+.\\s\")\n",
    "THREE_DASH_RE = re.compile(r\"---.*---\")\n",
    "MORE_THAN_THREE_POINTS_RE = re.compile(r\"\\.{4,}\")\n",
    "MODE = \"ptbrvarid\"   # options: \"ptbrvarid\" | \"ptradutor\"\n",
    "\n",
    "\n",
    "VALID_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzÃ Ã¡Ã¢Ã£Ã¥ÄÃ¨Ã©ÃªÃ«Ä›Ä—Ä“Ã®Ã¯Ã­Ã¬Ä¯Ä«ÄµÅ‚Ã±Å„Ã´Ã¶Ã²Ã³ÅÃµÅ¡Å›Ã»Ã¼Ã¹ÃºÅ«Ã¿Ã½ÅºÃ§Ä‡ÄÃ±Å„!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~Â«Â»â€œâ€ÂºÂªâ‚¬ \\t\\n\\r\\x0b\\x0c\"\n",
    "\n",
    "INVALID_START = [\n",
    "    \"List of recent changes\",\"Sort by\",\"Home |\",\"> Home\",\"useful tips\",\"Licenses:\",\"Search in: \",\n",
    "    \"Terms of Use - \",\"Home page\",\"Home Page\",\"Copyright\",\"Results/Page\",\n",
    "    \"!\",\"#\",\"$\",\"%\",\"&\",\"*\",\"+\",\n",
    "    \",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\"=\",\n",
    "    \">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\",\n",
    "]\n",
    "INVALID_MIDDLE = [\" @ \", \" / \", \" | \", \"[...]\", \"(...)\"]\n",
    "INVALID_END = [\" (\"]\n",
    "\n",
    "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "SPEAKER_LABEL_SINGLE_LETTER = re.compile(\n",
    "    r'(?m)(^|\\s|[(\\[\"â€œ])'          # \\1 = boundary we preserve\n",
    "    r'[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿]\\.'         # exactly one letter + dot\n",
    "    r'\\s*(?:--|[-â€“â€”]{1,2})\\s*'     # \"--\" or one/two dashes (hyphen/en/em), with spaces\n",
    ")\n",
    "\n",
    "# Word followed by double hyphen (speaker label), preserving the boundary before it\n",
    "SPEAKER_LABEL_WORD_DASH = re.compile(\n",
    "    r'(?m)(^|\\s)[A-Za-zÃ€-Ã–Ã˜-Ã¶Ã¸-Ã¿]+(?:\\.)?\\s+--\\s*'\n",
    ")\n",
    "\n",
    "# Dialogue-leading double dash right after strong punctuation or line start\n",
    "DIALOGUE_LEADING_DASH = re.compile(\n",
    "    r'(?m)(^|[\\.!\\?\\:\\;â€¦])\\s*--\\s*'\n",
    ")\n",
    "\n",
    "# Capitalize the first letter after strong punctuation or at start of text/line.\n",
    "# Keeps any opening quotes/brackets just before the letter.\n",
    "CAP_SENT_START = re.compile(\n",
    "    r'(?m)(^|[\\.!\\?\\:\\;â€¦]\\s+)([\\\"\\'â€œâ€Â«Â»\\(\\[\\{]*)([a-zÃ -Ã¶Ã¸-Ã¿])'\n",
    ")\n",
    "\n",
    "\n",
    "def remove_html_tags(text): return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "def remove_hashtags(text): return HASHTAG_RE.sub(\"\", text).strip()\n",
    "def remove_mentions(text): return MENTION_RE.sub(\"\", text).strip()\n",
    "def remove_retweets(text): return RETWEET_RE.sub(\"\", text).strip()\n",
    "def remove_urls(text): return URL_RE.sub(\"\", text).strip()\n",
    "def remove_cod_literature(text): return COD_RE.sub(\"\", text).strip()\n",
    "def remove_bullets(text): return BULLET_RE.sub(\"\", text).strip()\n",
    "def remove_three_dashes(text): return THREE_DASH_RE.sub(\"\", text).strip()\n",
    "def remove_quote_space_start(text): return QUOTE_SPACE_START_RE.sub('\"', text)\n",
    "def remove_quote_space_end(text):\n",
    "    if text.endswith(' \"'): return text[:-2] + '\"'\n",
    "    return text\n",
    "def has_more_than_three_points(text): return bool(MORE_THAN_THREE_POINTS_RE.search(text))\n",
    "def starts_with_month(text): return text.lower().startswith(tuple(MONTHS))\n",
    "def has_too_long_word(text): return any(word for word in text.split(\" \") if len(word) > 20)\n",
    "def has_invalid_start(text): return text.startswith(tuple(INVALID_START))\n",
    "def has_invalid_middle(text): return any(True for word in INVALID_MIDDLE if word in text)\n",
    "def has_invalid_end(text): return text.endswith(tuple(INVALID_END))\n",
    "def has_valid_brackets(text): return (text.count(\"(\")==text.count(\")\") and text.count(\"[\")==text.count(\"]\") and text.count(\"{\")==text.count(\"}\"))\n",
    "def has_valid_quotes(text): return text.count('\"') % 2 == 0 and text.count(\"â€œ\")==text.count(\"â€\")\n",
    "def is_empty(text): return len(text) == 0\n",
    "def has_invalid_character(text):\n",
    "    for char in text:\n",
    "        if char.lower() not in VALID_CHARS: return True\n",
    "    return False\n",
    "\n",
    "def normalize_double_quotes(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return (text.replace(\"Â«\", '\"').replace(\"Â»\", '\"')\n",
    "                .replace(\"â€œ\", '\"').replace(\"â€\", '\"')\n",
    "                .replace(\"â€ž\", '\"'))\n",
    "\n",
    "def remove_single_letter_speaker_labels(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return SPEAKER_LABEL_SINGLE_LETTER.sub(r\"\\1\", text)\n",
    "\n",
    "def remove_speaker_label_word_dash(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return SPEAKER_LABEL_WORD_DASH.sub(r'\\1', text)\n",
    "\n",
    "def remove_dialogue_leading_double_dash(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return DIALOGUE_LEADING_DASH.sub(lambda m: (m.group(1) or '') + ' ', text)\n",
    "\n",
    "def capitalize_sentence_starts(text: str) -> str:\n",
    "    if not text: return text\n",
    "    m = re.match(r'^([\\\"\\'â€œâ€Â«Â»\\(\\[\\{]*)([a-zÃ -Ã¶Ã¸-Ã¿])', text)\n",
    "    if m: text = m.group(1) + m.group(2).upper() + text[m.end():]\n",
    "    def repl(m): return (m.group(1) or '') + (m.group(2) or '') + m.group(3).upper()\n",
    "    return CAP_SENT_START.sub(repl, text)\n",
    "\n",
    "def author_transform_chain(text: str) -> str:\n",
    "    text = remove_retweets(text); text = remove_mentions(text); text = remove_hashtags(text)\n",
    "    text = remove_urls(text); text = remove_html_tags(text); text = normalize_double_quotes(text)\n",
    "    text = remove_single_letter_speaker_labels(text); text = remove_speaker_label_word_dash(text)\n",
    "    text = remove_dialogue_leading_double_dash(text); text = remove_cod_literature(text)\n",
    "    text = remove_bullets(text); text = remove_three_dashes(text)\n",
    "    text = remove_quote_space_start(text); text = remove_quote_space_end(text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    text = capitalize_sentence_starts(text)\n",
    "    return text\n",
    "\n",
    "def drop_nans_and_empties(ds):\n",
    "    return ds.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "def drop_exact_duplicates(ds, batch_size: int = 1000):\n",
    "    seen = set()\n",
    "    def tag_batch(batch):\n",
    "        keep = []\n",
    "        for t in batch[\"text\"]:\n",
    "            h = hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "            keep.append(h not in seen)\n",
    "            seen.add(h)\n",
    "        return {\"__keep__\": keep}\n",
    "    ds = ds.map(tag_batch, batched=True, batch_size=batch_size, num_proc=1)\n",
    "    ds = ds.filter(lambda k: k, input_columns=\"__keep__\").remove_columns([\"__keep__\"])\n",
    "    return ds\n",
    "\n",
    "def apply_clean_text_ascii(s: str) -> str:\n",
    "    return clean(s, fix_unicode=True, to_ascii=True, lower=False,\n",
    "                 no_line_breaks=False, no_urls=False, no_emails=False,\n",
    "                 no_phone_numbers=False, no_numbers=False, no_digits=False, no_currency_symbols=False)\n",
    "\n",
    "_FALLBACK_TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def pt_word_count(s: str) -> int:\n",
    "    try:\n",
    "        return len(nltk.word_tokenize(s, language=\"portuguese\"))\n",
    "    except LookupError:\n",
    "        return len(_FALLBACK_TOKEN_RE.findall(s))\n",
    "\n",
    "def add_length_column(ds, batch_size: int = 1000):\n",
    "    def _lens(batch): return {\"__len__\": [pt_word_count(t) for t in batch[\"text\"]]}\n",
    "    return ds.map(_lens, batched=True, batch_size=batch_size, num_proc=1)\n",
    "\n",
    "def iqr_bounds_from_lengths(lengths):\n",
    "    q1, q3 = np.percentile(lengths, 25), np.percentile(lengths, 75)\n",
    "    iqr = q3 - q1\n",
    "    return (q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "\n",
    "def apply_iqr_filter_on_cached_lengths(ds, lo, hi):\n",
    "    ds = ds.filter(lambda L: lo <= L <= hi, input_columns=\"__len__\")\n",
    "    return ds.remove_columns([\"__len__\"])\n",
    "\n",
    "def apply_clean_text_unicode_only(s: str) -> str:\n",
    "    return clean(s, fix_unicode=True, to_ascii=False, lower=False,\n",
    "                 no_line_breaks=False, no_urls=False, no_emails=False,\n",
    "                 no_phone_numbers=False, no_numbers=False, no_digits=False, no_currency_symbols=False)\n",
    "\n",
    "# ====== NEW: safe jusText wrapper + scope switch ======\n",
    "def safe_justext(text: str) -> str:\n",
    "    if not text: return text\n",
    "    try:\n",
    "        import justext\n",
    "        paras = justext.justext(text, justext.get_stoplist(\"Portuguese\"))\n",
    "        good = [p.text for p in paras if p.class_type == \"good\"]\n",
    "        return \"\\n\".join(good) if good else text\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def clean_one_domain_split(\n",
    "    domain: str,\n",
    "    split: str,\n",
    "    use_author_transforms: bool = True,\n",
    "    run_web_justext: bool = True,   # kept for back-compat (ignored if justext_scope is set)\n",
    "    apply_author_filters: bool = True,\n",
    "    keep_accents: bool = True,\n",
    "    num_proc: int = 1,\n",
    "    batch_size: int = 1000,\n",
    "    *,\n",
    "    justext_scope: str = \"web\",     # NEW: \"none\" | \"web\" | \"all\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Same as yours, but jusText can run on: none / web / all.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"liaad/PtBrVId-Raw\", domain, split=split)\n",
    "    ds = drop_nans_and_empties(ds)\n",
    "\n",
    "    # --- jusText control (NEW) ---\n",
    "    scope = justext_scope or (\"web\" if run_web_justext else \"none\")\n",
    "    if scope == \"all\" or (scope == \"web\" and domain == \"web\"):\n",
    "        ds = ds.map(lambda x: {\"text\": web_justext(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    # (rest of your function unchanged belowâ€¦)\n",
    "    if use_author_transforms:\n",
    "        ds = ds.map(lambda x: {\"text\": author_transform_chain(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    if keep_accents:\n",
    "        ds = ds.map(lambda x: {\"text\": apply_clean_text_unicode_only(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "    else:\n",
    "        ds = ds.map(lambda x: {\"text\": apply_clean_text_ascii(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    ds = drop_exact_duplicates(ds, batch_size=batch_size)\n",
    "\n",
    "    if apply_author_filters:\n",
    "        ds = ds.filter(\n",
    "            lambda t: (not starts_with_month(t))\n",
    "                      and (not has_too_long_word(t))\n",
    "                      and (not has_invalid_start(t))\n",
    "                      and (not has_invalid_middle(t))\n",
    "                      and (not has_invalid_end(t))\n",
    "                      and (not has_more_than_three_points(t))\n",
    "                      and (not is_empty(t))\n",
    "                      and (not has_invalid_character(t))\n",
    "                      and has_valid_brackets(t)\n",
    "                      and has_valid_quotes(t),\n",
    "            input_columns=\"text\",\n",
    "            num_proc=num_proc\n",
    "        )\n",
    "\n",
    "    ds = add_length_column(ds, batch_size=batch_size)\n",
    "    lo, hi = iqr_bounds_from_lengths(ds[\"__len__\"])\n",
    "    ds = apply_iqr_filter_on_cached_lengths(ds, lo, hi)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8db68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9192200f8f4b43a0951286d1c0f946ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journalistic/train: raw=1,842,804 â†’ after_IQR=1,574,068 (85.42%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bc9c394eec4b9aad228e953f1d3ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legal/train: raw=4,302,003 â†’ after_IQR=1,118,443 (26.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e768466ab4eef9c23d44ba3276853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literature/train: raw=81,984 â†’ after_IQR=39,382 (48.04%)\n",
      "politics/train: raw=34,605 â†’ after_IQR=22,831 (65.98%)\n",
      "social_media/train: raw=2,678,580 â†’ after_IQR=219,290 (8.19%)\n",
      "web/train: raw=133,664 â†’ after_IQR=17,714 (13.25%)\n",
      "[DONE] Inserted processed rows with your pipeline + jusText (dataset='PtBrVId'): 0\n",
      "\n",
      "=== Per-stage counts (subset) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>split</th>\n",
       "      <th>after_nonempty</th>\n",
       "      <th>after_jusText</th>\n",
       "      <th>after_author</th>\n",
       "      <th>after_clean</th>\n",
       "      <th>after_dedup</th>\n",
       "      <th>after_filters</th>\n",
       "      <th>after_IQR</th>\n",
       "      <th>drop_empty</th>\n",
       "      <th>drop_jusText</th>\n",
       "      <th>drop_author</th>\n",
       "      <th>drop_clean</th>\n",
       "      <th>drop_dedup</th>\n",
       "      <th>drop_filters</th>\n",
       "      <th>drop_IQR</th>\n",
       "      <th>IQR_lo</th>\n",
       "      <th>IQR_hi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>1842804</td>\n",
       "      <td>1717167</td>\n",
       "      <td>1717167</td>\n",
       "      <td>1717167</td>\n",
       "      <td>1717126</td>\n",
       "      <td>1607201</td>\n",
       "      <td>1574068</td>\n",
       "      <td>0</td>\n",
       "      <td>125637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>109925</td>\n",
       "      <td>33133</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>298.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legal</td>\n",
       "      <td>train</td>\n",
       "      <td>4302002</td>\n",
       "      <td>2110995</td>\n",
       "      <td>2110995</td>\n",
       "      <td>2110995</td>\n",
       "      <td>2099143</td>\n",
       "      <td>1189672</td>\n",
       "      <td>1118443</td>\n",
       "      <td>1</td>\n",
       "      <td>2191007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11852</td>\n",
       "      <td>909471</td>\n",
       "      <td>71229</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>122.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>literature</td>\n",
       "      <td>train</td>\n",
       "      <td>81984</td>\n",
       "      <td>59513</td>\n",
       "      <td>59513</td>\n",
       "      <td>59513</td>\n",
       "      <td>59513</td>\n",
       "      <td>40775</td>\n",
       "      <td>39382</td>\n",
       "      <td>0</td>\n",
       "      <td>22471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18738</td>\n",
       "      <td>1393</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>177.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>train</td>\n",
       "      <td>34604</td>\n",
       "      <td>32427</td>\n",
       "      <td>32427</td>\n",
       "      <td>32427</td>\n",
       "      <td>32427</td>\n",
       "      <td>24425</td>\n",
       "      <td>22831</td>\n",
       "      <td>1</td>\n",
       "      <td>2177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8002</td>\n",
       "      <td>1594</td>\n",
       "      <td>-245.0</td>\n",
       "      <td>747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social_media</td>\n",
       "      <td>train</td>\n",
       "      <td>2678579</td>\n",
       "      <td>266648</td>\n",
       "      <td>266645</td>\n",
       "      <td>266645</td>\n",
       "      <td>262629</td>\n",
       "      <td>220895</td>\n",
       "      <td>219290</td>\n",
       "      <td>1</td>\n",
       "      <td>2411931</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4016</td>\n",
       "      <td>41734</td>\n",
       "      <td>1605</td>\n",
       "      <td>22.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>web</td>\n",
       "      <td>train</td>\n",
       "      <td>133664</td>\n",
       "      <td>101401</td>\n",
       "      <td>101401</td>\n",
       "      <td>101401</td>\n",
       "      <td>101367</td>\n",
       "      <td>18949</td>\n",
       "      <td>17714</td>\n",
       "      <td>0</td>\n",
       "      <td>32263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>82418</td>\n",
       "      <td>1235</td>\n",
       "      <td>-189.5</td>\n",
       "      <td>502.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         domain  split  after_nonempty  after_jusText  after_author  \\\n",
       "0  journalistic  train         1842804        1717167       1717167   \n",
       "1         legal  train         4302002        2110995       2110995   \n",
       "2    literature  train           81984          59513         59513   \n",
       "3      politics  train           34604          32427         32427   \n",
       "4  social_media  train         2678579         266648        266645   \n",
       "5           web  train          133664         101401        101401   \n",
       "\n",
       "   after_clean  after_dedup  after_filters  after_IQR  drop_empty  \\\n",
       "0      1717167      1717126        1607201    1574068           0   \n",
       "1      2110995      2099143        1189672    1118443           1   \n",
       "2        59513        59513          40775      39382           0   \n",
       "3        32427        32427          24425      22831           1   \n",
       "4       266645       262629         220895     219290           1   \n",
       "5       101401       101367          18949      17714           0   \n",
       "\n",
       "   drop_jusText  drop_author  drop_clean  drop_dedup  drop_filters  drop_IQR  \\\n",
       "0        125637            0           0          41        109925     33133   \n",
       "1       2191007            0           0       11852        909471     71229   \n",
       "2         22471            0           0           0         18738      1393   \n",
       "3          2177            0           0           0          8002      1594   \n",
       "4       2411931            3           0        4016         41734      1605   \n",
       "5         32263            0           0          34         82418      1235   \n",
       "\n",
       "   IQR_lo  IQR_hi  \n",
       "0   -54.0   298.0  \n",
       "1    -1.5   122.5  \n",
       "2   -10.5   177.5  \n",
       "3  -245.0   747.0  \n",
       "4    22.0    70.0  \n",
       "5  -189.5   502.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Domain totals ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>raw</th>\n",
       "      <th>after_nonempty</th>\n",
       "      <th>after_jusText</th>\n",
       "      <th>after_author</th>\n",
       "      <th>after_clean</th>\n",
       "      <th>after_dedup</th>\n",
       "      <th>after_filters</th>\n",
       "      <th>after_IQR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legal</td>\n",
       "      <td>4302003.0</td>\n",
       "      <td>4302002.0</td>\n",
       "      <td>2110995.0</td>\n",
       "      <td>2110995.0</td>\n",
       "      <td>2110995.0</td>\n",
       "      <td>2099143.0</td>\n",
       "      <td>1189672.0</td>\n",
       "      <td>1118443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>2678580.0</td>\n",
       "      <td>2678579.0</td>\n",
       "      <td>266648.0</td>\n",
       "      <td>266645.0</td>\n",
       "      <td>266645.0</td>\n",
       "      <td>262629.0</td>\n",
       "      <td>220895.0</td>\n",
       "      <td>219290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>1842804.0</td>\n",
       "      <td>1842804.0</td>\n",
       "      <td>1717167.0</td>\n",
       "      <td>1717167.0</td>\n",
       "      <td>1717167.0</td>\n",
       "      <td>1717126.0</td>\n",
       "      <td>1607201.0</td>\n",
       "      <td>1574068.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>web</td>\n",
       "      <td>133664.0</td>\n",
       "      <td>133664.0</td>\n",
       "      <td>101401.0</td>\n",
       "      <td>101401.0</td>\n",
       "      <td>101401.0</td>\n",
       "      <td>101367.0</td>\n",
       "      <td>18949.0</td>\n",
       "      <td>17714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>literature</td>\n",
       "      <td>81984.0</td>\n",
       "      <td>81984.0</td>\n",
       "      <td>59513.0</td>\n",
       "      <td>59513.0</td>\n",
       "      <td>59513.0</td>\n",
       "      <td>59513.0</td>\n",
       "      <td>40775.0</td>\n",
       "      <td>39382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>34605.0</td>\n",
       "      <td>34604.0</td>\n",
       "      <td>32427.0</td>\n",
       "      <td>32427.0</td>\n",
       "      <td>32427.0</td>\n",
       "      <td>32427.0</td>\n",
       "      <td>24425.0</td>\n",
       "      <td>22831.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         domain        raw  after_nonempty  after_jusText  after_author  \\\n",
       "0         legal  4302003.0       4302002.0      2110995.0     2110995.0   \n",
       "1  social_media  2678580.0       2678579.0       266648.0      266645.0   \n",
       "2  journalistic  1842804.0       1842804.0      1717167.0     1717167.0   \n",
       "3           web   133664.0        133664.0       101401.0      101401.0   \n",
       "4    literature    81984.0         81984.0        59513.0       59513.0   \n",
       "5      politics    34605.0         34604.0        32427.0       32427.0   \n",
       "\n",
       "   after_clean  after_dedup  after_filters  after_IQR  \n",
       "0    2110995.0    2099143.0      1189672.0  1118443.0  \n",
       "1     266645.0     262629.0       220895.0   219290.0  \n",
       "2    1717167.0    1717126.0      1607201.0  1574068.0  \n",
       "3     101401.0     101367.0        18949.0    17714.0  \n",
       "4      59513.0      59513.0        40775.0    39382.0  \n",
       "5      32427.0      32427.0        24425.0    22831.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 2 + METRICS (single pass per domain/split with on-disk staging)\n",
    "# - Applies jusText on ALL domains (and really drops boilerplate)\n",
    "# - Writes final rows into ptbrvarid (dataset='PtBrVId')\n",
    "# - Writes per-stage counts into ptbrvarid_metrics\n",
    "# =============================\n",
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, numpy as np, hashlib, warnings, re\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "BATCH_ROWS = 20_000\n",
    "\n",
    "# -------- jusText that actually prunes --------\n",
    "def web_justext(text: str,\n",
    "                *,\n",
    "                drop_if_no_good: bool = True,\n",
    "                min_chars: int = 30,\n",
    "                min_ratio: float = 0.20) -> str:\n",
    "    \"\"\"\n",
    "    Run jusText and drop rows that look like boilerplate.\n",
    "    - drop_if_no_good: drop if no 'good' paragraphs\n",
    "    - min_chars: drop if cleaned text shorter than this\n",
    "    - min_ratio: drop if cleaned/original length ratio below this\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    try:\n",
    "        import justext\n",
    "        paras = justext.justext(text, justext.get_stoplist(\"Portuguese\"))\n",
    "        good  = [p.text for p in paras if p.class_type == \"good\"]\n",
    "        if not good:\n",
    "            return \"\" if drop_if_no_good else text\n",
    "        jt = \"\\n\".join(good).strip()\n",
    "        if len(jt) < min_chars:\n",
    "            return \"\"\n",
    "        if len(jt) / max(1, len(text)) < min_ratio:\n",
    "            return \"\"\n",
    "        return jt\n",
    "    except Exception:\n",
    "        return text  # conservative on parser error\n",
    "\n",
    "# -------- tiny helper: your heuristic predicate --------\n",
    "def _pass_filters(t: str) -> bool:\n",
    "    return (not starts_with_month(t)\n",
    "            and not has_too_long_word(t)\n",
    "            and not has_invalid_start(t)\n",
    "            and not has_invalid_middle(t)\n",
    "            and not has_invalid_end(t)\n",
    "            and not has_more_than_three_points(t)\n",
    "            and not is_empty(t)\n",
    "            and not has_invalid_character(t)\n",
    "            and has_valid_brackets(t)\n",
    "            and has_valid_quotes(t))\n",
    "\n",
    "# -------- DB setup --------\n",
    "with duckdb.connect(DB_PATH) as con:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ptbrvarid (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            label       TEXT,\n",
    "            text_pt_br  TEXT,\n",
    "            text_pt_pt  TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ptbrvarid_metrics (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            raw                 BIGINT,\n",
    "            after_nonempty      BIGINT,\n",
    "            after_jusText       BIGINT,\n",
    "            after_author        BIGINT,\n",
    "            after_clean         BIGINT,\n",
    "            after_dedup         BIGINT,\n",
    "            after_filters       BIGINT,\n",
    "            after_IQR           BIGINT,\n",
    "            drop_empty          BIGINT,\n",
    "            drop_jusText        BIGINT,\n",
    "            drop_author         BIGINT,\n",
    "            drop_clean          BIGINT,\n",
    "            drop_dedup          BIGINT,\n",
    "            drop_filters        BIGINT,\n",
    "            drop_IQR            BIGINT,\n",
    "            IQR_lo              DOUBLE,\n",
    "            IQR_hi              DOUBLE\n",
    "        )\n",
    "    \"\"\")\n",
    "    # Clear previous processed rows + metrics for a clean rebuild\n",
    "    con.execute(\"DELETE FROM ptbrvarid WHERE dataset='PtBrVId'\")\n",
    "    con.execute(\"DELETE FROM ptbrvarid_metrics WHERE dataset='PtBrVId'\")\n",
    "\n",
    "def _safe_tbl(name: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_]\", \"_\", name)\n",
    "\n",
    "def process_domain_split(domain: str, split: str,\n",
    "                         *, keep_accents: bool = True,\n",
    "                         use_author_transforms: bool = True,\n",
    "                         apply_author_filters: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Single (domain, split) processing:\n",
    "      Pass 1: stream RAW -> jusText -> author -> clean (accents kept) -> de-dup -> filters\n",
    "              write (label, cleaned_text, len_tokens) to stage table on disk\n",
    "              collect per-stage counters\n",
    "      Compute IQR from stage lengths\n",
    "      Insert survivors into ptbrvarid\n",
    "      Save metrics, drop stage table\n",
    "    \"\"\"\n",
    "    stage = f\"__ptbr_stage_{_safe_tbl(domain)}_{_safe_tbl(split)}\"\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        con.execute(f\"DROP TABLE IF EXISTS {stage}\")\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE {stage} (\n",
    "                domain TEXT,\n",
    "                split  TEXT,\n",
    "                label  TEXT,\n",
    "                text   TEXT,\n",
    "                len_tokens BIGINT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    raw_n = n_nonempty = n_after_jt = n_after_author = n_after_clean = n_after_dedup = n_after_filters = 0\n",
    "    seen, buf = set(), []\n",
    "\n",
    "    # ---------- PASS 1: build stage ----------\n",
    "    ds = load_dataset(\"liaad/PtBrVId-Raw\", domain, split=split, streaming=True)\n",
    "    for ex in ds:\n",
    "        raw_n += 1\n",
    "        t = (ex[\"text\"] or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        n_nonempty += 1\n",
    "\n",
    "        # jusText on ALL domains; now actually drops boilerplate\n",
    "        t = web_justext(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        n_after_jt += 1\n",
    "\n",
    "        if use_author_transforms:\n",
    "            t = author_transform_chain(t)\n",
    "            if not t:\n",
    "                continue\n",
    "        n_after_author += 1\n",
    "\n",
    "        t = apply_clean_text_unicode_only(t) if keep_accents else apply_clean_text_ascii(t)\n",
    "        t = (t or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        n_after_clean += 1\n",
    "\n",
    "        h = hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        n_after_dedup += 1\n",
    "\n",
    "        if apply_author_filters and (not _pass_filters(t)):\n",
    "            continue\n",
    "        n_after_filters += 1\n",
    "\n",
    "        L = pt_word_count(t)\n",
    "        lbl = 'pt-BR' if ex['label'] == 1 else 'pt-PT'\n",
    "        buf.append((domain, split, lbl, t, int(L)))\n",
    "\n",
    "        if len(buf) >= BATCH_ROWS:\n",
    "            with duckdb.connect(DB_PATH) as con:\n",
    "                con.executemany(f\"INSERT INTO {stage} VALUES (?,?,?,?,?)\", buf)\n",
    "            buf.clear()\n",
    "    if buf:\n",
    "        with duckdb.connect(DB_PATH) as con:\n",
    "            con.executemany(f\"INSERT INTO {stage} VALUES (?,?,?,?,?)\", buf)\n",
    "        buf.clear()\n",
    "\n",
    "    # If nothing survived pre-IQR, write metrics and return\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        n_stage = con.execute(f\"SELECT COUNT(*) FROM {stage}\").fetchone()[0]\n",
    "        if n_stage == 0:\n",
    "            metrics_row = ('PtBrVId', domain, split,\n",
    "                           raw_n, n_nonempty, n_after_jt, n_after_author, n_after_clean,\n",
    "                           n_after_dedup, n_after_filters, 0,\n",
    "                           raw_n-n_nonempty, n_nonempty-n_after_jt, n_after_jt-n_after_author,\n",
    "                           n_after_author-n_after_clean, n_after_clean-n_after_dedup,\n",
    "                           n_after_dedup-n_after_filters, n_after_filters-0,\n",
    "                           0.0, 0.0)\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO ptbrvarid_metrics (\n",
    "                    dataset, domain, split,\n",
    "                    raw, after_nonempty, after_jusText, after_author, after_clean,\n",
    "                    after_dedup, after_filters, after_IQR,\n",
    "                    drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "                    IQR_lo, IQR_hi\n",
    "                ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\", metrics_row)\n",
    "            con.execute(f\"DROP TABLE {stage}\")\n",
    "            print(f\"{domain}/{split}: raw={raw_n:,} â†’ after_IQR=0 (0.0%)\")\n",
    "            return\n",
    "\n",
    "        # Compute IQR bounds on lengths\n",
    "        q1, q3 = con.execute(\n",
    "            f\"SELECT quantile_cont(len_tokens,0.25), quantile_cont(len_tokens,0.75) FROM {stage}\"\n",
    "        ).fetchone()\n",
    "        iqr = q3 - q1\n",
    "        lo = float(q1 - 1.5 * iqr)\n",
    "        hi = float(q3 + 1.5 * iqr)\n",
    "\n",
    "        # Survivors count\n",
    "        n_after_iqr = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM {stage} WHERE len_tokens BETWEEN ? AND ?\", [lo, hi]\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        # Insert final rows into ptbrvarid\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO ptbrvarid\n",
    "            SELECT\n",
    "              'PtBrVId' AS dataset,\n",
    "              domain, split, label,\n",
    "              CASE WHEN label='pt-BR' THEN text ELSE NULL END AS text_pt_br,\n",
    "              CASE WHEN label='pt-PT' THEN text ELSE NULL END AS text_pt_pt\n",
    "            FROM {stage}\n",
    "            WHERE len_tokens BETWEEN ? AND ?\n",
    "        \"\"\", [lo, hi])\n",
    "\n",
    "        # Metrics row (FIXED: 20 placeholders + explicit columns)\n",
    "        metrics_row = ('PtBrVId', domain, split,\n",
    "                       raw_n, n_nonempty, n_after_jt, n_after_author, n_after_clean,\n",
    "                       n_after_dedup, n_after_filters, int(n_after_iqr),\n",
    "                       raw_n-n_nonempty, n_nonempty-n_after_jt, n_after_jt-n_after_author,\n",
    "                       n_after_author-n_after_clean, n_after_clean-n_after_dedup,\n",
    "                       n_after_dedup-n_after_filters, n_after_filters-int(n_after_iqr),\n",
    "                       lo, hi)\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO ptbrvarid_metrics (\n",
    "                dataset, domain, split,\n",
    "                raw, after_nonempty, after_jusText, after_author, after_clean,\n",
    "                after_dedup, after_filters, after_IQR,\n",
    "                drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "                IQR_lo, IQR_hi\n",
    "            ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "        \"\"\", metrics_row)\n",
    "\n",
    "        # Drop stage\n",
    "        con.execute(f\"DROP TABLE {stage}\")\n",
    "\n",
    "        pct = round(100.0 * (n_after_iqr / max(1, raw_n)), 2)\n",
    "        print(f\"{domain}/{split}: raw={raw_n:,} â†’ after_IQR={n_after_iqr:,} ({pct}%)\")\n",
    "\n",
    "\n",
    "# -------- Run all (domains/splits discovered from -Raw) --------\n",
    "domains = get_dataset_config_names(\"liaad/PtBrVId-Raw\")\n",
    "total_final = 0\n",
    "for d in domains:\n",
    "    splits = get_dataset_split_names(\"liaad/PtBrVId-Raw\", d)\n",
    "    for s in splits:\n",
    "        process_domain_split(d, s, keep_accents=True, use_author_transforms=True, apply_author_filters=True)\n",
    "        # add to running total\n",
    "        with duckdb.connect(DB_PATH) as con:\n",
    "            n = con.execute(\"\"\"\n",
    "                SELECT COUNT(*) FROM ptbrvarid\n",
    "                WHERE dataset='PtBrVId' AND domain=? AND split=?\n",
    "            \"\"\", [d, s]).fetchone()[0]\n",
    "            total_final += int(n)\n",
    "\n",
    "print(f\"[DONE] Inserted processed rows with your pipeline + jusText (dataset='PtBrVId'): {total_final:,}\")\n",
    "\n",
    "# -------- Nice summaries you can query later --------\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    by_dom = con.execute(\"\"\"\n",
    "        SELECT domain, split, after_nonempty, after_jusText, after_author,\n",
    "               after_clean, after_dedup, after_filters, after_IQR,\n",
    "               drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "               IQR_lo, IQR_hi\n",
    "        FROM ptbrvarid_metrics\n",
    "        WHERE dataset='PtBrVId'\n",
    "        ORDER BY domain, split\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"\\n=== Per-stage counts (subset) ===\")\n",
    "    display(by_dom.head(20))\n",
    "\n",
    "    totals = con.execute(\"\"\"\n",
    "        SELECT domain,\n",
    "               SUM(raw)              AS raw,\n",
    "               SUM(after_nonempty)   AS after_nonempty,\n",
    "               SUM(after_jusText)    AS after_jusText,\n",
    "               SUM(after_author)     AS after_author,\n",
    "               SUM(after_clean)      AS after_clean,\n",
    "               SUM(after_dedup)      AS after_dedup,\n",
    "               SUM(after_filters)    AS after_filters,\n",
    "               SUM(after_IQR)        AS after_IQR\n",
    "        FROM ptbrvarid_metrics\n",
    "        WHERE dataset='PtBrVId'\n",
    "        GROUP BY domain\n",
    "        ORDER BY raw DESC\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"\\n=== Domain totals ===\")\n",
    "    display(totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1f4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "\n",
    "def _fmt_millions(x, pos):\n",
    "    return f\"{x/1_000_000:.1f}M\" if x >= 1_000_000 else f\"{int(x):,}\"\n",
    "\n",
    "# ---------- aggregate totals ----------\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    tot = con.execute(\"\"\"\n",
    "      SELECT\n",
    "        SUM(raw)              AS raw,\n",
    "        SUM(after_nonempty)   AS after_nonempty,\n",
    "        SUM(after_jusText)    AS after_jusText,\n",
    "        SUM(after_dedup)      AS after_dedup,\n",
    "        SUM(after_filters)    AS after_filters,\n",
    "        SUM(after_IQR)        AS after_IQR,\n",
    "        SUM(drop_empty)       AS drop_empty,\n",
    "        SUM(drop_jusText)     AS drop_jusText,\n",
    "        SUM(drop_dedup)       AS drop_dedup,\n",
    "        SUM(drop_filters)     AS drop_filters,\n",
    "        SUM(drop_IQR)         AS drop_IQR\n",
    "      FROM ptbrvarid_metrics\n",
    "      WHERE dataset = 'PtBrVId'\n",
    "    \"\"\").fetchdf().iloc[0]\n",
    "\n",
    "# ============================================================\n",
    "# 1) Survivors after each step\n",
    "# ============================================================\n",
    "\n",
    "survivors = pd.Series({\n",
    "    \"Raw\":           int(tot[\"raw\"]),\n",
    "    \"After jusText\": int(tot[\"after_jusText\"]),\n",
    "    \"After De-dup\":  int(tot[\"after_dedup\"]),\n",
    "    \"After Filters\": int(tot[\"after_filters\"]),\n",
    "    \"After IQR\":     int(tot[\"after_IQR\"]),\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(survivors.index, survivors.values)\n",
    "\n",
    "ax.set_ylabel(\"Documents\", fontsize=16)\n",
    "ax.set_title(\" \", fontsize=16)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = survivors.values.max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b in bars:\n",
    "    v = b.get_height()\n",
    "    ax.annotate(\n",
    "        _fmt_millions(v, None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, v),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=14)\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 2) Deletions by step\n",
    "# ============================================================\n",
    "\n",
    "drops = pd.Series({\n",
    "    \"Empty\":    int(tot[\"drop_empty\"]),\n",
    "    \"jusText\":  int(tot[\"drop_jusText\"]),\n",
    "    \"De-dup\":   int(tot[\"drop_dedup\"]),\n",
    "    \"Filters\":  int(tot[\"drop_filters\"]),\n",
    "    \"IQR\":      int(tot[\"drop_IQR\"]),\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(drops.index, drops.values)\n",
    "\n",
    "ax.set_ylabel(\"Documents removed\", fontsize=25)\n",
    "ax.set_title(\" \", fontsize=25)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = drops.values.max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b in bars:\n",
    "    v = b.get_height()\n",
    "    ax.annotate(\n",
    "        _fmt_millions(v, None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, v),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=25)\n",
    "ax.tick_params(axis=\"y\", labelsize=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"removals_by_step_ptbrvarid.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 3) Per-domain view of jusTextâ€™s impact\n",
    "# ============================================================\n",
    "\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    dom = con.execute(\"\"\"\n",
    "      SELECT domain,\n",
    "             SUM(after_nonempty) AS nonempty,\n",
    "             SUM(after_jusText)  AS after_jt,\n",
    "             SUM(drop_jusText)   AS drop_jt\n",
    "      FROM ptbrvarid_metrics\n",
    "      WHERE dataset = 'PtBrVId'\n",
    "      GROUP BY domain\n",
    "      ORDER BY nonempty DESC\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(dom[\"domain\"], dom[\"drop_jt\"])\n",
    "\n",
    "ax.set_ylabel(\"Removed by jusText\", fontsize=25)\n",
    "ax.set_title(\" \", fontsize=25)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = dom[\"drop_jt\"].max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b, label in zip(bars, dom[\"drop_jt\"]):\n",
    "    ax.annotate(\n",
    "        _fmt_millions(int(label), None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, b.get_height()),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=15,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=30, ha=\"right\", fontsize=25)\n",
    "ax.tick_params(axis=\"y\", labelsize=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"jusText_by_domain.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2899c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2991728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n\n",
       "0  2991728"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, pandas as pd\n",
    "\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "total_rows = con.execute(\"\"\"\n",
    "-- Exact total (processed only)\n",
    "SELECT COUNT(*) AS n\n",
    "FROM ptbrvarid\n",
    "WHERE dataset = 'PtBrVId';\n",
    "\n",
    "    \"\"\").fetchdf()\n",
    "                             \n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/laiarodrigo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/laiarodrigo/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK paths: ['/home/laiarodrigo/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/share/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "Has package dir?  True\n",
      "['OlÃ¡', 'mundo', '!', 'Isto', 'Ã©', 'um', 'teste', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import os, nltk, pathlib\n",
    "\n",
    "# 1) Pick a single directory and make NLTK look there\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.environ[\"NLTK_DATA\"] = NLTK_USER_DIR  # ensure child processes see it too\n",
    "if NLTK_USER_DIR not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, NLTK_USER_DIR)\n",
    "\n",
    "# 2) Force-reinstall punkt + punkt_tab into that directory\n",
    "nltk.download(\"punkt\", download_dir=NLTK_USER_DIR, force=True, quiet=False)\n",
    "# Newer NLTK also needs this metadata package\n",
    "try:\n",
    "    nltk.download(\"punkt_tab\", download_dir=NLTK_USER_DIR, force=True, quiet=False)\n",
    "except Exception:\n",
    "    pass  # older NLTK won't have it\n",
    "\n",
    "# 3) Verify both the PACKAGE and the Portuguese model are visible\n",
    "print(\"NLTK paths:\", nltk.data.path)\n",
    "print(\"Has package dir? \", pathlib.Path(NLTK_USER_DIR, \"tokenizers\", \"punkt\").exists())\n",
    "nltk.data.find(\"tokenizers/punkt\")                       # should NOT raise\n",
    "nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")     # should NOT raise\n",
    "\n",
    "# Quick smoke test\n",
    "from nltk import word_tokenize\n",
    "print(word_tokenize(\"OlÃ¡ mundo! Isto Ã© um teste.\", language=\"portuguese\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f7f1e",
   "metadata": {},
   "source": [
    "**//PUT ALL PROCESSED DOMAINS AND SPLITS INTO PTBRVARID TABLE AND THEN TO TRAIN AND TEST DATA TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f958b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESET] Cleared 0 old rows from ptbrvarid\n",
      "\n",
      "[DOMAIN] journalistic â€“ 1 splits\n",
      "  [CLEAN] journalistic/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d0e1d0dcc04ef8ba12d7960d9b8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852c171efc404b7e9c2a45cbeec222cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2231ef0c5524498be437ce3f38c25ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6bfc8cdf8f489696f69d75e3ce8f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22165ab4b6654936bb4e562f279d91f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1842746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07da31e35f854cbb995e038d1f0ee1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1720715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fc7a317c2b4315a261fd94c3300e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1720715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] journalistic/train: wrote 1,687,188 rows\n",
      "[DOMAIN DONE] journalistic: wrote 1,687,188 rows across 1 splits\n",
      "\n",
      "[DOMAIN] legal â€“ 1 splits\n",
      "  [CLEAN] legal/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd8bef8a679407b950072e097d78979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4302003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46224327b44c4dd982a42548771b30e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3527/3949769824.py:92: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c44668f81d426aab2b43955860382b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8b432b4ccc41c3a4e9f23ff74fd3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507156b6ec4429e8879fd8e2a225de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec51943c26747dab72e2aec5f10c86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4208238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3074615d72c0464990b0c6cba9787bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2781134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ded30b06e994dbfb3c913903f5634a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2781134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] legal/train: wrote 2,664,121 rows\n",
      "[DOMAIN DONE] legal: wrote 2,664,121 rows across 1 splits\n",
      "\n",
      "[DOMAIN] literature â€“ 1 splits\n",
      "  [CLEAN] literature/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb350c17802c44cbab8b08f54ae79276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8cda5a4497473ea0b1a98bd5e04987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef756d09e944e61ad88f4df92ccf0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a736c90e6ff4c53a9cd6850fed5793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d983388f9a484bb51eb0f12ea02d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6338eb94643b4441a123fdde725bfaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/81981 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f9811e40384460941f4434b4933fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c5add56fbb47d480360151cd6697f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/57002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] literature/train: wrote 55,506 rows\n",
      "[DOMAIN DONE] literature: wrote 55,506 rows across 1 splits\n",
      "\n",
      "[DOMAIN] politics â€“ 1 splits\n",
      "  [CLEAN] politics/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53966f4b3254532a03eb7565f0ee968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34605 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8493da2f54c6ca0d8cf0e26c6c3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924aee39ea4e437694381e91f0fcac55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c0021d0c2c41219de59a06cd2186bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35615a5eb3654363a59af0ff7045fa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7929ac1d7ea24befa7d090c83b95859a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a305eead25824e4195f9657a62281d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feacde247dea44849c0bdbacd701262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/26444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] politics/train: wrote 24,838 rows\n",
      "[DOMAIN DONE] politics: wrote 24,838 rows across 1 splits\n",
      "\n",
      "[DOMAIN] social_media â€“ 1 splits\n",
      "  [CLEAN] social_media/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a46031002844449928c92523c83304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2678580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4c2a9b93c6417cb65a0798e15bbea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93463cbae84772bb7c89757cfe085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce8e35cba92449aaf518ef4de4fdafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f40b916d3224104a07acc5f65244f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c048a9d81ac540b992a8eb9887817deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2487475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58dc5a601114c88a4924e01d15fb340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2173248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f5e3c3b0404595ad9806076ea0be93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2173248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] social_media/train: wrote 1,993,190 rows\n",
      "[DOMAIN DONE] social_media: wrote 1,993,190 rows across 1 splits\n",
      "\n",
      "[DOMAIN] web â€“ 1 splits\n",
      "  [CLEAN] web/train â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8a95f8730d4bfba53f3c4d36783325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c504b49cfa0478593290044f3184c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573481e6812849c6a25c07367bcd6d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2b0bdd96bf4892ac770403e23e8cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0843d6b4e98a4d9db95a025a80dd66f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afcf1e6d5614cfc87f3c6e9f4fe4e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c980a8a16f3406ea9940fd552415a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/133552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f04d620cbdd41d49567082eef8fb6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21a653fd8e0465eb14d8ffb418301b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] web/train: wrote 25,829 rows\n",
      "[DOMAIN DONE] web: wrote 25,829 rows across 1 splits\n",
      "\n",
      "[ALL DONE] Inserted 6,450,672 rows total. Table now has 6,450,672 rows.\n"
     ]
    }
   ],
   "source": [
    "# populate_ptbrvarid_full_fixed.py\n",
    "# ------------------------------------------------------------\n",
    "# Fully (re)populate DuckDB table `ptbrvarid` from PtBrVId-Raw.\n",
    "# Requires: clean_one_domain_split(domain, split, ...)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# ----- label helpers & schema -----\n",
    "\n",
    "def _label_to_name(ds, val):\n",
    "    \"\"\"Map int/str label to a readable name using HF features when available.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        return val\n",
    "    try:\n",
    "        names = ds.features[\"label\"].names\n",
    "        if isinstance(val, int) and 0 <= val < len(names):\n",
    "            return names[val]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(val)\n",
    "\n",
    "def _is_pt_br(lbl_name: str, raw_label) -> bool:\n",
    "    \"\"\"\n",
    "    Decide BR vs PT by normalized name; fallback to your rule:\n",
    "    label 0 == pt-PT (European) â†’ everything else BR.\n",
    "    \"\"\"\n",
    "    s = (lbl_name or \"\").strip().lower().replace(\"_\", \"-\")\n",
    "    if s in {\"pt-br\", \"br\", \"ptbr\", \"brazil\", \"brazilian\"}:\n",
    "        return True\n",
    "    if s in {\"pt-pt\", \"pt\", \"eu\", \"european\"}:\n",
    "        return False\n",
    "    return (isinstance(raw_label, int) and raw_label != 0)\n",
    "\n",
    "def _ensure_ptbrvarid_schema(con: duckdb.DuckDBPyConnection):\n",
    "    # Create table if missing (6-column layout)\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ptbrvarid (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            label       TEXT,\n",
    "            text_pt_br  TEXT,\n",
    "            text_pt_pt  TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    # If the table already existed without `domain`, add it.\n",
    "    cols = [r[1] for r in con.execute(\"PRAGMA table_info('ptbrvarid')\").fetchall()]\n",
    "    if \"domain\" not in cols:\n",
    "        con.execute(\"ALTER TABLE ptbrvarid ADD COLUMN domain TEXT\")\n",
    "        print(\"[MIGRATE] Added missing column: domain\")\n",
    "\n",
    "# ----- main ingest -----\n",
    "\n",
    "def ingest_ptbrvarid_all_to_duckdb(\n",
    "    db_path: str,\n",
    "    *,\n",
    "    keep_accents: bool = True,\n",
    "    num_proc: int = 1,\n",
    "    batch_size_hf: int = 1000,\n",
    "    batch_size_db: int = 5000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Clears ptbrvarid and fully repopulates it with all cleaned rows.\n",
    "    Prints progress per split, per domain, and final totals.\n",
    "    \"\"\"\n",
    "    domains = get_dataset_config_names(\"liaad/PtBrVId-Raw\")\n",
    "\n",
    "    with duckdb.connect(db_path) as con:\n",
    "        _ensure_ptbrvarid_schema(con)\n",
    "\n",
    "        # wipe existing contents so this is the ONLY data in the table\n",
    "        try:\n",
    "            existing = con.execute(\"SELECT COUNT(*) FROM ptbrvarid\").fetchone()[0]\n",
    "        except duckdb.CatalogException:\n",
    "            existing = 0\n",
    "        con.execute(\"DELETE FROM ptbrvarid\")\n",
    "        print(f\"[RESET] Cleared {existing} old rows from ptbrvarid\")\n",
    "\n",
    "        grand_total = 0\n",
    "\n",
    "        for domain in domains:\n",
    "            splits = get_dataset_split_names(\"liaad/PtBrVId-Raw\", domain)\n",
    "            domain_total = 0\n",
    "            print(f\"\\n[DOMAIN] {domain} â€“ {len(splits)} splits\")\n",
    "\n",
    "            for split in splits:\n",
    "                print(f\"  [CLEAN] {domain}/{split} â€¦\")\n",
    "                ds = clean_one_domain_split(\n",
    "                    domain, split,\n",
    "                    keep_accents=keep_accents,\n",
    "                    num_proc=num_proc,\n",
    "                    batch_size=batch_size_hf,\n",
    "                )\n",
    "\n",
    "                buf, n = [], 0\n",
    "                for ex in ds.to_iterable_dataset():\n",
    "                    lbl_name = _label_to_name(ds, ex[\"label\"])\n",
    "                    text = ex[\"text\"]\n",
    "                    buf.append({\n",
    "                        \"dataset\": \"PtBrVarId\",\n",
    "                        \"domain\": domain,\n",
    "                        \"split\": split,\n",
    "                        \"label\": lbl_name,\n",
    "                        \"text_pt_br\": text if _is_pt_br(lbl_name, ex[\"label\"]) else None,\n",
    "                        \"text_pt_pt\": text if not _is_pt_br(lbl_name, ex[\"label\"]) else None,\n",
    "                    })\n",
    "\n",
    "                    if len(buf) >= batch_size_db:\n",
    "                        df = pd.DataFrame(buf)\n",
    "                        con.register(\"ptbr_buf\", df)\n",
    "                        # INSERT with explicit column list to avoid order mismatches\n",
    "                        con.execute(\"\"\"\n",
    "                            INSERT INTO ptbrvarid (dataset, domain, split, label, text_pt_br, text_pt_pt)\n",
    "                            SELECT dataset, domain, split, label, text_pt_br, text_pt_pt FROM ptbr_buf\n",
    "                        \"\"\")\n",
    "                        con.unregister(\"ptbr_buf\")\n",
    "                        n += len(buf)\n",
    "                        buf = []\n",
    "\n",
    "                if buf:\n",
    "                    df = pd.DataFrame(buf)\n",
    "                    con.register(\"ptbr_buf\", df)\n",
    "                    con.execute(\"\"\"\n",
    "                        INSERT INTO ptbrvarid (dataset, domain, split, label, text_pt_br, text_pt_pt)\n",
    "                        SELECT dataset, domain, split, label, text_pt_br, text_pt_pt FROM ptbr_buf\n",
    "                    \"\"\")\n",
    "                    con.unregister(\"ptbr_buf\")\n",
    "                    n += len(buf)\n",
    "\n",
    "                domain_total += n\n",
    "                grand_total += n\n",
    "                print(f\"  [OK] {domain}/{split}: wrote {n:,} rows\")\n",
    "\n",
    "            print(f\"[DOMAIN DONE] {domain}: wrote {domain_total:,} rows across {len(splits)} splits\")\n",
    "\n",
    "        final_count = con.execute(\"SELECT COUNT(*) FROM ptbrvarid\").fetchone()[0]\n",
    "        print(f\"\\n[ALL DONE] Inserted {grand_total:,} rows total. Table now has {final_count:,} rows.\")\n",
    "\n",
    "# ----- example usage -----\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"   # <-- your file\n",
    "ingest_ptbrvarid_all_to_duckdb(DB_PATH, keep_accents=True, num_proc=1, batch_size_hf=1000, batch_size_db=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15177eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>text_pt_br</th>\n",
       "      <th>text_pt_pt</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Cardoso e Cunha \"InsatisfatÃ³rio\" O resultado do referendo francÃªs Ã© insatisfatÃ³rio, tÃ­mido e modesto. Os problemas da Comunidade Europeia exigiam, da parte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Santo Tirso de fora Apesar da intenÃ§Ã£o das principais autarquias do Ave, para jÃ¡ Santo Tirso quer ficar de fora deste processo. Joaquim Couto, o presidente ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>\"1 Rpm\" Ã© a abreviatura para \"Uma RevoluÃ§Ã£o por Minuto\", e a respectiva traduÃ§Ã£o inglesa, uma vez que os LX-90 de Rui Pregal da Cunha e Pedro Paulo GonÃ§alve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>LuÃ­s AntÃ³nio Mendes Nobre pode, em Outubro, por alturada 5Âª ediÃ§Ã£o do Festival Internacional de MÃºsica de Macau, concretizar, como confessou, \"um velho sonh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>NÃ£o posso dizer mais nada. Por que Ã© que me estÃ¡ a fazer perguntas sobre Lisboa? Porque foi presidente da CÃ¢mara durante dez anos e nota-se que foi uma fase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>A vÃ­tima, de 32 anos, era conhecida no local como alcoÃ³lica: O relatÃ³rio da PJ salienta que o casal vivia em \"condiÃ§Ãµes miserÃ¡veis\". O juiz de instruÃ§Ã£o cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>O desafio comeÃ§ou com ajustamentos mÃºtuos, sendo patente a vantagem dos azuis de BelÃ©m, graÃ§as Ã  supremacia revelada pelo seu sector intermediÃ¡rio. Mauro Ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>Suspeito que os maiores interessados em manter a semi-clandestinidade sÃ£o, justamente, os delinquentes inclusive na polÃ­cia, beneficiados por propinas. A hi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>EleiÃ§Ãµes vÃ£o custar 180 mil contos O ministro da AdministraÃ§Ã£o Interna, Manuel Pereira, revelou ontem que os preparativos para as prÃ³ximas eleiÃ§Ãµes legislat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Ã‰ da competÃªncia do MinistÃ©rio dos NegÃ³cios Estrangeiros manter actualizado o levantamento das comunidades portuguesas emigradas ou em actividades de cooper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset         split  label text_pt_br  \\\n",
       "0      PtBrVId  journalistic  train      pt-PT   \n",
       "1      PtBrVId  journalistic  train      pt-PT   \n",
       "2      PtBrVId  journalistic  train      pt-PT   \n",
       "3      PtBrVId  journalistic  train      pt-PT   \n",
       "4      PtBrVId  journalistic  train      pt-PT   \n",
       "...        ...           ...    ...        ...   \n",
       "49995  PtBrVId  journalistic  train      pt-PT   \n",
       "49996  PtBrVId  journalistic  train      pt-PT   \n",
       "49997  PtBrVId  journalistic  train      pt-BR   \n",
       "49998  PtBrVId  journalistic  train      pt-PT   \n",
       "49999  PtBrVId  journalistic  train      pt-PT   \n",
       "\n",
       "                                                                                                                                                            text_pt_pt  \\\n",
       "0                                                                                                                                                                 None   \n",
       "1                                                                                                                                                                 None   \n",
       "2                                                                                                                                                                 None   \n",
       "3                                                                                                                                                                 None   \n",
       "4                                                                                                                                                                 None   \n",
       "...                                                                                                                                                                ...   \n",
       "49995                                                                                                                                                             None   \n",
       "49996                                                                                                                                                             None   \n",
       "49997  Suspeito que os maiores interessados em manter a semi-clandestinidade sÃ£o, justamente, os delinquentes inclusive na polÃ­cia, beneficiados por propinas. A hi...   \n",
       "49998                                                                                                                                                             None   \n",
       "49999                                                                                                                                                             None   \n",
       "\n",
       "                                                                                                                                                                domain  \n",
       "0      Cardoso e Cunha \"InsatisfatÃ³rio\" O resultado do referendo francÃªs Ã© insatisfatÃ³rio, tÃ­mido e modesto. Os problemas da Comunidade Europeia exigiam, da parte ...  \n",
       "1      Santo Tirso de fora Apesar da intenÃ§Ã£o das principais autarquias do Ave, para jÃ¡ Santo Tirso quer ficar de fora deste processo. Joaquim Couto, o presidente ...  \n",
       "2      \"1 Rpm\" Ã© a abreviatura para \"Uma RevoluÃ§Ã£o por Minuto\", e a respectiva traduÃ§Ã£o inglesa, uma vez que os LX-90 de Rui Pregal da Cunha e Pedro Paulo GonÃ§alve...  \n",
       "3      LuÃ­s AntÃ³nio Mendes Nobre pode, em Outubro, por alturada 5Âª ediÃ§Ã£o do Festival Internacional de MÃºsica de Macau, concretizar, como confessou, \"um velho sonh...  \n",
       "4      NÃ£o posso dizer mais nada. Por que Ã© que me estÃ¡ a fazer perguntas sobre Lisboa? Porque foi presidente da CÃ¢mara durante dez anos e nota-se que foi uma fase...  \n",
       "...                                                                                                                                                                ...  \n",
       "49995  A vÃ­tima, de 32 anos, era conhecida no local como alcoÃ³lica: O relatÃ³rio da PJ salienta que o casal vivia em \"condiÃ§Ãµes miserÃ¡veis\". O juiz de instruÃ§Ã£o cri...  \n",
       "49996  O desafio comeÃ§ou com ajustamentos mÃºtuos, sendo patente a vantagem dos azuis de BelÃ©m, graÃ§as Ã  supremacia revelada pelo seu sector intermediÃ¡rio. Mauro Ai...  \n",
       "49997                                                                                                                                                             None  \n",
       "49998  EleiÃ§Ãµes vÃ£o custar 180 mil contos O ministro da AdministraÃ§Ã£o Interna, Manuel Pereira, revelou ontem que os preparativos para as prÃ³ximas eleiÃ§Ãµes legislat...  \n",
       "49999  Ã‰ da competÃªncia do MinistÃ©rio dos NegÃ³cios Estrangeiros manter actualizado o levantamento das comunidades portuguesas emigradas ou em actividades de cooper...  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect('../data/duckdb/subs.duckdb')\n",
    "con.execute('SELECT * FROM ptbrvarid WHERE dataset=\\'PtBrVId\\' LIMIT 50000').df()\n",
    "con.execute('SELECT COUNT(*) FROM ptbrvarid WHERE dataset=\\'PtBrVId\\'').fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f92b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x783b47bb44b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, pathlib, pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "\n",
    "con = duckdb.connect(\"../data/duckdb/subs.duckdb\")\n",
    "\n",
    "# Known domain names to distinguish real domain vs misplaced text\n",
    "KNOWN_DOMAINS = (\"journalistic\",\"legal\",\"web\",\"literature\",\"politics\",\"social_media\")\n",
    "\n",
    "con.execute(\"DROP VIEW IF EXISTS ptbrvid_repaired_v;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW ptbrvid_repaired_v AS\n",
    "WITH raw AS (\n",
    "  SELECT dataset, domain, split, label, text_pt_br, text_pt_pt\n",
    "  FROM ptbrvarid\n",
    "  WHERE dataset='PtBrVId'\n",
    "),\n",
    "norm AS (\n",
    "  SELECT\n",
    "    -- language: prefer explicit label if present, else take the literal that was stuffed into text_pt_br\n",
    "    CASE\n",
    "      WHEN lower(label) IN ('pt-br','pt-pt') THEN CASE WHEN lower(label)='pt-br' THEN 'pt-BR' ELSE 'pt-PT' END\n",
    "      WHEN text_pt_br IN ('pt-BR','pt-PT')      THEN text_pt_br\n",
    "      ELSE NULL\n",
    "    END AS lang,\n",
    "\n",
    "    -- text: prefer the proper text columns; if empty, fall back to `domain` only if it looks like text\n",
    "    CASE\n",
    "      WHEN text_pt_br IS NOT NULL AND text_pt_br NOT IN ('pt-BR','pt-PT') THEN text_pt_br\n",
    "      WHEN text_pt_pt IS NOT NULL AND text_pt_pt NOT IN ('pt-BR','pt-PT') THEN text_pt_pt\n",
    "      WHEN domain IS NOT NULL AND lower(domain) NOT IN {KNOWN_DOMAINS}\n",
    "           AND length(domain) > 40 THEN domain\n",
    "      ELSE NULL\n",
    "    END AS text,\n",
    "\n",
    "    split, domain\n",
    "  FROM raw\n",
    "),\n",
    "ok AS (\n",
    "  SELECT\n",
    "    'PtBrVId' AS dataset,\n",
    "    split,\n",
    "    lang  AS label,\n",
    "    CASE WHEN lang='pt-BR' THEN text END AS text_pt_br,\n",
    "    CASE WHEN lang='pt-PT' THEN text END AS text_pt_pt\n",
    "  FROM norm\n",
    "  WHERE lang IS NOT NULL AND text IS NOT NULL\n",
    ")\n",
    "SELECT * FROM ok;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09319d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>text_pt_br</th>\n",
       "      <th>text_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legal</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>PorÃ©m, atente-se nesta parte do SumÃ¡rio: - O princÃ­pio do in dubio pro reo constitui uma imposiÃ§Ã£o dirigida ao julgador no sentido de se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legal</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>ConcluÃ­ram requerendo que \"se considere justificada a nÃ£o entregados valores indicados, requerendo ainda o aumento do valor disponÃ­vel m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legal</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>O contrato de locaÃ§Ã£o financeira apresenta uma estrutura bilateral, onde sÃ£o realizados dois contratos: O fornecedor celebra um contrato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>\"Sobre a terra, um ligeiro aguaceiro e um arco-Ã­ris completo envolvendo a histÃ³rica Torre de BelÃ©m, como numa aurÃ©ola de glÃ³ria. Vemos a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social_media</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Discorda Ã  vontade, porque nitidamente nÃ£o fazes a mÃ­nima ideia sobre o que estÃ¡s a falar. Desenvolvimento? Zero. Suporte? Zero. Trata-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>No Cinearte, em Santos, hÃ¡ mÃºsica improvisada a partir da 1h00. Eduardo Cunha e JoÃ£o Oliveira, nas guitarras elÃ©ctricas e Luis Desirat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>Outra alteraÃ§Ã£o Ã© o \"sugar craving\" (desejo exagerado de comer aÃ§Ãºcar). \"Ã‰ uma compulsÃ£o. As pessoas nÃ£o tÃªm domÃ­nio sobre a situaÃ§Ã£o\", ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>legal</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Que foi o arguido quem levou a cabo a agressÃ£o de que resultaram as lesÃµes e morte da sua mÃ£e nas circunstÃ¢ncias mencionadas nos factos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>journalistic</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>De resto, as propostas de Schaeuble sÃ£o olhadas, aqui, como um primeiro balÃ£o de ensaio Ã s posiÃ§Ãµes que a Alemanha defenderÃ¡ na ConferÃªn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>social_media</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>APELO Ã€ AUTARQUIA DE ANADIA! Muitos de nÃ³s abdicam, como eu, de receber salÃ¡rio para poder pagar salÃ¡rios dos funcionÃ¡rios e contas Ã  au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          split  label  \\\n",
       "0         legal  pt-PT   \n",
       "1         legal  pt-PT   \n",
       "2         legal  pt-PT   \n",
       "3  journalistic  pt-PT   \n",
       "4  social_media  pt-PT   \n",
       "5  journalistic  pt-PT   \n",
       "6  journalistic  pt-BR   \n",
       "7         legal  pt-PT   \n",
       "8  journalistic  pt-PT   \n",
       "9  social_media  pt-PT   \n",
       "\n",
       "                                                                                                                                    text_pt_br  \\\n",
       "0                                                                                                                                         None   \n",
       "1                                                                                                                                         None   \n",
       "2                                                                                                                                         None   \n",
       "3                                                                                                                                         None   \n",
       "4                                                                                                                                         None   \n",
       "5                                                                                                                                         None   \n",
       "6  Outra alteraÃ§Ã£o Ã© o \"sugar craving\" (desejo exagerado de comer aÃ§Ãºcar). \"Ã‰ uma compulsÃ£o. As pessoas nÃ£o tÃªm domÃ­nio sobre a situaÃ§Ã£o\", ...   \n",
       "7                                                                                                                                         None   \n",
       "8                                                                                                                                         None   \n",
       "9                                                                                                                                         None   \n",
       "\n",
       "                                                                                                                                    text_pt_pt  \n",
       "0  PorÃ©m, atente-se nesta parte do SumÃ¡rio: - O princÃ­pio do in dubio pro reo constitui uma imposiÃ§Ã£o dirigida ao julgador no sentido de se...  \n",
       "1  ConcluÃ­ram requerendo que \"se considere justificada a nÃ£o entregados valores indicados, requerendo ainda o aumento do valor disponÃ­vel m...  \n",
       "2  O contrato de locaÃ§Ã£o financeira apresenta uma estrutura bilateral, onde sÃ£o realizados dois contratos: O fornecedor celebra um contrato...  \n",
       "3  \"Sobre a terra, um ligeiro aguaceiro e um arco-Ã­ris completo envolvendo a histÃ³rica Torre de BelÃ©m, como numa aurÃ©ola de glÃ³ria. Vemos a...  \n",
       "4  Discorda Ã  vontade, porque nitidamente nÃ£o fazes a mÃ­nima ideia sobre o que estÃ¡s a falar. Desenvolvimento? Zero. Suporte? Zero. Trata-s...  \n",
       "5  No Cinearte, em Santos, hÃ¡ mÃºsica improvisada a partir da 1h00. Eduardo Cunha e JoÃ£o Oliveira, nas guitarras elÃ©ctricas e Luis Desirat, ...  \n",
       "6                                                                                                                                         None  \n",
       "7  Que foi o arguido quem levou a cabo a agressÃ£o de que resultaram as lesÃµes e morte da sua mÃ£e nas circunstÃ¢ncias mencionadas nos factos ...  \n",
       "8  De resto, as propostas de Schaeuble sÃ£o olhadas, aqui, como um primeiro balÃ£o de ensaio Ã s posiÃ§Ãµes que a Alemanha defenderÃ¡ na ConferÃªn...  \n",
       "9  APELO Ã€ AUTARQUIA DE ANADIA! Muitos de nÃ³s abdicam, como eu, de receber salÃ¡rio para poder pagar salÃ¡rios dos funcionÃ¡rios e contas Ã  au...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(con.execute(\"\"\"\n",
    "SELECT split, label, text_pt_br, text_pt_pt\n",
    "FROM ptbrvid_repaired_v\n",
    "ORDER BY random()\n",
    "LIMIT 10\n",
    "\"\"\").df())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4504ce6",
   "metadata": {},
   "source": [
    "**//FRMT DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://raw.githubusercontent.com/google-research/google-research/HEAD/frmt/dataset/entity_bucket/pt_entity_dev_en_pt-BR.tsv', 'https://raw.githubusercontent.com/google-research/google-research/HEAD/frmt/dataset/entity_bucket/pt_entity_dev_en_pt-PT.tsv', 'https://raw.githubusercontent.com/google-research/google-research/HEAD/frmt/dataset/entity_bucket/pt_entity_test_en_pt-BR.tsv']\n"
     ]
    }
   ],
   "source": [
    "GITHUB_RAW = (\"https://raw.githubusercontent.com/google-research/google-research/\"\n",
    "              \"HEAD/frmt/dataset\")\n",
    "\n",
    "buckets = {          # bucket â†’ filename prefix inside that bucket\n",
    "    \"lexical\": \"pt_lexical\",\n",
    "    \"entity\" : \"pt_entity\",\n",
    "    \"random\" : \"pt_random\",\n",
    "}\n",
    "\n",
    "splits   = [\"dev\", \"test\", \"exemplars\"]        # the paperâ€™s three splits\n",
    "regions  = [\"pt-BR\", \"pt-PT\"]                  # â¬…ï¸ we ignore zh-* files\n",
    "\n",
    "def urls(bucket):\n",
    "    prefix = buckets[bucket]\n",
    "    return [f\"{GITHUB_RAW}/{bucket}_bucket/\"\n",
    "            f\"{prefix}_{split}_en_{region}.tsv\"\n",
    "            for split in splits\n",
    "            for region in regions]\n",
    "\n",
    "# sanity-check\n",
    "print(urls(\"entity\")[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fede0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lexical_dev_pt_BR', 'lexical_dev_pt_PT', 'lexical_test_pt_BR', 'lexical_test_pt_PT', 'lexical_exemplars_pt_BR', 'lexical_exemplars_pt_PT']\n"
     ]
    }
   ],
   "source": [
    "def split_key(bucket, split, region):\n",
    "    # pt-BR  âžœ  pt_BR   (dash â†’ underscore)\n",
    "    return f\"{bucket}_{split}_{region.replace('-', '_')}\"\n",
    "\n",
    "data_files = {\n",
    "    split_key(bucket, split, region): [\n",
    "        f\"{GITHUB_RAW}/{bucket}_bucket/\"\n",
    "        f\"{buckets[bucket]}_{split}_en_{region}.tsv\"\n",
    "    ]\n",
    "    for bucket in buckets\n",
    "    for split  in splits\n",
    "    for region in regions\n",
    "}\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files   = data_files,\n",
    "        delimiter    = \"\\t\",\n",
    "        column_names = [\"en\", \"pt\"],\n",
    ")\n",
    "\n",
    "print(list(ds.keys())[:6])\n",
    "# ['lexical_dev_pt_BR', 'lexical_dev_pt_PT', 'lexical_test_pt_BR', â€¦]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_br = pd.DataFrame(ds['entity_dev_pt_BR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9081030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConstÃ¢ncio was Secretary of State for Planning in the I and II Provisional Government of Portugal from 1974 to 1975, and Secretary of State for Budget and Planning in 1976 in the IV Provisional Government.</td>\n",
       "      <td>ConstÃ¢ncio foi SecretÃ¡rio de Estado para Planejamento no 1Âº e 2Âº Governos ProvisÃ³rios de Portugal, de 1974 a 1975, e SecretÃ¡rio de Estado para OrÃ§amento e Planejamento em 1976 no 4Âº Governo ProvisÃ³rio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He then became Finance Minister from January to August 1978 in the II Constitutional Government of Portugal, and is therefore until now the youngest Portuguese Finance Minister since the revolution.</td>\n",
       "      <td>Depois, tornou-se Ministro das FinanÃ§as de janeiro a agosto de 1978 no 2Âº Governo Constitucional de Portugal, e Ã©, portanto, atÃ© hoje, o Ministro das FinanÃ§as portuguÃªs mais jovem desde a revoluÃ§Ã£o.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ConstÃ¢ncio was secretary-general of the Socialist Party from 1986 to 1989.</td>\n",
       "      <td>De 1986 a 1989, ConstÃ¢ncio foi secretÃ¡rio-geral do Partido Socialista.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He lost the legislative elections of 19 July 1987, but remained in office.</td>\n",
       "      <td>Ele perdeu as eleiÃ§Ãµes legislativas de 19 de julho de 1987, mas permaneceu no gabinete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He resigned the following year, being replaced by Jorge Sampaio.</td>\n",
       "      <td>No ano seguinte, ele renunciou, sendo substituÃ­do por Jorge Sampaio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>The northern and western sides of the castle, on the other hand, were naturally protected by the steep hillside sloping downward from the castle's foundations.</td>\n",
       "      <td>Os lados norte e oeste do castelo, por sua vez, eram naturalmente protegidos pela colina Ã­ngreme inclinada para baixo das fundaÃ§Ãµes do castelo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>The castle is also partially encircled by a moat, now dry.</td>\n",
       "      <td>O castelo tambÃ©m Ã©, em parte, rodeado por um fosso, agora seco.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>The main entrance is fronted by a stone bridge across the moat.</td>\n",
       "      <td>De frente para a entrada principal, hÃ¡ uma ponte de pedra sobre o fosso.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>On the west side, there is a long curtain wall extending downhill, ending at a tower (the Torre De CouraÃ§a).</td>\n",
       "      <td>No lado oeste, hÃ¡ uma divisÃ³ria que se estende para baixo, terminando em uma torre (a Torre de CouraÃ§a).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>This tower served to control the valley below, and it could also be used to escape, in case the castle was taken by enemies.</td>\n",
       "      <td>Essa torre servia para controlar o vale abaixo, e tambÃ©m podia ser usada para uma fuga, caso o castelo fosse tomado por inimigos.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                en  \\\n",
       "0    ConstÃ¢ncio was Secretary of State for Planning in the I and II Provisional Government of Portugal from 1974 to 1975, and Secretary of State for Budget and Planning in 1976 in the IV Provisional Government.   \n",
       "1           He then became Finance Minister from January to August 1978 in the II Constitutional Government of Portugal, and is therefore until now the youngest Portuguese Finance Minister since the revolution.   \n",
       "2                                                                                                                                       ConstÃ¢ncio was secretary-general of the Socialist Party from 1986 to 1989.   \n",
       "3                                                                                                                                       He lost the legislative elections of 19 July 1987, but remained in office.   \n",
       "4                                                                                                                                                 He resigned the following year, being replaced by Jorge Sampaio.   \n",
       "..                                                                                                                                                                                                             ...   \n",
       "930                                                The northern and western sides of the castle, on the other hand, were naturally protected by the steep hillside sloping downward from the castle's foundations.   \n",
       "931                                                                                                                                                     The castle is also partially encircled by a moat, now dry.   \n",
       "932                                                                                                                                                The main entrance is fronted by a stone bridge across the moat.   \n",
       "933                                                                                                   On the west side, there is a long curtain wall extending downhill, ending at a tower (the Torre De CouraÃ§a).   \n",
       "934                                                                                   This tower served to control the valley below, and it could also be used to escape, in case the castle was taken by enemies.   \n",
       "\n",
       "                                                                                                                                                                                                            pt  \n",
       "0    ConstÃ¢ncio foi SecretÃ¡rio de Estado para Planejamento no 1Âº e 2Âº Governos ProvisÃ³rios de Portugal, de 1974 a 1975, e SecretÃ¡rio de Estado para OrÃ§amento e Planejamento em 1976 no 4Âº Governo ProvisÃ³rio.  \n",
       "1       Depois, tornou-se Ministro das FinanÃ§as de janeiro a agosto de 1978 no 2Âº Governo Constitucional de Portugal, e Ã©, portanto, atÃ© hoje, o Ministro das FinanÃ§as portuguÃªs mais jovem desde a revoluÃ§Ã£o.  \n",
       "2                                                                                                                                       De 1986 a 1989, ConstÃ¢ncio foi secretÃ¡rio-geral do Partido Socialista.  \n",
       "3                                                                                                                      Ele perdeu as eleiÃ§Ãµes legislativas de 19 de julho de 1987, mas permaneceu no gabinete.  \n",
       "4                                                                                                                                         No ano seguinte, ele renunciou, sendo substituÃ­do por Jorge Sampaio.  \n",
       "..                                                                                                                                                                                                         ...  \n",
       "930                                                            Os lados norte e oeste do castelo, por sua vez, eram naturalmente protegidos pela colina Ã­ngreme inclinada para baixo das fundaÃ§Ãµes do castelo.  \n",
       "931                                                                                                                                            O castelo tambÃ©m Ã©, em parte, rodeado por um fosso, agora seco.  \n",
       "932                                                                                                                                   De frente para a entrada principal, hÃ¡ uma ponte de pedra sobre o fosso.  \n",
       "933                                                                                                   No lado oeste, hÃ¡ uma divisÃ³ria que se estende para baixo, terminando em uma torre (a Torre de CouraÃ§a).  \n",
       "934                                                                          Essa torre servia para controlar o vale abaixo, e tambÃ©m podia ser usada para uma fuga, caso o castelo fosse tomado por inimigos.  \n",
       "\n",
       "[935 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ee48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.</td>\n",
       "      <td>ApÃ³s as eleiÃ§Ãµes legislativas portuguesas de 2009, realizadas em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito pela segunda vez Primeiro-Ministro de Portugal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.</td>\n",
       "      <td>ApÃ³s as eleiÃ§Ãµes legislativas portuguesas de 2009, realizadas em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como Primeiro-Ministro de Portugal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          en  \\\n",
       "52  After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.   \n",
       "64  After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.   \n",
       "\n",
       "                                                                                                                                                                               pt  \n",
       "52              ApÃ³s as eleiÃ§Ãµes legislativas portuguesas de 2009, realizadas em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito pela segunda vez Primeiro-Ministro de Portugal.  \n",
       "64  ApÃ³s as eleiÃ§Ãµes legislativas portuguesas de 2009, realizadas em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como Primeiro-Ministro de Portugal.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_pt = pd.DataFrame(ds['entity_dev_pt_PT'])\n",
    "entity_pt[entity_pt[\"en\"].duplicated(keep=False)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.</td>\n",
       "      <td>ApÃ³s a eleiÃ§Ã£o legislativa portuguesa de 2009, realizada em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como primeiro-ministro de Portugal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.</td>\n",
       "      <td>ApÃ³s a eleiÃ§Ã£o legislativa portuguesa de 2009, realizada em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como primeiro-ministro de Portugal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          en  \\\n",
       "52  After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.   \n",
       "64  After the Portuguese legislative election of 2009, held on 27 September 2009, JosÃ© SÃ³crates was elected for a second term as Prime Minister of Portugal.   \n",
       "\n",
       "                                                                                                                                                                          pt  \n",
       "52  ApÃ³s a eleiÃ§Ã£o legislativa portuguesa de 2009, realizada em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como primeiro-ministro de Portugal.  \n",
       "64  ApÃ³s a eleiÃ§Ã£o legislativa portuguesa de 2009, realizada em 27 de setembro de 2009, JosÃ© SÃ³crates foi eleito para um segundo mandato como primeiro-ministro de Portugal.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_br[entity_br[\"en\"].duplicated(keep=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f132b2",
   "metadata": {},
   "source": [
    "**//GOLD COLLECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    gold_collection: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    referencia_DeepL: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    referencia_manual: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"joaosanches/golden_collection\")\n",
    "\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58278e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Segundo Kellner, apesar de o animal ser um baixinho (poderia atingir, no mÃ¡ximo, 2,5 metros de altura), suas patas e bacia tÃªm caracterÃ­sticas anatÃ´micas muito semelhantes Ã s do ilustre rÃ©ptil norte-americano.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Para a ONG, hÃ¡ evidÃªncias de que as companhias que mais exportam madeira para os EUA estejam envolvidas com o comÃ©rcio ilegal do produto.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mas, segundo a agÃªncia de notÃ­cias France Presse, a Venezuela jÃ¡ tem um caso suspeito: um homem de 48 anos que viajou recentemente Ã  China.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ele afirmou que dieta e exercÃ­cios devem continuar a protagonizar tratamentos para emagrecer.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O biÃ³logo William Eberhard, da Universidade da Costa Rica, descobriu que as larvas desse inseto, ao parasitar a aranha Plesiometa argyra, provocam mudanÃ§as no comportamento da hospedeira.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Isso significa que a preposiÃ§Ã£o Ã© o termo que relaciona substantivo a substantivo, verbo a substantivo, substantivo a verbo, adjetivo a substantivo, advÃ©rbio a substantivo, etc.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Foi entÃ£o que a vestimenta mais feminina que se conhece comeÃ§ou a ganhar forma: o espartilho.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Um de seus professores foi Martin Wegelius.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Nessa Ã©poca, iniciou uma verdadeira polÃªmica com o escritor democrata BjÃ¶rnstjerne BjÃ¶rnson, atravÃ©s de correspondÃªncia.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Disponibilizar conteÃºdo livre, ao acesso de todos, de qualidade, sendo uma fonte fÃ­avel de conhecimento educativo, informativo, em um plano horizontal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                    text\n",
       "0    Segundo Kellner, apesar de o animal ser um baixinho (poderia atingir, no mÃ¡ximo, 2,5 metros de altura), suas patas e bacia tÃªm caracterÃ­sticas anatÃ´micas muito semelhantes Ã s do ilustre rÃ©ptil norte-americano.\\n\n",
       "1                                                                            Para a ONG, hÃ¡ evidÃªncias de que as companhias que mais exportam madeira para os EUA estejam envolvidas com o comÃ©rcio ilegal do produto.\\n\n",
       "2                                                                          Mas, segundo a agÃªncia de notÃ­cias France Presse, a Venezuela jÃ¡ tem um caso suspeito: um homem de 48 anos que viajou recentemente Ã  China.\\n\n",
       "3                                                                                                                        Ele afirmou que dieta e exercÃ­cios devem continuar a protagonizar tratamentos para emagrecer.\\n\n",
       "4                          O biÃ³logo William Eberhard, da Universidade da Costa Rica, descobriu que as larvas desse inseto, ao parasitar a aranha Plesiometa argyra, provocam mudanÃ§as no comportamento da hospedeira.\\n\n",
       "..                                                                                                                                                                                                                   ...\n",
       "495                                  Isso significa que a preposiÃ§Ã£o Ã© o termo que relaciona substantivo a substantivo, verbo a substantivo, substantivo a verbo, adjetivo a substantivo, advÃ©rbio a substantivo, etc.\\n\n",
       "496                                                                                                                      Foi entÃ£o que a vestimenta mais feminina que se conhece comeÃ§ou a ganhar forma: o espartilho.\\n\n",
       "497                                                                                                                                                                        Um de seus professores foi Martin Wegelius.\\n\n",
       "498                                                                                           Nessa Ã©poca, iniciou uma verdadeira polÃªmica com o escritor democrata BjÃ¶rnstjerne BjÃ¶rnson, atravÃ©s de correspondÃªncia.\\n\n",
       "499                                                              Disponibilizar conteÃºdo livre, ao acesso de todos, de qualidade, sendo uma fonte fÃ­avel de conhecimento educativo, informativo, em um plano horizontal.\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ds['gold_collection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14402f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Segundo Kellner, apesar de o animal ser muito pequeno (poderia atingir, no mÃ¡ximo, 2,5 metros de altura), as suas patas e bacia tÃªm caracterÃ­sticas anatÃ³micas muito semelhantes Ã s do ilustre rÃ©ptil norte-americano.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Para a ONG, hÃ¡ evidÃªncias de que as empresas que mais exportam madeira para os EUA estejam envolvidas com o comÃ©rcio ilegal do produto.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mas, segundo a agÃªncia de notÃ­cias France Presse, a Venezuela jÃ¡ tem um caso suspeito: um homem de 48 anos que viajou recentemente Ã  China.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ele afirmou que dieta e exercÃ­cios devem continuar a protagonizar tratamentos para emagrecer.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O biÃ³logo William Eberhard, da Universidade da Costa Rica, descobriu que as larvas desse inseto, ao parasitar a aranha Plesiometa argyra, provocam mudanÃ§as no comportamento da hospedeira.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Isso significa que a preposiÃ§Ã£o Ã© o termo que relaciona substantivo a substantivo, verbo a substantivo, substantivo a verbo, adjetivo a substantivo, advÃ©rbio a substantivo, etc.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Foi entÃ£o que a peÃ§a de vestuÃ¡rio mais feminina que se conhece comeÃ§ou a ganhar forma: o espartilho.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Um dos seus professores foi Martin Wegelius.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Nessa Ã©poca, iniciou-se uma verdadeira polÃ©mica com o escritor democrata BjÃ¶rnstjerne BjÃ¶rnson, atravÃ©s de correspondÃªncia.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Disponibilizar conteÃºdo livre, ao acesso de todos, de qualidade, sendo uma fonte fÃ­avel de conhecimento educativo, informativo, em um plano horizontal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                         text\n",
       "0    Segundo Kellner, apesar de o animal ser muito pequeno (poderia atingir, no mÃ¡ximo, 2,5 metros de altura), as suas patas e bacia tÃªm caracterÃ­sticas anatÃ³micas muito semelhantes Ã s do ilustre rÃ©ptil norte-americano.\\n\n",
       "1                                                                                   Para a ONG, hÃ¡ evidÃªncias de que as empresas que mais exportam madeira para os EUA estejam envolvidas com o comÃ©rcio ilegal do produto.\\n\n",
       "2                                                                               Mas, segundo a agÃªncia de notÃ­cias France Presse, a Venezuela jÃ¡ tem um caso suspeito: um homem de 48 anos que viajou recentemente Ã  China.\\n\n",
       "3                                                                                                                             Ele afirmou que dieta e exercÃ­cios devem continuar a protagonizar tratamentos para emagrecer.\\n\n",
       "4                               O biÃ³logo William Eberhard, da Universidade da Costa Rica, descobriu que as larvas desse inseto, ao parasitar a aranha Plesiometa argyra, provocam mudanÃ§as no comportamento da hospedeira.\\n\n",
       "..                                                                                                                                                                                                                        ...\n",
       "495                                       Isso significa que a preposiÃ§Ã£o Ã© o termo que relaciona substantivo a substantivo, verbo a substantivo, substantivo a verbo, adjetivo a substantivo, advÃ©rbio a substantivo, etc.\\n\n",
       "496                                                                                                                    Foi entÃ£o que a peÃ§a de vestuÃ¡rio mais feminina que se conhece comeÃ§ou a ganhar forma: o espartilho.\\n\n",
       "497                                                                                                                                                                            Um dos seus professores foi Martin Wegelius.\\n\n",
       "498                                                                                             Nessa Ã©poca, iniciou-se uma verdadeira polÃ©mica com o escritor democrata BjÃ¶rnstjerne BjÃ¶rnson, atravÃ©s de correspondÃªncia.\\n\n",
       "499                                                                   Disponibilizar conteÃºdo livre, ao acesso de todos, de qualidade, sendo uma fonte fÃ­avel de conhecimento educativo, informativo, em um plano horizontal.\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ds['referencia_manual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31850ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
