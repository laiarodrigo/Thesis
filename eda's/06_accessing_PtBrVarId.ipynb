{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80399a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, pathlib, gc\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import os, nltk\n",
    "import os, nltk, re\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "DB_PATH = '../data/duckdb/subs.duckdb'\n",
    "TABLE   = 'ptbrvarid'   # raw goes here, as you requested\n",
    "BATCH   = 20_000\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "if NLTK_USER_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_USER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cb72c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: journalistic\n",
      "  Processing split: train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m                     con.unregister(\u001b[33m'\u001b[39m\u001b[33mbuf\u001b[39m\u001b[33m'\u001b[39m); n_new += \u001b[38;5;28mlen\u001b[39m(buf); buf.clear(); gc.collect()\n\u001b[32m     46\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_in\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mingest_ptbrvid_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mingest_ptbrvid_raw\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mingest_ptbrvid_raw\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_dataset_config_names\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mliaad/PtBrVId-Raw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing domain: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdomain\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mingest_ptbrvid_raw\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m ds  = load_dataset(\u001b[33m'\u001b[39m\u001b[33mliaad/PtBrVId-Raw\u001b[39m\u001b[33m'\u001b[39m, domain, split=split, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m buf = []; n_in = n_new = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt-BR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt-PT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbr\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt-BR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2538\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2535\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2538\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2055\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2048\u001b[39m     formatter = get_formatter(\n\u001b[32m   2049\u001b[39m         \u001b[38;5;28mself\u001b[39m.formatting.format_type,\n\u001b[32m   2050\u001b[39m         features=\u001b[38;5;28mself\u001b[39m._features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.is_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2051\u001b[39m         token_per_repo_id=\u001b[38;5;28mself\u001b[39m.token_per_repo_id,\n\u001b[32m   2052\u001b[39m     )\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   2054\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2080\u001b[39m, in \u001b[36mFormattedExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2078\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable._iter_arrow():\n\u001b[32m   2079\u001b[39m     columns = \u001b[38;5;28mset\u001b[39m(pa_table.column_names)\n\u001b[32m-> \u001b[39m\u001b[32m2080\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;66;03m# add missing columns\u001b[39;00m\n\u001b[32m   2082\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/features/features.py:1822\u001b[39m, in \u001b[36mFeatures.arrow_schema\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1814\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   1815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34marrow_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1816\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1817\u001b[39m \u001b[33;03m    Features schema.\u001b[39;00m\n\u001b[32m   1818\u001b[39m \n\u001b[32m   1819\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   1820\u001b[39m \u001b[33;03m        :obj:`pyarrow.Schema`\u001b[39;00m\n\u001b[32m   1821\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1822\u001b[39m     hf_metadata = {\u001b[33m\"\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}}\n\u001b[32m   1823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.schema(\u001b[38;5;28mself\u001b[39m.type).with_metadata({\u001b[33m\"\u001b[39m\u001b[33mhuggingface\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(hf_metadata)})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/features/features.py:1889\u001b[39m, in \u001b[36mFeatures.to_dict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1888\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/utils/py_utils.py:229\u001b[39m, in \u001b[36masdict\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dataclass_instance(obj):\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a dict or a dataclass\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/utils/py_utils.py:222\u001b[39m, in \u001b[36masdict.<locals>._asdict_inner\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)(_asdict_inner(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {_asdict_inner(k): \u001b[43m_asdict_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj.items()}\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m copy.deepcopy(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/utils/py_utils.py:205\u001b[39m, in \u001b[36masdict.<locals>._asdict_inner\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_dataclass_instance\u001b[39m(obj):\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m is_dataclass(obj) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_asdict_inner\u001b[39m(obj):\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_dataclass_instance(obj):\n\u001b[32m    207\u001b[39m         result = {}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, pathlib, gc\n",
    "\n",
    "DB_PATH = '../data/duckdb/subs.duckdb'\n",
    "TABLE   = 'ptbrvarid'   # raw goes here, as you requested\n",
    "BATCH   = 20_000\n",
    "\n",
    "with duckdb.connect(DB_PATH) as con:\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ptbrvarid (\n",
    "        dataset     TEXT,\n",
    "        domain      TEXT,\n",
    "        split       TEXT,\n",
    "        label       TEXT,           -- 'pt-BR' | 'pt-PT'\n",
    "        text_pt_br  TEXT,\n",
    "        text_pt_pt  TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    # keep only RAW rows scoped by dataset tag so we can also store processed later\n",
    "    con.execute(\"DELETE FROM ptbrvarid WHERE dataset='PtBrVId-Raw'\")\n",
    "\n",
    "def ingest_ptbrvid_raw():\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        for domain in get_dataset_config_names('liaad/PtBrVId-Raw'):\n",
    "            print(f\"Processing domain: {domain}\")\n",
    "            for split in get_dataset_split_names('liaad/PtBrVId-Raw', domain):\n",
    "                print(f\"  Processing split: {split}\")\n",
    "                ds  = load_dataset('liaad/PtBrVId-Raw', domain, split=split, streaming=True)\n",
    "                buf = []; n_in = n_new = 0\n",
    "                for ex in ds:\n",
    "                    lbl = 'pt-BR' if ex['label'] == 1 else 'pt-PT'\n",
    "                    br  = ex['text'] if lbl == 'pt-BR' else None\n",
    "                    pt  = ex['text'] if lbl == 'pt-PT' else None\n",
    "                    buf.append(('PtBrVId-Raw', domain, split, lbl, br, pt))\n",
    "                    n_in += 1\n",
    "                    if len(buf) >= BATCH:\n",
    "                        df = pd.DataFrame(buf, columns=['dataset','domain','split','label','text_pt_br','text_pt_pt'])\n",
    "                        con.register('buf', df)\n",
    "                        con.execute(\"INSERT INTO ptbrvarid SELECT * FROM buf\")\n",
    "                        con.unregister('buf'); n_new += len(buf); buf.clear(); gc.collect()\n",
    "                if buf:\n",
    "                    df = pd.DataFrame(buf, columns=['dataset','domain','split','label','text_pt_br','text_pt_pt'])\n",
    "                    con.register('buf', df)\n",
    "                    con.execute(\"INSERT INTO ptbrvarid SELECT * FROM buf\")\n",
    "                    con.unregister('buf'); n_new += len(buf); buf.clear(); gc.collect()\n",
    "                print(f\"✓ {domain}/{split}: read {n_in:,}, wrote {n_new:,}\")\n",
    "\n",
    "ingest_ptbrvid_raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b17d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_punkt():\n",
    "    \"\"\"\n",
    "    Ensure Portuguese punkt is available and visible to this kernel.\n",
    "    Newer NLTK may also require 'punkt_tab'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "    except LookupError:\n",
    "        # download into the same directory we added to nltk.data.path\n",
    "        nltk.download(\"punkt\", download_dir=NLTK_USER_DIR, quiet=True)\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt_tab\", download_dir=NLTK_USER_DIR, quiet=True)\n",
    "            nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")\n",
    "\n",
    "# ----------------------------\n",
    "# Author's regex & helpers (verbatim)\n",
    "# ----------------------------\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "\n",
    "HTML_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#…])*\")\n",
    "HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
    "QUOTE_SPACE_START_RE = re.compile(r\"^\\\"\\s\")\n",
    "QUOTE_SPACE_END_RE = re.compile(r\"\\s\\\"$\")\n",
    "MENTION_RE = re.compile(r\"@(\\w+)\")\n",
    "RETWEET_RE = re.compile(r\"RT @(\\w+):\")\n",
    "COD_RE = re.compile(r\"COD _ (\\w+) \")\n",
    "BULLET_RE = re.compile(r\"^(\\d)+.\\s\")\n",
    "THREE_DASH_RE = re.compile(r\"---.*---\")\n",
    "MORE_THAN_THREE_POINTS_RE = re.compile(r\"\\.{4,}\")\n",
    "MODE = \"ptbrvarid\"   # options: \"ptbrvarid\" | \"ptradutor\"\n",
    "\n",
    "\n",
    "VALID_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzàáâãåāèéêëěėēîïíìįīĵłñńôöòóōõšśûüùúūÿýźçćčñń!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~«»“”ºª€ \\t\\n\\r\\x0b\\x0c\"\n",
    "\n",
    "INVALID_START = [\n",
    "    \"List of recent changes\",\"Sort by\",\"Home |\",\"> Home\",\"useful tips\",\"Licenses:\",\"Search in: \",\n",
    "    \"Terms of Use - \",\"Home page\",\"Home Page\",\"Copyright\",\"Results/Page\",\n",
    "    \"!\",\"#\",\"$\",\"%\",\"&\",\"*\",\"+\",\n",
    "    \",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\"=\",\n",
    "    \">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\",\n",
    "]\n",
    "INVALID_MIDDLE = [\" @ \", \" / \", \" | \", \"[...]\", \"(...)\"]\n",
    "INVALID_END = [\" (\"]\n",
    "\n",
    "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "SPEAKER_LABEL_SINGLE_LETTER = re.compile(\n",
    "    r'(?m)(^|\\s|[(\\[\"“])'          # \\1 = boundary we preserve\n",
    "    r'[A-Za-zÀ-ÖØ-öø-ÿ]\\.'         # exactly one letter + dot\n",
    "    r'\\s*(?:--|[-–—]{1,2})\\s*'     # \"--\" or one/two dashes (hyphen/en/em), with spaces\n",
    ")\n",
    "\n",
    "# Word followed by double hyphen (speaker label), preserving the boundary before it\n",
    "SPEAKER_LABEL_WORD_DASH = re.compile(\n",
    "    r'(?m)(^|\\s)[A-Za-zÀ-ÖØ-öø-ÿ]+(?:\\.)?\\s+--\\s*'\n",
    ")\n",
    "\n",
    "# Dialogue-leading double dash right after strong punctuation or line start\n",
    "DIALOGUE_LEADING_DASH = re.compile(\n",
    "    r'(?m)(^|[\\.!\\?\\:\\;…])\\s*--\\s*'\n",
    ")\n",
    "\n",
    "# Capitalize the first letter after strong punctuation or at start of text/line.\n",
    "# Keeps any opening quotes/brackets just before the letter.\n",
    "CAP_SENT_START = re.compile(\n",
    "    r'(?m)(^|[\\.!\\?\\:\\;…]\\s+)([\\\"\\'“”«»\\(\\[\\{]*)([a-zà-öø-ÿ])'\n",
    ")\n",
    "\n",
    "\n",
    "def remove_html_tags(text): return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "def remove_hashtags(text): return HASHTAG_RE.sub(\"\", text).strip()\n",
    "def remove_mentions(text): return MENTION_RE.sub(\"\", text).strip()\n",
    "def remove_retweets(text): return RETWEET_RE.sub(\"\", text).strip()\n",
    "def remove_urls(text): return URL_RE.sub(\"\", text).strip()\n",
    "def remove_cod_literature(text): return COD_RE.sub(\"\", text).strip()\n",
    "def remove_bullets(text): return BULLET_RE.sub(\"\", text).strip()\n",
    "def remove_three_dashes(text): return THREE_DASH_RE.sub(\"\", text).strip()\n",
    "def remove_quote_space_start(text): return QUOTE_SPACE_START_RE.sub('\"', text)\n",
    "def remove_quote_space_end(text):\n",
    "    if text.endswith(' \"'): return text[:-2] + '\"'\n",
    "    return text\n",
    "def has_more_than_three_points(text): return bool(MORE_THAN_THREE_POINTS_RE.search(text))\n",
    "def starts_with_month(text): return text.lower().startswith(tuple(MONTHS))\n",
    "def has_too_long_word(text): return any(word for word in text.split(\" \") if len(word) > 20)\n",
    "def has_invalid_start(text): return text.startswith(tuple(INVALID_START))\n",
    "def has_invalid_middle(text): return any(True for word in INVALID_MIDDLE if word in text)\n",
    "def has_invalid_end(text): return text.endswith(tuple(INVALID_END))\n",
    "def has_valid_brackets(text): return (text.count(\"(\")==text.count(\")\") and text.count(\"[\")==text.count(\"]\") and text.count(\"{\")==text.count(\"}\"))\n",
    "def has_valid_quotes(text): return text.count('\"') % 2 == 0 and text.count(\"“\")==text.count(\"”\")\n",
    "def is_empty(text): return len(text) == 0\n",
    "def has_invalid_character(text):\n",
    "    for char in text:\n",
    "        if char.lower() not in VALID_CHARS: return True\n",
    "    return False\n",
    "\n",
    "def normalize_double_quotes(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return (text.replace(\"«\", '\"').replace(\"»\", '\"')\n",
    "                .replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "                .replace(\"„\", '\"'))\n",
    "\n",
    "def remove_single_letter_speaker_labels(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return SPEAKER_LABEL_SINGLE_LETTER.sub(r\"\\1\", text)\n",
    "\n",
    "def remove_speaker_label_word_dash(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return SPEAKER_LABEL_WORD_DASH.sub(r'\\1', text)\n",
    "\n",
    "def remove_dialogue_leading_double_dash(text: str) -> str:\n",
    "    if not text: return text\n",
    "    return DIALOGUE_LEADING_DASH.sub(lambda m: (m.group(1) or '') + ' ', text)\n",
    "\n",
    "def capitalize_sentence_starts(text: str) -> str:\n",
    "    if not text: return text\n",
    "    m = re.match(r'^([\\\"\\'“”«»\\(\\[\\{]*)([a-zà-öø-ÿ])', text)\n",
    "    if m: text = m.group(1) + m.group(2).upper() + text[m.end():]\n",
    "    def repl(m): return (m.group(1) or '') + (m.group(2) or '') + m.group(3).upper()\n",
    "    return CAP_SENT_START.sub(repl, text)\n",
    "\n",
    "def author_transform_chain(text: str) -> str:\n",
    "    text = remove_retweets(text); text = remove_mentions(text); text = remove_hashtags(text)\n",
    "    text = remove_urls(text); text = remove_html_tags(text); text = normalize_double_quotes(text)\n",
    "    text = remove_single_letter_speaker_labels(text); text = remove_speaker_label_word_dash(text)\n",
    "    text = remove_dialogue_leading_double_dash(text); text = remove_cod_literature(text)\n",
    "    text = remove_bullets(text); text = remove_three_dashes(text)\n",
    "    text = remove_quote_space_start(text); text = remove_quote_space_end(text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    text = capitalize_sentence_starts(text)\n",
    "    return text\n",
    "\n",
    "def drop_nans_and_empties(ds):\n",
    "    return ds.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "def drop_exact_duplicates(ds, batch_size: int = 1000):\n",
    "    seen = set()\n",
    "    def tag_batch(batch):\n",
    "        keep = []\n",
    "        for t in batch[\"text\"]:\n",
    "            h = hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "            keep.append(h not in seen)\n",
    "            seen.add(h)\n",
    "        return {\"__keep__\": keep}\n",
    "    ds = ds.map(tag_batch, batched=True, batch_size=batch_size, num_proc=1)\n",
    "    ds = ds.filter(lambda k: k, input_columns=\"__keep__\").remove_columns([\"__keep__\"])\n",
    "    return ds\n",
    "\n",
    "def apply_clean_text_ascii(s: str) -> str:\n",
    "    return clean(s, fix_unicode=True, to_ascii=True, lower=False,\n",
    "                 no_line_breaks=False, no_urls=False, no_emails=False,\n",
    "                 no_phone_numbers=False, no_numbers=False, no_digits=False, no_currency_symbols=False)\n",
    "\n",
    "_FALLBACK_TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def pt_word_count(s: str) -> int:\n",
    "    try:\n",
    "        return len(nltk.word_tokenize(s, language=\"portuguese\"))\n",
    "    except LookupError:\n",
    "        return len(_FALLBACK_TOKEN_RE.findall(s))\n",
    "\n",
    "def add_length_column(ds, batch_size: int = 1000):\n",
    "    def _lens(batch): return {\"__len__\": [pt_word_count(t) for t in batch[\"text\"]]}\n",
    "    return ds.map(_lens, batched=True, batch_size=batch_size, num_proc=1)\n",
    "\n",
    "def iqr_bounds_from_lengths(lengths):\n",
    "    q1, q3 = np.percentile(lengths, 25), np.percentile(lengths, 75)\n",
    "    iqr = q3 - q1\n",
    "    return (q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "\n",
    "def apply_iqr_filter_on_cached_lengths(ds, lo, hi):\n",
    "    ds = ds.filter(lambda L: lo <= L <= hi, input_columns=\"__len__\")\n",
    "    return ds.remove_columns([\"__len__\"])\n",
    "\n",
    "def apply_clean_text_unicode_only(s: str) -> str:\n",
    "    return clean(s, fix_unicode=True, to_ascii=False, lower=False,\n",
    "                 no_line_breaks=False, no_urls=False, no_emails=False,\n",
    "                 no_phone_numbers=False, no_numbers=False, no_digits=False, no_currency_symbols=False)\n",
    "\n",
    "# ====== NEW: safe jusText wrapper + scope switch ======\n",
    "def safe_justext(text: str) -> str:\n",
    "    if not text: return text\n",
    "    try:\n",
    "        import justext\n",
    "        paras = justext.justext(text, justext.get_stoplist(\"Portuguese\"))\n",
    "        good = [p.text for p in paras if p.class_type == \"good\"]\n",
    "        return \"\\n\".join(good) if good else text\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def web_justext(text: str,\n",
    "                *,\n",
    "                drop_if_no_good: bool = True,\n",
    "                min_chars: int = 30,\n",
    "                min_ratio: float = 0.20) -> str:\n",
    "    \"\"\"\n",
    "    Run jusText and drop rows that look like boilerplate.\n",
    "    - drop_if_no_good: drop if no 'good' paragraphs\n",
    "    - min_chars: drop if cleaned text shorter than this\n",
    "    - min_ratio: drop if cleaned/original length ratio below this\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    try:\n",
    "        import justext\n",
    "        paras = justext.justext(text, justext.get_stoplist(\"Portuguese\"))\n",
    "        good  = [p.text for p in paras if p.class_type == \"good\"]\n",
    "        if not good:\n",
    "            return \"\" if drop_if_no_good else text\n",
    "        jt = \"\\n\".join(good).strip()\n",
    "        if len(jt) < min_chars:\n",
    "            return \"\"\n",
    "        if len(jt) / max(1, len(text)) < min_ratio:\n",
    "            return \"\"\n",
    "        return jt\n",
    "    except Exception:\n",
    "        return text  # conservative on parser error\n",
    "\n",
    "def clean_one_domain_split(\n",
    "    domain: str,\n",
    "    split: str,\n",
    "    use_author_transforms: bool = True,\n",
    "    run_web_justext: bool = True,   # kept for back-compat (ignored if justext_scope is set)\n",
    "    apply_author_filters: bool = True,\n",
    "    keep_accents: bool = True,\n",
    "    num_proc: int = 1,\n",
    "    batch_size: int = 1000,\n",
    "    *,\n",
    "    justext_scope: str = \"web\",     # NEW: \"none\" | \"web\" | \"all\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Same as yours, but jusText can run on: none / web / all.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"liaad/PtBrVId-Raw\", domain, split=split)\n",
    "    ds = drop_nans_and_empties(ds)\n",
    "\n",
    "    # --- jusText control (NEW) ---\n",
    "    scope = justext_scope or (\"web\" if run_web_justext else \"none\")\n",
    "    if scope == \"all\" or (scope == \"web\" and domain == \"web\"):\n",
    "        ds = ds.map(lambda x: {\"text\": web_justext(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    # (rest of your function unchanged below…)\n",
    "    if use_author_transforms:\n",
    "        ds = ds.map(lambda x: {\"text\": author_transform_chain(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    if keep_accents:\n",
    "        ds = ds.map(lambda x: {\"text\": apply_clean_text_unicode_only(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "    else:\n",
    "        ds = ds.map(lambda x: {\"text\": apply_clean_text_ascii(x[\"text\"])},\n",
    "                    num_proc=num_proc, batched=False)\n",
    "\n",
    "    ds = drop_exact_duplicates(ds, batch_size=batch_size)\n",
    "\n",
    "    if apply_author_filters:\n",
    "        ds = ds.filter(\n",
    "            lambda t: (not starts_with_month(t))\n",
    "                      and (not has_too_long_word(t))\n",
    "                      and (not has_invalid_start(t))\n",
    "                      and (not has_invalid_middle(t))\n",
    "                      and (not has_invalid_end(t))\n",
    "                      and (not has_more_than_three_points(t))\n",
    "                      and (not is_empty(t))\n",
    "                      and (not has_invalid_character(t))\n",
    "                      and has_valid_brackets(t)\n",
    "                      and has_valid_quotes(t),\n",
    "            input_columns=\"text\",\n",
    "            num_proc=num_proc\n",
    "        )\n",
    "\n",
    "    ds = add_length_column(ds, batch_size=batch_size)\n",
    "    lo, hi = iqr_bounds_from_lengths(ds[\"__len__\"])\n",
    "    ds = apply_iqr_filter_on_cached_lengths(ds, lo, hi)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe8db68",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 233\u001b[39m\n\u001b[32m    231\u001b[39m splits = get_dataset_split_names(\u001b[33m\"\u001b[39m\u001b[33mliaad/PtBrVId-Raw\u001b[39m\u001b[33m\"\u001b[39m, d)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     \u001b[43mprocess_domain_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_accents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_author_transforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_author_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m     \u001b[38;5;66;03m# add to running total\u001b[39;00m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m duckdb.connect(DB_PATH) \u001b[38;5;28;01mas\u001b[39;00m con:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mprocess_domain_split\u001b[39m\u001b[34m(domain, split, keep_accents, use_author_transforms, apply_author_filters)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# ---------- PASS 1: build stage ----------\u001b[39;00m\n\u001b[32m    104\u001b[39m ds = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mliaad/PtBrVId-Raw\u001b[39m\u001b[33m\"\u001b[39m, domain, split=split, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_n\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2538\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2535\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2538\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2055\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2048\u001b[39m     formatter = get_formatter(\n\u001b[32m   2049\u001b[39m         \u001b[38;5;28mself\u001b[39m.formatting.format_type,\n\u001b[32m   2050\u001b[39m         features=\u001b[38;5;28mself\u001b[39m._features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.is_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2051\u001b[39m         token_per_repo_id=\u001b[38;5;28mself\u001b[39m.token_per_repo_id,\n\u001b[32m   2052\u001b[39m     )\n\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   2054\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2078\u001b[39m, in \u001b[36mFormattedExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2076\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features:\n\u001b[32m   2077\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable._iter_arrow()\n\u001b[32m-> \u001b[39m\u001b[32m2078\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:539\u001b[39m, in \u001b[36mRebatchedArrowExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m     previous_state = \u001b[38;5;28mself\u001b[39m.ex_iterable.state_dict()\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state\u001b[39m\u001b[33m\"\u001b[39m] = previous_state\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_chunks_since_previous_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_chunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_chunks_to_skip\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:378\u001b[39m, in \u001b[36mArrowExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    376\u001b[39m shard_example_idx_start = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mshard_example_idx\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    377\u001b[39m shard_example_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tables_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/packaged_modules/parquet/parquet.py:166\u001b[39m, in \u001b[36mParquet._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    165\u001b[39m     parquet_fragment = parquet_file_format.make_fragment(f)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparquet_fragment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrow_groups\u001b[49m:\n\u001b[32m    167\u001b[39m         batch_size = \u001b[38;5;28mself\u001b[39m.config.batch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_fragment.row_groups[\u001b[32m0\u001b[39m].num_rows\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[32m    169\u001b[39m             parquet_fragment.to_batches(\n\u001b[32m    170\u001b[39m                 batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m             )\n\u001b[32m    176\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/pyarrow/_dataset_parquet.pyx:389\u001b[39m, in \u001b[36mpyarrow._dataset_parquet.ParquetFileFragment.row_groups.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/pyarrow/_dataset_parquet.pyx:396\u001b[39m, in \u001b[36mpyarrow._dataset_parquet.ParquetFileFragment.metadata.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/pyarrow/_dataset_parquet.pyx:385\u001b[39m, in \u001b[36mpyarrow._dataset_parquet.ParquetFileFragment.ensure_complete_metadata\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/utils/file_utils.py:824\u001b[39m, in \u001b[36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m         out = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    825\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m CONNECTION_ERRORS_TO_RETRY \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:1007\u001b[39m, in \u001b[36mHfFileSystemFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   1005\u001b[39m         \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[32m   1006\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/fsspec/spec.py:2083\u001b[39m, in \u001b[36mAbstractBufferedFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   2081\u001b[39m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[32m   2082\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2085\u001b[39m logger.debug(\n\u001b[32m   2086\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2087\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2090\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache._log_stats(),\n\u001b[32m   2091\u001b[39m )\n\u001b[32m   2092\u001b[39m \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/fsspec/caching.py:249\u001b[39m, in \u001b[36mReadAheadCache._fetch\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    247\u001b[39m end = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.size, end + \u001b[38;5;28mself\u001b[39m.blocksize)\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m.total_requested_bytes += end - start\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28mself\u001b[39m.cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28mself\u001b[39m.start = start\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.end = \u001b[38;5;28mself\u001b[39m.start + \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:961\u001b[39m, in \u001b[36mHfFileSystemFile._fetch_range\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    950\u001b[39m headers = {\n\u001b[32m    951\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrange\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    952\u001b[39m     **\u001b[38;5;28mself\u001b[39m.fs._api._build_hf_headers(),\n\u001b[32m    953\u001b[39m }\n\u001b[32m    954\u001b[39m url = hf_hub_url(\n\u001b[32m    955\u001b[39m     repo_id=\u001b[38;5;28mself\u001b[39m.resolved_path.repo_id,\n\u001b[32m    956\u001b[39m     revision=\u001b[38;5;28mself\u001b[39m.resolved_path.revision,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.fs.endpoint,\n\u001b[32m    960\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m r = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m504\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m hf_raise_for_status(r)\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/urllib3/response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:484\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    483\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    486\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# STEP 2 + METRICS (single pass per domain/split with on-disk staging)\n",
    "# - Applies jusText on ALL domains (and really drops boilerplate)\n",
    "# - Writes final rows into ptbrvarid (dataset='PtBrVId')\n",
    "# - Writes per-stage counts into ptbrvarid_metrics\n",
    "# =============================\n",
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb, pandas as pd, numpy as np, hashlib, warnings, re\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "BATCH_ROWS = 20_000\n",
    "\n",
    "# -------- jusText that actually prunes --------\n",
    "\n",
    "\n",
    "# -------- tiny helper: your heuristic predicate --------\n",
    "def _pass_filters(t: str) -> bool:\n",
    "    return (not starts_with_month(t)\n",
    "            and not has_too_long_word(t)\n",
    "            and not has_invalid_start(t)\n",
    "            and not has_invalid_middle(t)\n",
    "            and not has_invalid_end(t)\n",
    "            and not has_more_than_three_points(t)\n",
    "            and not is_empty(t)\n",
    "            and not has_invalid_character(t)\n",
    "            and has_valid_brackets(t)\n",
    "            and has_valid_quotes(t))\n",
    "\n",
    "# -------- DB setup --------\n",
    "with duckdb.connect(DB_PATH) as con:\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ptbrvarid (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            label       TEXT,\n",
    "            text_pt_br  TEXT,\n",
    "            text_pt_pt  TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ptbrvarid_metrics (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            raw                 BIGINT,\n",
    "            after_nonempty      BIGINT,\n",
    "            after_jusText       BIGINT,\n",
    "            after_author        BIGINT,\n",
    "            after_clean         BIGINT,\n",
    "            after_dedup         BIGINT,\n",
    "            after_filters       BIGINT,\n",
    "            after_IQR           BIGINT,\n",
    "            drop_empty          BIGINT,\n",
    "            drop_jusText        BIGINT,\n",
    "            drop_author         BIGINT,\n",
    "            drop_clean          BIGINT,\n",
    "            drop_dedup          BIGINT,\n",
    "            drop_filters        BIGINT,\n",
    "            drop_IQR            BIGINT,\n",
    "            IQR_lo              DOUBLE,\n",
    "            IQR_hi              DOUBLE\n",
    "        )\n",
    "    \"\"\")\n",
    "    # Clear previous processed rows + metrics for a clean rebuild\n",
    "    con.execute(\"DELETE FROM ptbrvarid WHERE dataset='PtBrVId'\")\n",
    "    con.execute(\"DELETE FROM ptbrvarid_metrics WHERE dataset='PtBrVId'\")\n",
    "\n",
    "def _safe_tbl(name: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_]\", \"_\", name)\n",
    "\n",
    "def process_domain_split(domain: str, split: str,\n",
    "                         *, keep_accents: bool = True,\n",
    "                         use_author_transforms: bool = True,\n",
    "                         apply_author_filters: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Single (domain, split) processing:\n",
    "      Pass 1: stream RAW -> jusText -> author -> clean (accents kept) -> de-dup -> filters\n",
    "              write (label, cleaned_text, len_tokens) to stage table on disk\n",
    "              collect per-stage counters\n",
    "      Compute IQR from stage lengths\n",
    "      Insert survivors into ptbrvarid\n",
    "      Save metrics, drop stage table\n",
    "    \"\"\"\n",
    "    stage = f\"__ptbr_stage_{_safe_tbl(domain)}_{_safe_tbl(split)}\"\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        con.execute(f\"DROP TABLE IF EXISTS {stage}\")\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE {stage} (\n",
    "                domain TEXT,\n",
    "                split  TEXT,\n",
    "                label  TEXT,\n",
    "                text   TEXT,\n",
    "                len_tokens BIGINT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "    raw_n = n_nonempty = n_after_jt = n_after_author = n_after_clean = n_after_dedup = n_after_filters = 0\n",
    "    seen, buf = set(), []\n",
    "\n",
    "    # ---------- PASS 1: build stage ----------\n",
    "    ds = load_dataset(\"liaad/PtBrVId-Raw\", domain, split=split, streaming=True)\n",
    "    for ex in ds:\n",
    "        raw_n += 1\n",
    "        t = (ex[\"text\"] or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        n_nonempty += 1\n",
    "\n",
    "        # jusText on ALL domains; now actually drops boilerplate\n",
    "        t = web_justext(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        n_after_jt += 1\n",
    "\n",
    "        if use_author_transforms:\n",
    "            t = author_transform_chain(t)\n",
    "            if not t:\n",
    "                continue\n",
    "        n_after_author += 1\n",
    "\n",
    "        t = apply_clean_text_unicode_only(t) if keep_accents else apply_clean_text_ascii(t)\n",
    "        t = (t or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        n_after_clean += 1\n",
    "\n",
    "        h = hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        n_after_dedup += 1\n",
    "\n",
    "        if apply_author_filters and (not _pass_filters(t)):\n",
    "            continue\n",
    "        n_after_filters += 1\n",
    "\n",
    "        L = pt_word_count(t)\n",
    "        lbl = 'pt-BR' if ex['label'] == 1 else 'pt-PT'\n",
    "        buf.append((domain, split, lbl, t, int(L)))\n",
    "\n",
    "        if len(buf) >= BATCH_ROWS:\n",
    "            with duckdb.connect(DB_PATH) as con:\n",
    "                con.executemany(f\"INSERT INTO {stage} VALUES (?,?,?,?,?)\", buf)\n",
    "            buf.clear()\n",
    "    if buf:\n",
    "        with duckdb.connect(DB_PATH) as con:\n",
    "            con.executemany(f\"INSERT INTO {stage} VALUES (?,?,?,?,?)\", buf)\n",
    "        buf.clear()\n",
    "\n",
    "    # If nothing survived pre-IQR, write metrics and return\n",
    "    with duckdb.connect(DB_PATH) as con:\n",
    "        n_stage = con.execute(f\"SELECT COUNT(*) FROM {stage}\").fetchone()[0]\n",
    "        if n_stage == 0:\n",
    "            metrics_row = ('PtBrVId', domain, split,\n",
    "                           raw_n, n_nonempty, n_after_jt, n_after_author, n_after_clean,\n",
    "                           n_after_dedup, n_after_filters, 0,\n",
    "                           raw_n-n_nonempty, n_nonempty-n_after_jt, n_after_jt-n_after_author,\n",
    "                           n_after_author-n_after_clean, n_after_clean-n_after_dedup,\n",
    "                           n_after_dedup-n_after_filters, n_after_filters-0,\n",
    "                           0.0, 0.0)\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO ptbrvarid_metrics (\n",
    "                    dataset, domain, split,\n",
    "                    raw, after_nonempty, after_jusText, after_author, after_clean,\n",
    "                    after_dedup, after_filters, after_IQR,\n",
    "                    drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "                    IQR_lo, IQR_hi\n",
    "                ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\", metrics_row)\n",
    "            con.execute(f\"DROP TABLE {stage}\")\n",
    "            print(f\"{domain}/{split}: raw={raw_n:,} → after_IQR=0 (0.0%)\")\n",
    "            return\n",
    "\n",
    "        # Compute IQR bounds on lengths\n",
    "        q1, q3 = con.execute(\n",
    "            f\"SELECT quantile_cont(len_tokens,0.25), quantile_cont(len_tokens,0.75) FROM {stage}\"\n",
    "        ).fetchone()\n",
    "        iqr = q3 - q1\n",
    "        lo = float(q1 - 1.5 * iqr)\n",
    "        hi = float(q3 + 1.5 * iqr)\n",
    "\n",
    "        # Survivors count\n",
    "        n_after_iqr = con.execute(\n",
    "            f\"SELECT COUNT(*) FROM {stage} WHERE len_tokens BETWEEN ? AND ?\", [lo, hi]\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        # Insert final rows into ptbrvarid\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO ptbrvarid\n",
    "            SELECT\n",
    "              'PtBrVId' AS dataset,\n",
    "              domain, split, label,\n",
    "              CASE WHEN label='pt-BR' THEN text ELSE NULL END AS text_pt_br,\n",
    "              CASE WHEN label='pt-PT' THEN text ELSE NULL END AS text_pt_pt\n",
    "            FROM {stage}\n",
    "            WHERE len_tokens BETWEEN ? AND ?\n",
    "        \"\"\", [lo, hi])\n",
    "\n",
    "        # Metrics row (FIXED: 20 placeholders + explicit columns)\n",
    "        metrics_row = ('PtBrVId', domain, split,\n",
    "                       raw_n, n_nonempty, n_after_jt, n_after_author, n_after_clean,\n",
    "                       n_after_dedup, n_after_filters, int(n_after_iqr),\n",
    "                       raw_n-n_nonempty, n_nonempty-n_after_jt, n_after_jt-n_after_author,\n",
    "                       n_after_author-n_after_clean, n_after_clean-n_after_dedup,\n",
    "                       n_after_dedup-n_after_filters, n_after_filters-int(n_after_iqr),\n",
    "                       lo, hi)\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO ptbrvarid_metrics (\n",
    "                dataset, domain, split,\n",
    "                raw, after_nonempty, after_jusText, after_author, after_clean,\n",
    "                after_dedup, after_filters, after_IQR,\n",
    "                drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "                IQR_lo, IQR_hi\n",
    "            ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "        \"\"\", metrics_row)\n",
    "\n",
    "        # Drop stage\n",
    "        con.execute(f\"DROP TABLE {stage}\")\n",
    "\n",
    "        pct = round(100.0 * (n_after_iqr / max(1, raw_n)), 2)\n",
    "        print(f\"{domain}/{split}: raw={raw_n:,} → after_IQR={n_after_iqr:,} ({pct}%)\")\n",
    "\n",
    "\n",
    "# -------- Run all (domains/splits discovered from -Raw) --------\n",
    "domains = get_dataset_config_names(\"liaad/PtBrVId-Raw\")\n",
    "total_final = 0\n",
    "for d in domains:\n",
    "    splits = get_dataset_split_names(\"liaad/PtBrVId-Raw\", d)\n",
    "    for s in splits:\n",
    "        process_domain_split(d, s, keep_accents=True, use_author_transforms=True, apply_author_filters=True)\n",
    "        # add to running total\n",
    "        with duckdb.connect(DB_PATH) as con:\n",
    "            n = con.execute(\"\"\"\n",
    "                SELECT COUNT(*) FROM ptbrvarid\n",
    "                WHERE dataset='PtBrVId' AND domain=? AND split=?\n",
    "            \"\"\", [d, s]).fetchone()[0]\n",
    "            total_final += int(n)\n",
    "\n",
    "print(f\"[DONE] Inserted processed rows with your pipeline + jusText (dataset='PtBrVId'): {total_final:,}\")\n",
    "\n",
    "# -------- Nice summaries you can query later --------\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    by_dom = con.execute(\"\"\"\n",
    "        SELECT domain, split, after_nonempty, after_jusText, after_author,\n",
    "               after_clean, after_dedup, after_filters, after_IQR,\n",
    "               drop_empty, drop_jusText, drop_author, drop_clean, drop_dedup, drop_filters, drop_IQR,\n",
    "               IQR_lo, IQR_hi\n",
    "        FROM ptbrvarid_metrics\n",
    "        WHERE dataset='PtBrVId'\n",
    "        ORDER BY domain, split\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"\\n=== Per-stage counts (subset) ===\")\n",
    "    display(by_dom.head(20))\n",
    "\n",
    "    totals = con.execute(\"\"\"\n",
    "        SELECT domain,\n",
    "               SUM(raw)              AS raw,\n",
    "               SUM(after_nonempty)   AS after_nonempty,\n",
    "               SUM(after_jusText)    AS after_jusText,\n",
    "               SUM(after_author)     AS after_author,\n",
    "               SUM(after_clean)      AS after_clean,\n",
    "               SUM(after_dedup)      AS after_dedup,\n",
    "               SUM(after_filters)    AS after_filters,\n",
    "               SUM(after_IQR)        AS after_IQR\n",
    "        FROM ptbrvarid_metrics\n",
    "        WHERE dataset='PtBrVId'\n",
    "        GROUP BY domain\n",
    "        ORDER BY raw DESC\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"\\n=== Domain totals ===\")\n",
    "    display(totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "process_domain_split(\"journalistic\", \"train\", keep_accents=True, use_author_transforms=True, apply_author_filters=True)\n",
    "print(\"elapsed_sec:\", round(time.time() - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1f4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "\n",
    "def _fmt_millions(x, pos):\n",
    "    return f\"{x/1_000_000:.1f}M\" if x >= 1_000_000 else f\"{int(x):,}\"\n",
    "\n",
    "# ---------- aggregate totals ----------\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    tot = con.execute(\"\"\"\n",
    "      SELECT\n",
    "        SUM(raw)              AS raw,\n",
    "        SUM(after_nonempty)   AS after_nonempty,\n",
    "        SUM(after_jusText)    AS after_jusText,\n",
    "        SUM(after_dedup)      AS after_dedup,\n",
    "        SUM(after_filters)    AS after_filters,\n",
    "        SUM(after_IQR)        AS after_IQR,\n",
    "        SUM(drop_empty)       AS drop_empty,\n",
    "        SUM(drop_jusText)     AS drop_jusText,\n",
    "        SUM(drop_dedup)       AS drop_dedup,\n",
    "        SUM(drop_filters)     AS drop_filters,\n",
    "        SUM(drop_IQR)         AS drop_IQR\n",
    "      FROM ptbrvarid_metrics\n",
    "      WHERE dataset = 'PtBrVId'\n",
    "    \"\"\").fetchdf().iloc[0]\n",
    "\n",
    "# ============================================================\n",
    "# 1) Survivors after each step\n",
    "# ============================================================\n",
    "\n",
    "survivors = pd.Series({\n",
    "    \"Raw\":           int(tot[\"raw\"]),\n",
    "    \"After jusText\": int(tot[\"after_jusText\"]),\n",
    "    \"After De-dup\":  int(tot[\"after_dedup\"]),\n",
    "    \"After Filters\": int(tot[\"after_filters\"]),\n",
    "    \"After IQR\":     int(tot[\"after_IQR\"]),\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(survivors.index, survivors.values)\n",
    "\n",
    "ax.set_ylabel(\"Documents\", fontsize=16)\n",
    "ax.set_title(\" \", fontsize=16)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = survivors.values.max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b in bars:\n",
    "    v = b.get_height()\n",
    "    ax.annotate(\n",
    "        _fmt_millions(v, None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, v),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=24)\n",
    "ax.tick_params(axis=\"y\", labelsize=24)\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 2) Deletions by step\n",
    "# ============================================================\n",
    "\n",
    "drops = pd.Series({\n",
    "    \"Empty\":    int(tot[\"drop_empty\"]),\n",
    "    \"jusText\":  int(tot[\"drop_jusText\"]),\n",
    "    \"De-dup\":   int(tot[\"drop_dedup\"]),\n",
    "    \"Filters\":  int(tot[\"drop_filters\"]),\n",
    "    \"IQR\":      int(tot[\"drop_IQR\"]),\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(20, 9))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(drops.index, drops.values)\n",
    "\n",
    "ax.set_ylabel(\"Documents removed\", fontsize=35)\n",
    "ax.set_title(\" \", fontsize=35)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = drops.values.max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b in bars:\n",
    "    v = b.get_height()\n",
    "    ax.annotate(\n",
    "        _fmt_millions(v, None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, v),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=34,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=35)\n",
    "ax.tick_params(axis=\"y\", labelsize=35)\n",
    "plt.subplots_adjust(left=0.10, right=0.98, bottom=0.22, top=0.95)\n",
    "plt.savefig(\"removals_by_step_ptbrvarid.pdf\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# 3) Per-domain view of jusText’s impact\n",
    "# ============================================================\n",
    "\n",
    "with duckdb.connect(DB_PATH, read_only=True) as con:\n",
    "    dom = con.execute(\"\"\"\n",
    "      SELECT domain,\n",
    "             SUM(after_nonempty) AS nonempty,\n",
    "             SUM(after_jusText)  AS after_jt,\n",
    "             SUM(drop_jusText)   AS drop_jt\n",
    "      FROM ptbrvarid_metrics\n",
    "      WHERE dataset = 'PtBrVId'\n",
    "      GROUP BY domain\n",
    "      ORDER BY nonempty DESC\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "plt.figure(figsize=(20, 9))\n",
    "ax = plt.gca()\n",
    "bars = ax.bar(dom[\"domain\"], dom[\"drop_jt\"])\n",
    "\n",
    "ax.set_ylabel(\"Removed by jusText\", fontsize=35)\n",
    "ax.set_title(\" \", fontsize=35)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(_fmt_millions))\n",
    "ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "\n",
    "max_val = dom[\"drop_jt\"].max()\n",
    "ax.set_ylim(0, max_val * 1.25)\n",
    "\n",
    "for b, label in zip(bars, dom[\"drop_jt\"]):\n",
    "    ax.annotate(\n",
    "        _fmt_millions(int(label), None),\n",
    "        xy=(b.get_x() + b.get_width() / 2, b.get_height()),\n",
    "        xytext=(0, 8),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=35,\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=30, ha=\"right\", fontsize=35)\n",
    "ax.tick_params(axis=\"y\", labelsize=35)\n",
    "plt.subplots_adjust(left=0.10, right=0.98, bottom=0.22, top=0.95)\n",
    "plt.savefig(\"jusText_by_domain.pdf\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2899c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2991728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n\n",
       "0  2991728"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, pandas as pd\n",
    "\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "total_rows = con.execute(\"\"\"\n",
    "-- Exact total (processed only)\n",
    "SELECT COUNT(*) AS n\n",
    "FROM ptbrvarid\n",
    "WHERE dataset = 'PtBrVId';\n",
    "\n",
    "    \"\"\").fetchdf()\n",
    "                             \n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/laiarodrigo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/laiarodrigo/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK paths: ['/home/laiarodrigo/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/share/nltk_data', '/home/laiarodrigo/repos/Thesis/thesis/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "Has package dir?  True\n",
      "['Olá', 'mundo', '!', 'Isto', 'é', 'um', 'teste', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import os, nltk, pathlib\n",
    "\n",
    "# 1) Pick a single directory and make NLTK look there\n",
    "NLTK_USER_DIR = os.path.expanduser(\"~/nltk_data\")\n",
    "os.environ[\"NLTK_DATA\"] = NLTK_USER_DIR  # ensure child processes see it too\n",
    "if NLTK_USER_DIR not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, NLTK_USER_DIR)\n",
    "\n",
    "# 2) Force-reinstall punkt + punkt_tab into that directory\n",
    "nltk.download(\"punkt\", download_dir=NLTK_USER_DIR, force=True, quiet=False)\n",
    "# Newer NLTK also needs this metadata package\n",
    "try:\n",
    "    nltk.download(\"punkt_tab\", download_dir=NLTK_USER_DIR, force=True, quiet=False)\n",
    "except Exception:\n",
    "    pass  # older NLTK won't have it\n",
    "\n",
    "# 3) Verify both the PACKAGE and the Portuguese model are visible\n",
    "print(\"NLTK paths:\", nltk.data.path)\n",
    "print(\"Has package dir? \", pathlib.Path(NLTK_USER_DIR, \"tokenizers\", \"punkt\").exists())\n",
    "nltk.data.find(\"tokenizers/punkt\")                       # should NOT raise\n",
    "nltk.data.find(\"tokenizers/punkt/portuguese.pickle\")     # should NOT raise\n",
    "\n",
    "# Quick smoke test\n",
    "from nltk import word_tokenize\n",
    "print(word_tokenize(\"Olá mundo! Isto é um teste.\", language=\"portuguese\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f7f1e",
   "metadata": {},
   "source": [
    "**//PUT ALL PROCESSED DOMAINS AND SPLITS INTO PTBRVARID TABLE AND THEN TO TRAIN AND TEST DATA TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f958b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESET] ptbrvarid recreated\n",
      "\n",
      "[DOMAIN] journalistic – 1 splits\n",
      "  [CLEAN] journalistic/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa03063b29bd48de88e418cb473f9f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3398b4ac4bfe48839b1b44a67949d705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e2ac19bf2e40b1a03bc493c63c867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e766d831cc848afa27041f1b8729982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17716c8bfe645cdba9b77d6f3958f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1842804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c22783820f409c825d38dc5defe97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/1717127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d842c501736e4414af5ba6d17ddc08bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1607201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c030ee465754c76bf121736cb39332a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1607201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] journalistic/train: wrote 1,574,068\n",
      "\n",
      "[DOMAIN] legal – 1 splits\n",
      "  [CLEAN] legal/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6180ff8e05e4adaabb15cec0eb9dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52be751ce96e4c28a1ea7923ecbc0e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6d24d22bae47d79161aa9f5f80a328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2cf483406c42cfbf997fef2d4d7cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bc936983d24304858e08a2e478fa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4302002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d314d408c0f46da922d5c8500c25e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/2099144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3919b4507164b71bd0d325763af7c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1189672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927e8e5fa3ab4b81bba000a4c1fadbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1189672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] legal/train: wrote 1,118,443\n",
      "\n",
      "[DOMAIN] literature – 1 splits\n",
      "  [CLEAN] literature/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232d97a9b84248fb9aa2d57933ce8263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f91977594e496fa935ffe17ebfb0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec647b70f234276bb0fb7a9f977fd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab4e69785dc4f6e81880eb9e3aef61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd22dc415c44e7841492b1e5eb7bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/81984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d4b4f9a02f4bbca1e10651d4695c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/59514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7ed69a20d04fa0b53c3748c63dcf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/40775 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0636861e80a14eed9dd3cb250087435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/40775 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] literature/train: wrote 39,382\n",
      "\n",
      "[DOMAIN] politics – 1 splits\n",
      "  [CLEAN] politics/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8360318d48de4e2ba8155448b9064d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372756a9c7044ea3afcbb9d9e71b4d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4184040f9b74631be3ab556e6ba5c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec2873ad1ad45909bb4c97d6c203880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4cbce13a044b2818aeafd44aa460d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafc63718cd3424d86a759f8fb1253eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/32428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa045897647d46d890d9aaffaaa04bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/24425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211209642e5b495dae3fb86f14a84feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] politics/train: wrote 22,831\n",
      "\n",
      "[DOMAIN] social_media – 1 splits\n",
      "  [CLEAN] social_media/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed2d705df634080866dfa6b4ae0fc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e2de8aaa42400799b82ad89a59677d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae97e60c46804446acacb9bb9e455bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8aa52106c5433db7621a6fa81fc01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7c95d9cb84306908688c2046bd1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2678579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930ed6bcadd84bed92983ed3ab2cd3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/262630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0150868ae8d4d788a219d15ca022a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/220895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4459d9f859d4ff992c0fafb0e3b57e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/220895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] social_media/train: wrote 219,290\n",
      "\n",
      "[DOMAIN] web – 1 splits\n",
      "  [CLEAN] web/train ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85711336e16479aa3d55dbbe6569fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641e188a365b4c5f96b5cb8883d4cfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8633cf26a5674478be2c521132cef444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450cfa6df6e34bf7ab6ee2a06c09859e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24811f0221df4b20a0e8c31cabddc9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/133664 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f30601458ff4bc992deb8864cb21ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=1):   0%|          | 0/101368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f3db27e25948749c8d2328b8735b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/18949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6047fc757077405dabebdead52957306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] web/train: wrote 17,714\n",
      "\n",
      "[ALL DONE] Inserted 2,991,728 rows total. Table now has 2,991,728 rows.\n",
      "✓ Domain integrity OK.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPTION A (canonical): use clean_one_domain_split() + strong jusText\n",
    "# Then write ALL processed domains/splits into DuckDB ptbrvarid\n",
    "# ============================================================\n",
    "\n",
    "from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "BATCH_DB = 5000\n",
    "\n",
    "# ----------------------------\n",
    "# 0) SAFETY: enforce strong jusText is actually pruning\n",
    "#    (only needed if web_justext isn't already defined above)\n",
    "# ----------------------------\n",
    "try:\n",
    "    web_justext\n",
    "except NameError:\n",
    "    def web_justext(text: str,\n",
    "                    *,\n",
    "                    drop_if_no_good: bool = True,\n",
    "                    min_chars: int = 30,\n",
    "                    min_ratio: float = 0.20) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        try:\n",
    "            import justext\n",
    "            paras = justext.justext(text, justext.get_stoplist(\"Portuguese\"))\n",
    "            good  = [p.text for p in paras if p.class_type == \"good\"]\n",
    "            if not good:\n",
    "                return \"\" if drop_if_no_good else text\n",
    "            jt = \"\\n\".join(good).strip()\n",
    "            if len(jt) < min_chars:\n",
    "                return \"\"\n",
    "            if len(jt) / max(1, len(text)) < min_ratio:\n",
    "                return \"\"\n",
    "            return jt\n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Helpers (label mapping + schema reset)\n",
    "# ----------------------------\n",
    "def _label_to_name(ds, val):\n",
    "    \"\"\"Map int/str label to a readable name using HF features when available.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        return val\n",
    "    try:\n",
    "        names = ds.features[\"label\"].names\n",
    "        if isinstance(val, int) and 0 <= val < len(names):\n",
    "            return names[val]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(val)\n",
    "\n",
    "def _is_pt_br(lbl_name: str, raw_label) -> bool:\n",
    "    \"\"\"\n",
    "    Decide BR vs PT by normalized name; fallback to your rule:\n",
    "    label 0 == pt-PT (European) → everything else BR.\n",
    "    \"\"\"\n",
    "    s = (lbl_name or \"\").strip().lower().replace(\"_\", \"-\")\n",
    "    if s in {\"pt-br\", \"br\", \"ptbr\", \"brazil\", \"brazilian\"}:\n",
    "        return True\n",
    "    if s in {\"pt-pt\", \"pt\", \"eu\", \"european\"}:\n",
    "        return False\n",
    "    return (isinstance(raw_label, int) and raw_label != 0)\n",
    "\n",
    "def reset_ptbrvarid_table(con: duckdb.DuckDBPyConnection):\n",
    "    \"\"\"\n",
    "    Drop+recreate to avoid schema drift / column-order bugs forever.\n",
    "    \"\"\"\n",
    "    con.execute(\"DROP TABLE IF EXISTS ptbrvarid;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE ptbrvarid (\n",
    "            dataset     TEXT,\n",
    "            domain      TEXT,\n",
    "            split       TEXT,\n",
    "            label       TEXT,\n",
    "            text_pt_br  TEXT,\n",
    "            text_pt_pt  TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) IMPORTANT: ensure clean_one_domain_split really drops jusText blanks\n",
    "#    If you already added a filter after jusText in the function, you can skip this.\n",
    "#    Otherwise, we defensively drop empties here too.\n",
    "# ----------------------------\n",
    "def _safe_text(s):\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Main run: process all domains/splits and insert\n",
    "# ----------------------------\n",
    "domains = get_dataset_config_names(\"liaad/PtBrVId-Raw\")\n",
    "\n",
    "with duckdb.connect(DB_PATH) as con:\n",
    "    reset_ptbrvarid_table(con)\n",
    "    print(\"[RESET] ptbrvarid recreated\")\n",
    "\n",
    "    grand_total = 0\n",
    "\n",
    "    for domain in domains:\n",
    "        splits = get_dataset_split_names(\"liaad/PtBrVId-Raw\", domain)\n",
    "        print(f\"\\n[DOMAIN] {domain} – {len(splits)} splits\")\n",
    "\n",
    "        for split in splits:\n",
    "            print(f\"  [CLEAN] {domain}/{split} ...\")\n",
    "\n",
    "            # Force jusText on ALL domains\n",
    "            ds = clean_one_domain_split(\n",
    "                domain, split,\n",
    "                keep_accents=True,\n",
    "                use_author_transforms=True,\n",
    "                apply_author_filters=True,\n",
    "                num_proc=1,\n",
    "                batch_size=1000,\n",
    "                justext_scope=\"all\",\n",
    "            )\n",
    "\n",
    "            buf = []\n",
    "            wrote = 0\n",
    "\n",
    "            for ex in ds.to_iterable_dataset():\n",
    "                text = _safe_text(ex.get(\"text\", \"\"))\n",
    "                if not text:\n",
    "                    continue  # drop blanks (including jusText-pruned rows)\n",
    "\n",
    "                lbl_name = _label_to_name(ds, ex.get(\"label\"))\n",
    "                is_br = _is_pt_br(lbl_name, ex.get(\"label\"))\n",
    "\n",
    "                buf.append({\n",
    "                    \"dataset\": \"PtBrVId\",   # keep consistent with build_scripts.py\n",
    "                    \"domain\": domain,\n",
    "                    \"split\": split,\n",
    "                    \"label\": \"pt-BR\" if is_br else \"pt-PT\",\n",
    "                    \"text_pt_br\": text if is_br else None,\n",
    "                    \"text_pt_pt\": text if not is_br else None,\n",
    "                })\n",
    "\n",
    "                if len(buf) >= BATCH_DB:\n",
    "                    df = pd.DataFrame(buf)\n",
    "                    con.register(\"ptbr_buf\", df)\n",
    "                    con.execute(\"\"\"\n",
    "                        INSERT INTO ptbrvarid (dataset, domain, split, label, text_pt_br, text_pt_pt)\n",
    "                        SELECT dataset, domain, split, label, text_pt_br, text_pt_pt FROM ptbr_buf\n",
    "                    \"\"\")\n",
    "                    con.unregister(\"ptbr_buf\")\n",
    "                    wrote += len(buf)\n",
    "                    buf.clear()\n",
    "\n",
    "            if buf:\n",
    "                df = pd.DataFrame(buf)\n",
    "                con.register(\"ptbr_buf\", df)\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO ptbrvarid (dataset, domain, split, label, text_pt_br, text_pt_pt)\n",
    "                    SELECT dataset, domain, split, label, text_pt_br, text_pt_pt FROM ptbr_buf\n",
    "                \"\"\")\n",
    "                con.unregister(\"ptbr_buf\")\n",
    "                wrote += len(buf)\n",
    "                buf.clear()\n",
    "\n",
    "            grand_total += wrote\n",
    "            print(f\"  [OK] {domain}/{split}: wrote {wrote:,}\")\n",
    "\n",
    "    final_count = con.execute(\"SELECT COUNT(*) FROM ptbrvarid\").fetchone()[0]\n",
    "    print(f\"\\n[ALL DONE] Inserted {grand_total:,} rows total. Table now has {final_count:,} rows.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) Hard integrity check: domain must be one of the known subset labels\n",
    "    # ----------------------------\n",
    "    KNOWN = (\"journalistic\",\"legal\",\"web\",\"literature\",\"politics\",\"social_media\")\n",
    "    bad = con.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM ptbrvarid\n",
    "        WHERE dataset='PtBrVId'\n",
    "          AND (domain IS NULL OR lower(trim(domain)) NOT IN {KNOWN})\n",
    "    \"\"\").fetchone()[0]\n",
    "    assert bad == 0, f\"Found {bad} rows with invalid domain!\"\n",
    "\n",
    "print(\"✓ Domain integrity OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15177eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>domain</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>text_pt_br</th>\n",
       "      <th>text_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Cardoso e Cunha \"Insatisfatório\" O resultado do referendo francês é insatisfatório, tímido e modesto. Os problemas da Comunidade Europeia exigiam, da parte de um país que sempre esteve na linha da frente, uma vitória mais clara. A vitória tangencial do \"sim\" dá que pensar. Esta margem tão pequena que deu a vitória ao \"sim\" torna imprevisível o que vai acontecer na Grã-Bretanha.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Santo Tirso de fora Apesar da intenção das principais autarquias do Ave, para já Santo Tirso quer ficar de fora deste processo. Joaquim Couto, o presidente da edilidade tirsense, não está muito convencido da interesse das empresas e prefere concessionar alguns serviços a privados. É o caso da recolha e transporte dos lixos domésticos e da gestão e exploração do abastecimento de água. \"A concessão a privados dá mais transparência à gestão da autarquia\", referiu ao PÚBLICO o edil. Quanto às empresas públicas municipais, Couto duvida da sua utilidade e pergunta mesmo se valerá a pena autonomizar sem ter garantias de que não haverá um aumento das despesas e melhorias nos serviços. De resto, o autarca garante ainda que \"a gestão privada das coisas tem-se mostrado mais eficiente para os dinheiros públicos\". Em vez da legislação que permite a criação de empresas municipais, Couto preferia ter visto aprovada legislação que aumentasse o poder das câmaras e que lhes desse \"mais autonomia financeira\". \"Neste campo, somos os últimos da Europa\", refere, garantindo ainda que as verbas distribuídas pelo Estado às autarquias deveriam \"triplicar\". É que, diz, \"nenhuma empresa funciona se não tiver uma lei de financiamento capaz\". Emília Monteiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>\"1 Rpm\" é a abreviatura para \"Uma Revolução por Minuto\", e a respectiva tradução inglesa, uma vez que os LX-90 de Rui Pregal da Cunha e Pedro Paulo Gonçalves (ex-Heróis do Mar) vão editar não um, mas dois álbuns de estreia. Lembra a estratégia dos seus colegas de editora Guns N' Roses, até porque o que permitirá diferenciar os dois discos nos escaparates serão os contrastes de o roxo numa das capas é amarelo na outra e vice-versa. Mas as comparações acabam aí: Ao contrário dos Roses, os discos dos LX-90 incluem as mesmas faixas, com a diferença de num disco elas surgirem cantadas em português e no outro em inglês. Como pode deduzir-se deste aparato, trata-se de um dos lançamentos mais ousados de sempre no capítulo da música feita no nosso país, e tem pelas próprias exigências que se vítima de uma série de acidentes de percurso. Embora o disco começasse por ser gravado nos estúdios Exit, com a produção dos ingleses Sam e Danny, estes consideraram que faltavam ali máquinas indispensáveis, e o grupo mudou-se para os estúdios de Paço de Arcos da Valentim de Carvalho. Em Setembro, com a mesma dupla de produtores, a banda voou para Londres, no intuito de ultimar as misturas finais, tendo-se então avançado o princípio deste mês como data de edição do trabalho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Luís António Mendes Nobre pode, em Outubro, por alturada 5ª edição do Festival Internacional de Música de Macau, concretizar, como confessou, \"um velho sonho\". Do Oriente, Mendes Nobre só conhece a Tailândia, onde fez férias há dois anos. Para o pianista Adriano Jordão, director artístico do certame, o concurso \"correu muito bem. À Missão de Macau chegaram 306 boletins, o que para o nível exigido nas perguntas pode considerar-se bastante bom\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Não posso dizer mais nada. Por que é que me está a fazer perguntas sobre Lisboa? Porque foi presidente da Câmara durante dez anos e nota-se que foi uma fase muito marcante da sua vida, à qual ficou afectivamente muito ligado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>A vítima, de 32 anos, era conhecida no local como alcoólica: O relatório da PJ salienta que o casal vivia em \"condições miseráveis\". O juiz de instrução criminal confirmou a detenção do suspeito, enquanto aguarda julgamento.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>O desafio começou com ajustamentos mútuos, sendo patente a vantagem dos azuis de Belém, graças à supremacia revelada pelo seu sector intermediário. Mauro Airez assume um claro papel de liderança e pouco bastou para Rodolfo Reis ver que Nascimento não estava à altura da capacidade técnica e física do argentino. A oito minutos do intervalo substituiu--o por Dreiffus, encarregando Elias da missão que Nascimento não estava a desempenhar cabalmente. Mais soltos e com uma maior ligação entre todos os sectores da equipa, os visitantes dispuseram, aliás, da primeira ocasião de golo, quando Gonçalves desperdiçou o ensejo de bater Acácio, ontem sujeito a poucas mas complicadas intervenções, normalmente geradas ou finalizadas por Mauro Airez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-BR</td>\n",
       "      <td>Suspeito que os maiores interessados em manter a semi-clandestinidade são, justamente, os delinquentes inclusive na polícia, beneficiados por propinas. A hipocrisia está no seguinte: Os governos surgem, hoje, como os maiores promotores da jogatina através de loterias, prometendo a redenção dos até agora, seu maior beneficiário foi, aliás, o deputado João Alves. Os governantes apostam na ilusão do pobre. Como se já não bastasse a gatunagem dos impostos, tiram seu dinheiro, prometendo obras sociais para esse mesmo pobre. É, na verdade, quase um estelionato: Todos que conhecem um mínimo de bastidores de administração sabem como são desperdiçados recursos sociais. Não é só.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>Eleições vão custar 180 mil contos O ministro da Administração Interna, Manuel Pereira, revelou ontem que os preparativos para as próximas eleições legislativas vão custar ao seu ministério cerca de 180 mil contos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>PtBrVId</td>\n",
       "      <td>journalistic</td>\n",
       "      <td>train</td>\n",
       "      <td>pt-PT</td>\n",
       "      <td>None</td>\n",
       "      <td>É da competência do Ministério dos Negócios Estrangeiros manter actualizado o levantamento das comunidades portuguesas emigradas ou em actividades de cooperação em outros países. Trata-se de uma tarefa delegada na Secretaria de Estado das Comunidades e executada pelas embaixadas e consulados. Na dependência do Ministério dos Negócios Estrangeiros funciona permanentemente um órgão de trabalho chamado \"serviço de protecção consular\", ao qual cabe, por mera rotina, manter actualizados os dados gerais sobre as comunidades portuguesas. Deve ser um registo que possibilite determinar rapidamente o número de cidadãos e sua localização, permitindo saber-se também se são indivíduos singulares ou famílias e se no conjunto há crianças. No planeamento básico, por regra, cabe aos adidos militares inventariar aeroportos (ou portos) de escala e viabilidade de reabastecimento de aeronaves ou navios e à diplomacia assegurar a cooperação dos países vizinhos do \"alvo\" a alcançar para a operação de protecção ou repatriamento. Foi o que se passou com a situação no Zaire, em que a diplomacia portuguesa assegurou a cooperação do Governo do Congo-Brazzaville, nas duas situações em que o \"plano de regresso\" foi activado.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset        domain  split  label  \\\n",
       "0      PtBrVId  journalistic  train  pt-PT   \n",
       "1      PtBrVId  journalistic  train  pt-PT   \n",
       "2      PtBrVId  journalistic  train  pt-PT   \n",
       "3      PtBrVId  journalistic  train  pt-PT   \n",
       "4      PtBrVId  journalistic  train  pt-PT   \n",
       "...        ...           ...    ...    ...   \n",
       "49995  PtBrVId  journalistic  train  pt-PT   \n",
       "49996  PtBrVId  journalistic  train  pt-PT   \n",
       "49997  PtBrVId  journalistic  train  pt-BR   \n",
       "49998  PtBrVId  journalistic  train  pt-PT   \n",
       "49999  PtBrVId  journalistic  train  pt-PT   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text_pt_br  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "49995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "49996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "49997  Suspeito que os maiores interessados em manter a semi-clandestinidade são, justamente, os delinquentes inclusive na polícia, beneficiados por propinas. A hipocrisia está no seguinte: Os governos surgem, hoje, como os maiores promotores da jogatina através de loterias, prometendo a redenção dos até agora, seu maior beneficiário foi, aliás, o deputado João Alves. Os governantes apostam na ilusão do pobre. Como se já não bastasse a gatunagem dos impostos, tiram seu dinheiro, prometendo obras sociais para esse mesmo pobre. É, na verdade, quase um estelionato: Todos que conhecem um mínimo de bastidores de administração sabem como são desperdiçados recursos sociais. Não é só.   \n",
       "49998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "49999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text_pt_pt  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Cardoso e Cunha \"Insatisfatório\" O resultado do referendo francês é insatisfatório, tímido e modesto. Os problemas da Comunidade Europeia exigiam, da parte de um país que sempre esteve na linha da frente, uma vitória mais clara. A vitória tangencial do \"sim\" dá que pensar. Esta margem tão pequena que deu a vitória ao \"sim\" torna imprevisível o que vai acontecer na Grã-Bretanha.  \n",
       "1                               Santo Tirso de fora Apesar da intenção das principais autarquias do Ave, para já Santo Tirso quer ficar de fora deste processo. Joaquim Couto, o presidente da edilidade tirsense, não está muito convencido da interesse das empresas e prefere concessionar alguns serviços a privados. É o caso da recolha e transporte dos lixos domésticos e da gestão e exploração do abastecimento de água. \"A concessão a privados dá mais transparência à gestão da autarquia\", referiu ao PÚBLICO o edil. Quanto às empresas públicas municipais, Couto duvida da sua utilidade e pergunta mesmo se valerá a pena autonomizar sem ter garantias de que não haverá um aumento das despesas e melhorias nos serviços. De resto, o autarca garante ainda que \"a gestão privada das coisas tem-se mostrado mais eficiente para os dinheiros públicos\". Em vez da legislação que permite a criação de empresas municipais, Couto preferia ter visto aprovada legislação que aumentasse o poder das câmaras e que lhes desse \"mais autonomia financeira\". \"Neste campo, somos os últimos da Europa\", refere, garantindo ainda que as verbas distribuídas pelo Estado às autarquias deveriam \"triplicar\". É que, diz, \"nenhuma empresa funciona se não tiver uma lei de financiamento capaz\". Emília Monteiro  \n",
       "2      \"1 Rpm\" é a abreviatura para \"Uma Revolução por Minuto\", e a respectiva tradução inglesa, uma vez que os LX-90 de Rui Pregal da Cunha e Pedro Paulo Gonçalves (ex-Heróis do Mar) vão editar não um, mas dois álbuns de estreia. Lembra a estratégia dos seus colegas de editora Guns N' Roses, até porque o que permitirá diferenciar os dois discos nos escaparates serão os contrastes de o roxo numa das capas é amarelo na outra e vice-versa. Mas as comparações acabam aí: Ao contrário dos Roses, os discos dos LX-90 incluem as mesmas faixas, com a diferença de num disco elas surgirem cantadas em português e no outro em inglês. Como pode deduzir-se deste aparato, trata-se de um dos lançamentos mais ousados de sempre no capítulo da música feita no nosso país, e tem pelas próprias exigências que se vítima de uma série de acidentes de percurso. Embora o disco começasse por ser gravado nos estúdios Exit, com a produção dos ingleses Sam e Danny, estes consideraram que faltavam ali máquinas indispensáveis, e o grupo mudou-se para os estúdios de Paço de Arcos da Valentim de Carvalho. Em Setembro, com a mesma dupla de produtores, a banda voou para Londres, no intuito de ultimar as misturas finais, tendo-se então avançado o princípio deste mês como data de edição do trabalho.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Luís António Mendes Nobre pode, em Outubro, por alturada 5ª edição do Festival Internacional de Música de Macau, concretizar, como confessou, \"um velho sonho\". Do Oriente, Mendes Nobre só conhece a Tailândia, onde fez férias há dois anos. Para o pianista Adriano Jordão, director artístico do certame, o concurso \"correu muito bem. À Missão de Macau chegaram 306 boletins, o que para o nível exigido nas perguntas pode considerar-se bastante bom\".  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Não posso dizer mais nada. Por que é que me está a fazer perguntas sobre Lisboa? Porque foi presidente da Câmara durante dez anos e nota-se que foi uma fase muito marcante da sua vida, à qual ficou afectivamente muito ligado.  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  \n",
       "49995                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A vítima, de 32 anos, era conhecida no local como alcoólica: O relatório da PJ salienta que o casal vivia em \"condições miseráveis\". O juiz de instrução criminal confirmou a detenção do suspeito, enquanto aguarda julgamento.  \n",
       "49996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      O desafio começou com ajustamentos mútuos, sendo patente a vantagem dos azuis de Belém, graças à supremacia revelada pelo seu sector intermediário. Mauro Airez assume um claro papel de liderança e pouco bastou para Rodolfo Reis ver que Nascimento não estava à altura da capacidade técnica e física do argentino. A oito minutos do intervalo substituiu--o por Dreiffus, encarregando Elias da missão que Nascimento não estava a desempenhar cabalmente. Mais soltos e com uma maior ligação entre todos os sectores da equipa, os visitantes dispuseram, aliás, da primeira ocasião de golo, quando Gonçalves desperdiçou o ensejo de bater Acácio, ontem sujeito a poucas mas complicadas intervenções, normalmente geradas ou finalizadas por Mauro Airez.  \n",
       "49997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None  \n",
       "49998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Eleições vão custar 180 mil contos O ministro da Administração Interna, Manuel Pereira, revelou ontem que os preparativos para as próximas eleições legislativas vão custar ao seu ministério cerca de 180 mil contos.  \n",
       "49999                                                             É da competência do Ministério dos Negócios Estrangeiros manter actualizado o levantamento das comunidades portuguesas emigradas ou em actividades de cooperação em outros países. Trata-se de uma tarefa delegada na Secretaria de Estado das Comunidades e executada pelas embaixadas e consulados. Na dependência do Ministério dos Negócios Estrangeiros funciona permanentemente um órgão de trabalho chamado \"serviço de protecção consular\", ao qual cabe, por mera rotina, manter actualizados os dados gerais sobre as comunidades portuguesas. Deve ser um registo que possibilite determinar rapidamente o número de cidadãos e sua localização, permitindo saber-se também se são indivíduos singulares ou famílias e se no conjunto há crianças. No planeamento básico, por regra, cabe aos adidos militares inventariar aeroportos (ou portos) de escala e viabilidade de reabastecimento de aeronaves ou navios e à diplomacia assegurar a cooperação dos países vizinhos do \"alvo\" a alcançar para a operação de protecção ou repatriamento. Foi o que se passou com a situação no Zaire, em que a diplomacia portuguesa assegurou a cooperação do Governo do Congo-Brazzaville, nas duas situações em que o \"plano de regresso\" foi activado.  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect('../data/duckdb/subs.duckdb')\n",
    "con.execute('SELECT * FROM ptbrvarid WHERE dataset=\\'PtBrVId\\' LIMIT 50000').df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f92b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x783b47bb44b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, pathlib, pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "\n",
    "con = duckdb.connect(\"../data/duckdb/subs.duckdb\")\n",
    "\n",
    "# Known domain names to distinguish real domain vs misplaced text\n",
    "KNOWN_DOMAINS = (\"journalistic\",\"legal\",\"web\",\"literature\",\"politics\",\"social_media\")\n",
    "\n",
    "con.execute(\"DROP VIEW IF EXISTS ptbrvid_repaired_v;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE VIEW ptbrvid_repaired_v AS\n",
    "WITH raw AS (\n",
    "  SELECT dataset, domain, split, label, text_pt_br, text_pt_pt\n",
    "  FROM ptbrvarid\n",
    "  WHERE dataset='PtBrVId'\n",
    "),\n",
    "norm AS (\n",
    "  SELECT\n",
    "    -- language: prefer explicit label if present, else take the literal that was stuffed into text_pt_br\n",
    "    CASE\n",
    "      WHEN lower(label) IN ('pt-br','pt-pt') THEN CASE WHEN lower(label)='pt-br' THEN 'pt-BR' ELSE 'pt-PT' END\n",
    "      WHEN text_pt_br IN ('pt-BR','pt-PT')      THEN text_pt_br\n",
    "      ELSE NULL\n",
    "    END AS lang,\n",
    "\n",
    "    -- text: prefer the proper text columns; if empty, fall back to `domain` only if it looks like text\n",
    "    CASE\n",
    "      WHEN text_pt_br IS NOT NULL AND text_pt_br NOT IN ('pt-BR','pt-PT') THEN text_pt_br\n",
    "      WHEN text_pt_pt IS NOT NULL AND text_pt_pt NOT IN ('pt-BR','pt-PT') THEN text_pt_pt\n",
    "      WHEN domain IS NOT NULL AND lower(domain) NOT IN {KNOWN_DOMAINS}\n",
    "           AND length(domain) > 40 THEN domain\n",
    "      ELSE NULL\n",
    "    END AS text,\n",
    "\n",
    "    split, domain\n",
    "  FROM raw\n",
    "),\n",
    "ok AS (\n",
    "  SELECT\n",
    "    'PtBrVId' AS dataset,\n",
    "    split,\n",
    "    lang  AS label,\n",
    "    CASE WHEN lang='pt-BR' THEN text END AS text_pt_br,\n",
    "    CASE WHEN lang='pt-PT' THEN text END AS text_pt_pt\n",
    "  FROM norm\n",
    "  WHERE lang IS NOT NULL AND text IS NOT NULL\n",
    ")\n",
    "SELECT * FROM ok;\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
