{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13_check_data\n",
        "\n",
        "Check error 1: FRMT has no rows where `text_pt_br` or `text_pt_pt` are empty (null or whitespace) in train/validation/test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2666f10b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_DB: /home/laiarodrigo/repos/Thesis/data/duckdb/subs_project.duckdb\n",
            "SOURCE_DB: /home/laiarodrigo/repos/Thesis/data/duckdb/subs.duckdb\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "def find_project_db(start: Path | None = None, max_up: int = 6) -> Path:\n",
        "    p = (start or Path.cwd()).resolve()\n",
        "    for _ in range(max_up + 1):\n",
        "        cand = p / 'data' / 'duckdb' / 'subs_project.duckdb'\n",
        "        if cand.exists():\n",
        "            return cand\n",
        "        p = p.parent\n",
        "    raise FileNotFoundError('Could not find data/duckdb/subs_project.duckdb by walking up from cwd.')\n",
        "\n",
        "PROJECT_DB = find_project_db()\n",
        "SOURCE_DB = PROJECT_DB.parent / 'subs.duckdb'\n",
        "print('PROJECT_DB:', PROJECT_DB)\n",
        "print('SOURCE_DB:', SOURCE_DB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff25d2d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ATTACH skipped: Binder Error: Unique file handle conflict: Database \"src\" is already attached with path \"/home/laiarodrigo/repos/Thesis/data/duckdb/subs.duckdb\", \n",
            "tables/views: ['_gold_src_with_rn', 'all_data', 'frmt_dev', 'frmt_dev_clean_v', 'frmt_dev_split_v', 'frmt_test', 'frmt_test_clean_v', 'gold_seen', 'gold_test', 'gold_test_v', 'movies', 'opus_cleaned', 'opus_deleted_log', 'opus_filter_simple', 'opus_foreign_blocks', 'opus_moses', 'opus_moses_backup', 'opus_moses_filtered', 'opus_moses_preview', 'opus_ops_delete', 'opus_ops_progress', 'opus_ops_update', 'opus_replacements', 'opus_source', 'ptbr_processed_v', 'ptbr_split_assign_v', 'ptbr_unique_v', 'ptbrvarid', 'ptbrvarid_jt_dropped_examples', 'ptbrvarid_metrics', 'ptbrvarid_processed_v', 'ptbrvarid_seen', 'ptbrvarid_split_v', 'ptbrvarid_text_v', 'ptbrvarid_v', 'ptbrvid_repaired_v', 'ptbrvid_repaired_v', 'subtitle_pairs', 'subtitle_pairs_2', 'test_data', 'test_data', 'test_pairs_guard', 'train_data', 'train_data']\n"
          ]
        }
      ],
      "source": [
        "con = duckdb.connect(str(PROJECT_DB), read_only=True)\n",
        "\n",
        "if SOURCE_DB.exists():\n",
        "    try:\n",
        "        con.execute(f\"ATTACH '{SOURCE_DB.as_posix()}' AS src\")\n",
        "    except Exception as e:\n",
        "        print('ATTACH skipped:', e)\n",
        "\n",
        "tables = con.execute(\"\"\"\n",
        "    SELECT table_name\n",
        "    FROM information_schema.tables\n",
        "    WHERE table_schema NOT IN ('information_schema', 'pg_catalog')\n",
        "    ORDER BY table_name\n",
        "\"\"\").df()['table_name'].tolist()\n",
        "\n",
        "print('tables/views:', tables)\n",
        "tables = ['test_data', 'train_data']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d8d5943f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "candidates with dataset/text_pt_br/text_pt_pt: ['test_data', 'train_data']\n",
            "candidates with split column: ['test_data', 'train_data']\n"
          ]
        }
      ],
      "source": [
        "def table_cols(name: str) -> set[str]:\n",
        "    info = con.execute(f\"PRAGMA table_info('{name}')\").df()\n",
        "    return set(info['name'].tolist())\n",
        "\n",
        "required = {'dataset', 'text_pt_br', 'text_pt_pt'}\n",
        "candidates = []\n",
        "for t in tables:\n",
        "    cols = table_cols(t)\n",
        "    if required.issubset(cols):\n",
        "        candidates.append(t)\n",
        "\n",
        "split_candidates = [t for t in candidates if 'split' in table_cols(t)]\n",
        "\n",
        "print('candidates with dataset/text_pt_br/text_pt_pt:', candidates)\n",
        "print('candidates with split column:', split_candidates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165582d4",
      "metadata": {},
      "source": [
        "## Quick sanity: sample rows from train_data/test_data\n",
        "\n",
        "Show 2 rows per dataset and per split to confirm columns and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3d9ded30",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m expected_cols = [\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbucket\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtheme\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext_pt_br\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtext_pt_pt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mref_pt_pt_manual\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mref_pt_pt_deepl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_rows\u001b[39m(table: \u001b[38;5;28mstr\u001b[39m) -> \u001b[43mpd\u001b[49m.DataFrame:\n\u001b[32m      7\u001b[39m     cols = table_cols(table)\n\u001b[32m      8\u001b[39m     missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m expected_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cols]\n",
            "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "expected_cols = [\n",
        "    'dataset', 'split', 'source', 'bucket', 'theme', 'domain', 'label',\n",
        "    'text_pt_br', 'text_pt_pt', 'ref_pt_pt_manual', 'ref_pt_pt_deepl'\n",
        "]\n",
        "\n",
        "def sample_rows(table: str) -> pd.DataFrame:\n",
        "    cols = table_cols(table)\n",
        "    missing = [c for c in expected_cols if c not in cols]\n",
        "    print(f\"\\n[{table}] columns={len(cols)} missing={missing}\")\n",
        "\n",
        "    has_split = 'split' in cols\n",
        "    select_parts = []\n",
        "    if 'dataset' in cols:\n",
        "        select_parts.append('dataset')\n",
        "    select_parts.append('split' if has_split else \"'test' AS split\")\n",
        "    for c in ['source','bucket','theme','domain','label','text_pt_br','text_pt_pt','ref_pt_pt_manual','ref_pt_pt_deepl']:\n",
        "        if c in cols:\n",
        "            select_parts.append(c)\n",
        "\n",
        "    select_list = ', '.join(select_parts)\n",
        "\n",
        "    q = f\"\"\"\n",
        "        WITH base AS (\n",
        "          SELECT {select_list}\n",
        "          FROM {table}\n",
        "        ),\n",
        "        ranked AS (\n",
        "          SELECT *, row_number() OVER (PARTITION BY dataset, split ORDER BY random()) AS rn\n",
        "          FROM base\n",
        "        )\n",
        "        SELECT * FROM ranked\n",
        "        WHERE rn <= 2\n",
        "        ORDER BY dataset, split, rn\n",
        "    \"\"\"\n",
        "    return con.execute(q).df()\n",
        "\n",
        "train_sample = sample_rows('train_data')\n",
        "display(train_sample)\n",
        "\n",
        "test_sample = sample_rows('test_data')\n",
        "display(test_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba3e9946",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tables' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Override these if needed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m RELATION_WITH_SPLIT = \u001b[43mtables\u001b[49m[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# e.g., 'all_data' or any table/view with a split column\u001b[39;00m\n\u001b[32m      3\u001b[39m SPLIT_RELATIONS = {}  \u001b[38;5;66;03m# e.g., {'train': 'train_data', 'validation': 'val_data', 'test': 'test_data'}\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RELATION_WITH_SPLIT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m split_candidates:\n",
            "\u001b[31mNameError\u001b[39m: name 'tables' is not defined"
          ]
        }
      ],
      "source": [
        "# Override these if needed\n",
        "RELATION_WITH_SPLIT = tables[1]  # e.g., 'all_data' or any table/view with a split column\n",
        "SPLIT_RELATIONS = {}  # e.g., {'train': 'train_data', 'validation': 'val_data', 'test': 'test_data'}\n",
        "\n",
        "if RELATION_WITH_SPLIT is None and split_candidates:\n",
        "    RELATION_WITH_SPLIT = split_candidates[0]\n",
        "\n",
        "if not RELATION_WITH_SPLIT and not SPLIT_RELATIONS:\n",
        "    # Try to infer per-split relations from names\n",
        "    for t in candidates:\n",
        "        name = t.lower()\n",
        "        if 'train' in name:\n",
        "            SPLIT_RELATIONS.setdefault('train', t)\n",
        "        elif 'val' in name or 'valid' in name or 'validation' in name:\n",
        "            SPLIT_RELATIONS.setdefault('validation', t)\n",
        "        elif 'test' in name:\n",
        "            SPLIT_RELATIONS.setdefault('test', t)\n",
        "\n",
        "print('RELATION_WITH_SPLIT:', RELATION_WITH_SPLIT)\n",
        "print('SPLIT_RELATIONS:', SPLIT_RELATIONS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da32d014",
      "metadata": {},
      "outputs": [],
      "source": [
        "EMPTY_BR = '(text_pt_br IS NULL OR length(trim(text_pt_br)) = 0)'\n",
        "EMPTY_PT = '(text_pt_pt IS NULL OR length(trim(text_pt_pt)) = 0)'\n",
        "ANY_EMPTY = f'({EMPTY_BR} OR {EMPTY_PT})'\n",
        "BOTH_EMPTY = f'({EMPTY_BR} AND {EMPTY_PT})'\n",
        "\n",
        "def query_counts_from_relation(relation: str, split_value: str | None = None) -> pd.DataFrame:\n",
        "    where = \"dataset = 'FRMT'\"\n",
        "    if split_value is not None:\n",
        "        where += f\" AND lower(split) = '{split_value.lower()}'\"\n",
        "        split_expr = f\"'{split_value}'\"\n",
        "    else:\n",
        "        split_expr = 'lower(split)'\n",
        "    q = f\"\"\"\n",
        "        SELECT\n",
        "          {split_expr} AS split,\n",
        "          count(*) AS n_total,\n",
        "          sum(CASE WHEN {EMPTY_BR} THEN 1 ELSE 0 END) AS n_empty_br,\n",
        "          sum(CASE WHEN {EMPTY_PT} THEN 1 ELSE 0 END) AS n_empty_pt,\n",
        "          sum(CASE WHEN {ANY_EMPTY} THEN 1 ELSE 0 END) AS n_any_empty,\n",
        "          sum(CASE WHEN {BOTH_EMPTY} THEN 1 ELSE 0 END) AS n_both_empty\n",
        "        FROM {relation}\n",
        "        WHERE {where}\n",
        "        GROUP BY 1\n",
        "        ORDER BY 1\n",
        "    \"\"\"\n",
        "    return con.execute(q).df()\n",
        "\n",
        "def query_examples_from_relation(relation: str, split_value: str | None = None, limit: int = 20) -> pd.DataFrame:\n",
        "    where = \"dataset = 'FRMT' AND (\" + ANY_EMPTY + \")\"\n",
        "    if split_value is not None:\n",
        "        where += f\" AND lower(split) = '{split_value.lower()}'\"\n",
        "    q = f\"\"\"\n",
        "        SELECT dataset, split, text_pt_br, text_pt_pt\n",
        "        FROM {relation}\n",
        "        WHERE {where}\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "    return con.execute(q).df()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7240b67a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>n_total</th>\n",
              "      <th>n_empty_br</th>\n",
              "      <th>n_empty_pt</th>\n",
              "      <th>n_any_empty</th>\n",
              "      <th>n_both_empty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>2502</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>valid</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split  n_total  n_empty_br  n_empty_pt  n_any_empty  n_both_empty\n",
              "0  train     2502         0.0         0.0          0.0           0.0\n",
              "1  valid       20         0.0         0.0          0.0           0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: no empty text rows found for FRMT in the selected splits.\n"
          ]
        }
      ],
      "source": [
        "if RELATION_WITH_SPLIT:\n",
        "    # Single relation that includes a split column\n",
        "    res = query_counts_from_relation(RELATION_WITH_SPLIT)\n",
        "else:\n",
        "    # Separate relations for each split\n",
        "    frames = []\n",
        "    for split_name, rel in SPLIT_RELATIONS.items():\n",
        "        frames.append(query_counts_from_relation(rel, split_value=split_name))\n",
        "    res = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "\n",
        "display(res)\n",
        "\n",
        "if not res.empty and (res['n_any_empty'] > 0).any():\n",
        "    print('Found empty text rows. Showing examples:')\n",
        "    if RELATION_WITH_SPLIT:\n",
        "        display(query_examples_from_relation(RELATION_WITH_SPLIT))\n",
        "    else:\n",
        "        for split_name, rel in SPLIT_RELATIONS.items():\n",
        "            df_ex = query_examples_from_relation(rel, split_value=split_name)\n",
        "            if not df_ex.empty:\n",
        "                print('Split:', split_name)\n",
        "                display(df_ex)\n",
        "else:\n",
        "    print('OK: no empty text rows found for FRMT in the selected splits.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57230f0c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "932a7841",
      "metadata": {},
      "source": [
        "## Check identical pt-BR/pt-PT pairs by dataset/split (excluding PtBrVId)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8a6b83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize for comparison\n",
        "def _norm_sql(col: str) -> str:\n",
        "    return f\"lower(trim({col}))\"\n",
        "\n",
        "# train_data (train/valid)\n",
        "train_same = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        split,\n",
        "        dataset,\n",
        "        COUNT(*) AS n_rows,\n",
        "        SUM(CASE\n",
        "              WHEN text_pt_br IS NOT NULL AND text_pt_pt IS NOT NULL\n",
        "               AND length(trim(text_pt_br))>0 AND length(trim(text_pt_pt))>0\n",
        "               AND lower(trim(text_pt_br)) = lower(trim(text_pt_pt))\n",
        "              THEN 1 ELSE 0 END) AS n_same\n",
        "    FROM train_data\n",
        "    WHERE dataset <> 'PtBrVId'\n",
        "    GROUP BY split, dataset\n",
        "    ORDER BY split, dataset\n",
        "\"\"\").df()\n",
        "\n",
        "# test_data (test)\n",
        "test_same = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        'test' AS split,\n",
        "        dataset,\n",
        "        COUNT(*) AS n_rows,\n",
        "        SUM(CASE\n",
        "              WHEN text_pt_br IS NOT NULL AND text_pt_pt IS NOT NULL\n",
        "               AND length(trim(text_pt_br))>0 AND length(trim(text_pt_pt))>0\n",
        "               AND lower(trim(text_pt_br)) = lower(trim(text_pt_pt))\n",
        "              THEN 1 ELSE 0 END) AS n_same\n",
        "    FROM test_data\n",
        "    WHERE dataset <> 'PtBrVId'\n",
        "    GROUP BY dataset\n",
        "    ORDER BY dataset\n",
        "\"\"\").df()\n",
        "\n",
        "import pandas as pd\n",
        "same_stats = pd.concat([train_same, test_same], ignore_index=True)\n",
        "same_stats['pct_same'] = (same_stats['n_same'] / same_stats['n_rows'] * 100).round(3)\n",
        "same_stats\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
