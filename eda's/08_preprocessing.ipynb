{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84263414",
   "metadata": {},
   "source": [
    "**//IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facf6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiarodrigo/repos/Thesis/thesis/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd, pathlib, itertools, textwrap, re, gc\n",
    "import numpy as np\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from rapidfuzz import fuzz, distance\n",
    "from simalign import SentenceAligner\n",
    "from typing import Optional, Dict, Any, Tuple, List, Iterable, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951770de",
   "metadata": {},
   "source": [
    "**//CONFIGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4085ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = '../data/duckdb/subs.duckdb'\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>|\\{[^}]+\\}')\n",
    "NL_RE  = re.compile(r'\\s*\\n\\s*')\n",
    "SENT_SPLIT_RE = re.compile(r'(?<=[\\.\\?\\!…])\\s+')\n",
    "\n",
    "ABBREVS = {\"dr\",\"dra\",\"sr\",\"sra\",\"srta\",\"prof\",\"profa\",\"etc\",\"av\",\"nº\",\"n.º\",\"vs\",\"p.ex\"}\n",
    "ABBR_RX = re.compile(r'\\b(' + '|'.join(re.escape(x) for x in ABBREVS) + r')\\.', re.IGNORECASE)\n",
    "\n",
    "# PREPROCESSING\n",
    "HARD_STOPS = \".?!…\"\n",
    "SOFT_TAILS = \",;:—–-\"\n",
    "DASHES     = \"-–—\"\n",
    "\n",
    "LETTER      = r\"[^\\W\\d_]\"                                   # any letter, no digits/underscore\n",
    "NAME_TOKEN  = rf\"{LETTER}(?:{LETTER}|[.'\\-])*\"              # e.g., Steve, O'Neill, João-Pedro\n",
    "NAME_PHRASE = rf\"{NAME_TOKEN}(?:\\s+{NAME_TOKEN}){{0,2}}\"    # up to 3-word names\n",
    "\n",
    "WS = r\"(?:\\s|\\u00A0|\\u202F)*\"                               # normal/narrow/nbsp\n",
    "SPEAKER_LABEL_DROP = re.compile(\n",
    "    rf\"(^|[^\\w]){NAME_PHRASE}{WS}[:\\uFF1A]{WS}\",            # keep boundary, drop label\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "WORD = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
    "# Lightweight PT stopword set for \"content-token\" accounting\n",
    "PT_PREPS = {\"de\",\"do\",\"da\",\"dos\",\"das\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\"com\",\"para\",\"por\",\"a\",\"ao\",\"à\",\"às\",\"aos\"}\n",
    "PT_DETS  = {\"o\",\"a\",\"os\",\"as\",\"um\",\"uma\",\"uns\",\"umas\",\"este\",\"esta\",\"estes\",\"estas\",\"esse\",\"essa\",\"esses\",\"essas\",\"aquele\",\"aquela\",\"aqueles\",\"aquelas\"}\n",
    "PT_CLITICS={\"me\",\"te\",\"se\",\"lhe\",\"nos\",\"vos\",\"lhes\"}\n",
    "PT_CONJ  = {\"e\",\"ou\",\"mas\",\"nem\",\"que\",\"porque\",\"pois\",\"porém\",\"porem\"}\n",
    "PT_NEG   = {\"não\",\"nao\"}\n",
    "PT_STOPWORDS = (PT_PREPS | PT_DETS | PT_CLITICS | PT_CONJ | PT_NEG)\n",
    "\n",
    "# sentence splitter (no look-behind) for light stats only\n",
    "_SENT_RE = re.compile(r'.*?[.!?…]+(?:[\"”»\\'\\)\\]\\}]+)?(?=\\s|$)|.+?(?=\\s|$)', re.UNICODE)\n",
    "\n",
    "ALIGN_METHOD = \"inter\"  # alternatives: \"inter\" (↑recall), \"mwmf\", \"itermax\", \"union\"\n",
    "\n",
    "# === alignment tokenizer (use ONLY for SimAlign) ===\n",
    "WS_TOKEN = re.compile(r\"\\S+\", re.UNICODE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- cleaning + similarity ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return TAG_RE.sub('', NL_RE.sub(' ', s)).strip()\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    a = clean_text(a); b = clean_text(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    # edit-distance core signals (all penalize insertions/deletions)\n",
    "    s_edit  = fuzz.ratio(a, b) / 100.0\n",
    "    s_sort  = fuzz.token_sort_ratio(a, b) / 100.0         # order-insensitive but still length-aware\n",
    "    s_lev   = distance.Levenshtein.normalized_similarity(a, b)  # 0..1\n",
    "\n",
    "    # penalize big length mismatches (e.g., a is much longer than b)\n",
    "    lp = min(len(a), len(b)) / max(len(a), len(b))  # 0..1\n",
    "\n",
    "    # blend; weights are tame and easy to tune\n",
    "    base = 0.5*s_edit + 0.2*s_sort + 0.3*s_lev\n",
    "    return base * (0.5 + 0.5*lp)   # shrink score when lengths differ a lot\n",
    "\n",
    "# ---------- clause split (sentences first, comma/dash fallback) ----------\n",
    "def mask_abbrevs(t: str) -> str: return ABBR_RX.sub(lambda m: m.group(1)+\"§\", t or \"\")\n",
    "def unmask_abbrevs(t: str) -> str: return (t or \"\").replace(\"§\",\".\")\n",
    "\n",
    "def sentence_split(t: str):\n",
    "    tt = mask_abbrevs(t or \"\")\n",
    "    parts = [unmask_abbrevs(p).strip() for p in SENT_SPLIT_RE.split(tt.strip()) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def split_tail_clause(text: str, max_tail_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = ' '.join(parts[:-1]).strip(); tail = parts[-1].strip()\n",
    "        if head and tail: return head, tail\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.rfind(token)\n",
    "        if k != -1 and 1 <= len(t) - (k+len(token)) <= max_tail_chars:\n",
    "            return t[:k].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def split_head_clause(text: str, max_head_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = parts[0].strip(); rest = ' '.join(parts[1:]).strip()\n",
    "        if head and rest: return head, rest\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.find(token)\n",
    "        if k != -1 and 1 <= k+1 <= max_head_chars:\n",
    "            return t[:k+len(token)].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def ok_piece(seg: str, min_chars=6, min_tokens=2):\n",
    "    toks = [w for w in re.findall(r'\\b\\w+\\b', seg or \"\", flags=re.UNICODE) if any(c.isalpha() for c in w)]\n",
    "    return bool(seg) and len(seg) >= min_chars and len(toks) >= min_tokens\n",
    "\n",
    "def _py_int(x):\n",
    "    # robust cast for numpy/pandas scalars and plain ints\n",
    "    if isinstance(x, (np.generic,)):  # np.int64, np.int32, etc.\n",
    "        return int(x.item())\n",
    "    return int(x)\n",
    "\n",
    "def load_opus_window(start_line: int, window: int = 600) -> pd.DataFrame:\n",
    "    start_line = _py_int(start_line)\n",
    "    window     = _py_int(window)\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [start_line, start_line + window - 1]).df()\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "\n",
    "# ---------- PASS A: neighbor MOVES (choose tail→next or head←next if it increases sum) ----------\n",
    "def apply_neighbor_moves(df: pd.DataFrame, margin=0.04, max_clause_chars=60):\n",
    "    df2 = df.copy()\n",
    "    log = []\n",
    "    n = len(df2)\n",
    "    for i in range(n-1):\n",
    "        for lang, other in ((\"sent_pt_pt\",\"sent_pt_br\"), (\"sent_pt_br\",\"sent_pt_pt\")):\n",
    "            L_i,  L_ip1  = df2.at[i,lang],     df2.at[i+1,lang]\n",
    "            R_i,  R_ip1  = df2.at[i,other],    df2.at[i+1,other]\n",
    "            keep_sum = sim(L_i,R_i) + sim(L_ip1,R_ip1)\n",
    "\n",
    "            # option 1: move tail of i -> front of i+1\n",
    "            head, tail = split_tail_clause(L_i, max_tail_chars=max_clause_chars)\n",
    "            gain1 = -1e9\n",
    "            if ok_piece(tail) and ok_piece(head, min_chars=4):\n",
    "                move_sum1 = sim(head, R_i) + sim((tail + \" \" + (L_ip1 or \"\")).strip(), R_ip1)\n",
    "                gain1 = move_sum1 - keep_sum\n",
    "\n",
    "            # option 2: move head of i+1 -> end of i\n",
    "            head2, rest2 = split_head_clause(L_ip1, max_head_chars=max_clause_chars)\n",
    "            gain2 = -1e9\n",
    "            if ok_piece(head2) and ok_piece(rest2, min_chars=4):\n",
    "                move_sum2 = sim(((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip(), R_i) + sim(rest2, R_ip1)\n",
    "                gain2 = move_sum2 - keep_sum\n",
    "\n",
    "            # apply the better positive option\n",
    "            if gain1 > margin and gain1 >= gain2:\n",
    "                df2.at[i,lang]     = head\n",
    "                df2.at[i+1,lang]   = (tail + \" \" + (L_ip1 or \"\")).strip()\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"tail_to_next\", \"gain\": float(gain1)})\n",
    "            elif gain2 > margin and gain2 > gain1:\n",
    "                df2.at[i,lang]     = (((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip())\n",
    "                df2.at[i+1,lang]   = rest2\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"head_from_next\", \"gain\": float(gain2)})\n",
    "            # else: no move\n",
    "    return df2, pd.DataFrame(log)\n",
    "\n",
    "# # ---------- example run on a tiny window ----------\n",
    "# df  = load_opus_window(start_line=13580016, window=10)\n",
    "\n",
    "# # A) move commas/clauses across neighbors when it helps the two-row sum\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "\n",
    "# print(\"Moves:\", len(move_log))\n",
    "# print(move_log.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f006fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _moved_piece(bi, ai, bip1, aip1, op):\n",
    "    \"\"\"Best-effort extract of the moved fragment from before/after strings.\"\"\"\n",
    "    if op == \"tail_to_next\":\n",
    "        # ai = head; piece = suffix removed from bi\n",
    "        if bi.startswith(ai):\n",
    "            return bi[len(ai):].strip()\n",
    "        # fallback: prefix added to next\n",
    "        added = max(0, len(aip1) - len(bip1))\n",
    "        return aip1[:added].strip()\n",
    "    else:  # \"head_from_next\"\n",
    "        # aip1 = rest; piece = prefix removed from bip1\n",
    "        if len(bip1) > len(aip1):\n",
    "            return bip1[:len(bip1) - len(aip1)].strip()\n",
    "        # fallback: suffix added to i\n",
    "        if ai.startswith(bi):\n",
    "            return ai[len(bi):].strip()\n",
    "        return \"\"\n",
    "\n",
    "def preview_moves(df_before, df_after, move_log, k=8):\n",
    "    \"\"\"\n",
    "    Show top-k moves with before/after texts, moved fragment, and score deltas.\n",
    "    Assumes df_before/df_after are the same window (same line_no order).\n",
    "    \"\"\"\n",
    "    if move_log is None or move_log.empty:\n",
    "        print(\"No moves to preview.\")\n",
    "        return\n",
    "\n",
    "    log = move_log.sort_values(\"gain\", ascending=False).head(k)\n",
    "\n",
    "    for _, r in log.iterrows():\n",
    "        i   = int(r[\"i\"])\n",
    "        op  = r[\"op\"]\n",
    "        lang = r[\"lang\"]\n",
    "        other = \"sent_pt_pt\" if lang == \"sent_pt_br\" else \"sent_pt_br\"\n",
    "\n",
    "        # pull rows\n",
    "        bi   = df_before.at[i,   lang]\n",
    "        bip1 = df_before.at[i+1, lang]\n",
    "        ai   = df_after.at[i,    lang]\n",
    "        aip1 = df_after.at[i+1,  lang]\n",
    "\n",
    "        Ri   = df_before.at[i,   other]\n",
    "        Rip1 = df_before.at[i+1, other]  # other side doesn't change during move\n",
    "\n",
    "        piece = _moved_piece(bi, ai, bip1, aip1, op)\n",
    "\n",
    "        keep_sum = sim(bi, Ri) + sim(bip1, Rip1)\n",
    "        new_sum  = sim(ai, Ri) + sim(aip1, Rip1)\n",
    "        d_i   = sim(ai, Ri)   - sim(bi, Ri)\n",
    "        d_ip1 = sim(aip1, Rip1) - sim(bip1, Rip1)\n",
    "\n",
    "        line_i   = int(df_before.at[i,   \"line_no\"])\n",
    "        line_ip1 = int(df_before.at[i+1, \"line_no\"])\n",
    "\n",
    "        print(\"\\n────────────────────────────────────────\")\n",
    "        print(f\"lines {line_i} → {line_ip1} | {lang} | {op} | gain {float(r['gain']):.3f}\")\n",
    "        print(f\"moved piece: [{piece}]\")\n",
    "        print(f\"sum sim: {keep_sum:.3f} → {new_sum:.3f}  (Δi={d_i:+.3f}, Δi+1={d_ip1:+.3f})\")\n",
    "\n",
    "        print(\"\\n— BEFORE —\")\n",
    "        print(f\"i   ({lang}): {bi}\")\n",
    "        print(f\"i+1 ({lang}): {bip1}\")\n",
    "        print(f\"i   ({other}): {Ri}\")\n",
    "        print(f\"i+1 ({other}): {Rip1}\")\n",
    "\n",
    "        print(\"\\n— AFTER —\")\n",
    "        print(f\"i   ({lang}): {ai}\")\n",
    "        print(f\"i+1 ({lang}): {aip1}\")\n",
    "\n",
    "# # Usage example (with what you already computed):\n",
    "# df = load_opus_window(start_line=1750340, window=50)\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "# preview_moves(df, moved_df, move_log, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937373b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>sent_pt_br</th>\n",
       "      <th>sent_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6766357</td>\n",
       "      <td>6766358</td>\n",
       "      <td>Muito bem, OK. Ok? Ok.</td>\n",
       "      <td>Está. Bem, tudo bem. Está bem?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6766360</td>\n",
       "      <td>6766361</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Tudo bem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6766361</td>\n",
       "      <td>6766362</td>\n",
       "      <td>Não, não. Rosa, escute.</td>\n",
       "      <td>Adeusinho. Não, não. Rosa, ouça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6766363</td>\n",
       "      <td>6766364</td>\n",
       "      <td>Preciso encontrar a senhora Lieberman. OK.</td>\n",
       "      <td>Preciso de encontrar a Sra. Lieberman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6766365</td>\n",
       "      <td>6766366</td>\n",
       "      <td>Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.</td>\n",
       "      <td>Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6766368</td>\n",
       "      <td>6766369</td>\n",
       "      <td>Ok.</td>\n",
       "      <td>Pronto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6766369</td>\n",
       "      <td>6766370</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Pronto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6766370</td>\n",
       "      <td>6766371</td>\n",
       "      <td>Gracias.</td>\n",
       "      <td>Adeusinho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6766371</td>\n",
       "      <td>6766372</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Gracias.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6766372</td>\n",
       "      <td>6766373</td>\n",
       "      <td>Devolva a bola de gude!</td>\n",
       "      <td>Dá-me esse berlinde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6766373</td>\n",
       "      <td>6766374</td>\n",
       "      <td>É do meu pai! Me dê!</td>\n",
       "      <td>É do meu pai! Dá-mo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6766374</td>\n",
       "      <td>6766375</td>\n",
       "      <td>É a bola de gude do meu pai.</td>\n",
       "      <td>É o berlinde do meu pai.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  \\\n",
       "0   6766357  6766358   \n",
       "1   6766360  6766361   \n",
       "2   6766361  6766362   \n",
       "3   6766363  6766364   \n",
       "4   6766365  6766366   \n",
       "5   6766368  6766369   \n",
       "6   6766369  6766370   \n",
       "7   6766370  6766371   \n",
       "8   6766371  6766372   \n",
       "9   6766372  6766373   \n",
       "10  6766373  6766374   \n",
       "11  6766374  6766375   \n",
       "\n",
       "                                                                       sent_pt_br  \\\n",
       "0                                                          Muito bem, OK. Ok? Ok.   \n",
       "1                                                                          Adeus.   \n",
       "2                                                         Não, não. Rosa, escute.   \n",
       "3                                      Preciso encontrar a senhora Lieberman. OK.   \n",
       "4   Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.   \n",
       "5                                                                             Ok.   \n",
       "6                                                                          Adeus.   \n",
       "7                                                                        Gracias.   \n",
       "8                                                                          Adeus.   \n",
       "9                                                         Devolva a bola de gude!   \n",
       "10                                                           É do meu pai! Me dê!   \n",
       "11                                                   É a bola de gude do meu pai.   \n",
       "\n",
       "                                                                                 sent_pt_pt  \n",
       "0                                                            Está. Bem, tudo bem. Está bem?  \n",
       "1                                                                                 Tudo bem.  \n",
       "2                                                          Adeusinho. Não, não. Rosa, ouça.  \n",
       "3                                                    Preciso de encontrar a Sra. Lieberman.  \n",
       "4   Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".  \n",
       "5                                                                                   Pronto.  \n",
       "6                                                                                   Pronto.  \n",
       "7                                                                                Adeusinho.  \n",
       "8                                                                                  Gracias.  \n",
       "9                                                                      Dá-me esse berlinde.  \n",
       "10                                                                     É do meu pai! Dá-mo!  \n",
       "11                                                                 É o berlinde do meu pai.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preview_window_final_df(\n",
    "    start_line: int,\n",
    "    window: int = 50,\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run apply_neighbor_moves repeatedly (moves only) on a window until no more moves\n",
    "    or max_iters is reached. Return a DF with the same columns as opus_moses\n",
    "    (line_no, pair_id, sent_pt_br, sent_pt_pt) reflecting the FINAL subtitles.\n",
    "    \"\"\"\n",
    "    # load the original window\n",
    "    cur = load_opus_window(start_line=start_line, window=window)\n",
    "\n",
    "    # iterate moves to convergence\n",
    "    for _ in range(max_iters):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "\n",
    "    # return only the opus_moses columns, in order\n",
    "    return cur.loc[:, [\"line_no\", \"pair_id\", \"sent_pt_br\", \"sent_pt_pt\"]].copy()\n",
    "\n",
    "final_df = preview_window_final_df(start_line=6766355, window=20, margin=0.04, max_clause_chars=60, max_iters=5)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569388ce",
   "metadata": {},
   "source": [
    "**//PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa95ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_speaker_labels_keep_content(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    # normalize space variants first\n",
    "    s = (s.replace(\"\\u00A0\", \" \")\n",
    "           .replace(\"\\u202F\", \" \")\n",
    "           .replace(\"\\u2007\", \" \")\n",
    "           .replace(\"\\u2009\", \" \"))\n",
    "\n",
    "    # keep the boundary, drop the label\n",
    "    s = SPEAKER_LABEL_DROP.sub(r\"\\1\", s)\n",
    "\n",
    "    # tidy spacing\n",
    "    s = re.sub(r\"\\s+([,.;:!?…)\\]\\}}])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([(\\[\\{{«“\\\"'])\\s+\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _rstrip_quotes(s): return re.sub(r'[\\s\"\\']+$', '', s or \"\")\n",
    "def _last_char(s): \n",
    "    t = _rstrip_quotes(s or \"\").rstrip()\n",
    "    return t[-1:] if t else \"\"\n",
    "def _first_alpha_case(s):\n",
    "    for ch in (s or \"\"):\n",
    "        if ch.isalpha(): return \"upper\" if ch.isupper() else \"lower\"\n",
    "    return None\n",
    "\n",
    "def _starts_with_dash(s): return bool(re.match(r'^\\s*['+re.escape(DASHES)+r']\\s*', s or \"\"))\n",
    "def _strip_leading_dash(s): return re.sub(r'^\\s*['+re.escape(DASHES)+r']\\s*', '', s or \"\")\n",
    "def _remove_dash_after_punct(s): return re.sub(r'([,\\.!\\?])\\s*['+re.escape(DASHES)+r']\\s*', r'\\1 ', s or \"\")\n",
    "\n",
    "def _normalize_spaces(s):\n",
    "    s = (s or \"\").replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\s+([,.;:?!…])', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "def _is_all_caps_alpha(s):\n",
    "    letters = [c for c in (s or \"\") if c.isalpha()]\n",
    "    return bool(letters) and all(c.isupper() for c in letters)\n",
    "\n",
    "def _sentence_case_from_lower(s):\n",
    "    t, out, cap = (s or \"\").lower(), [], True\n",
    "    for ch in t:\n",
    "        if cap and ch.isalpha(): out.append(ch.upper()); cap=False\n",
    "        else: out.append(ch)\n",
    "        if ch in HARD_STOPS: cap=True\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _capitalize_first_alpha(s):\n",
    "    if not s: return s\n",
    "    chars=list(s)\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch.isalpha(): chars[i]=ch.upper(); break\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def _normalize_line_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = _strip_leading_dash(s)\n",
    "    s = _drop_speaker_labels_keep_content(s)\n",
    "    s = _remove_dash_after_punct(s)\n",
    "    s = _normalize_spaces(s)\n",
    "    if _is_all_caps_alpha(s):\n",
    "        s = _sentence_case_from_lower(s)\n",
    "    return s\n",
    "\n",
    "def _join_text(a,b):\n",
    "    b2 = _strip_leading_dash(b).lstrip()\n",
    "    if not a: return b2\n",
    "    if not b2: return a\n",
    "    return (a.rstrip() + \" \" + b2).strip()\n",
    "\n",
    "def _has_inner_hard_stop(s):  # two sentences in one row\n",
    "    return bool(re.search(r'[.?!…].+\\S.*[.?!…]', s or \"\"))\n",
    "\n",
    "def _should_merge_pair(br_a, br_b, pt_a, pt_b):\n",
    "    a_end_br = _last_char(br_a); a_end_pt = _last_char(pt_a)\n",
    "    b_head_br = _first_alpha_case(br_b); b_head_pt = _first_alpha_case(pt_b)\n",
    "    hard_br = a_end_br in HARD_STOPS; hard_pt = a_end_pt in HARD_STOPS\n",
    "\n",
    "    cont_br = ((a_end_br in SOFT_TAILS) or (len(br_a) < 40)) and (b_head_br==\"lower\" or _starts_with_dash(br_b))\n",
    "    cont_pt = ((a_end_pt in SOFT_TAILS) or (len(pt_a) < 40)) and (b_head_pt==\"lower\" or _starts_with_dash(pt_b))\n",
    "\n",
    "    underseg = (_has_inner_hard_stop(pt_a) and not _has_inner_hard_stop(br_a)) or \\\n",
    "               (_has_inner_hard_stop(br_a) and not _has_inner_hard_stop(pt_a))\n",
    "\n",
    "    if hard_br and hard_pt and (b_head_br==\"upper\") and (b_head_pt==\"upper\") and not underseg:\n",
    "        return False\n",
    "    return bool(cont_br or cont_pt or underseg)\n",
    "\n",
    "\n",
    "\n",
    "def plan_ops_over_corpus(block_size=50_000, reset=False):\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        lo, hi = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        # reset this run (only when you want a fresh start)\n",
    "        if reset:\n",
    "            con.execute(\"DELETE FROM opus_ops_update\")\n",
    "            con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "            con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "            con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "        # resume point\n",
    "        done = int(con.execute(\"SELECT done_through FROM opus_ops_progress\").fetchone()[0])\n",
    "        cur  = max(lo, done + 1)\n",
    "\n",
    "        carry = None\n",
    "        last_br, last_pt = None, None\n",
    "\n",
    "        while cur <= hi:\n",
    "            win = min(block_size, hi - cur + 1)\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [cur, cur+win-1]).df()\n",
    "\n",
    "            rows = df.to_dict(\"records\")\n",
    "            if carry is not None:\n",
    "                rows = [carry] + rows\n",
    "                carry = None\n",
    "\n",
    "            updates, deletes = [], []\n",
    "            i, n = 0, len(rows)\n",
    "            while i < n:\n",
    "                base = rows[i]; i += 1\n",
    "                br = _normalize_line_text(base[\"sent_pt_br\"])\n",
    "                pt = _normalize_line_text(base[\"sent_pt_pt\"])\n",
    "                group_lines = [int(base[\"line_no\"])]\n",
    "\n",
    "                while i < n:\n",
    "                    nxt = rows[i]\n",
    "                    br2 = _normalize_line_text(nxt[\"sent_pt_br\"])\n",
    "                    pt2 = _normalize_line_text(nxt[\"sent_pt_pt\"])\n",
    "                    if _should_merge_pair(br, br2, pt, pt2):\n",
    "                        br = _join_text(br, br2)\n",
    "                        pt = _join_text(pt, pt2)\n",
    "                        group_lines.append(int(nxt[\"line_no\"]))\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if i >= n:\n",
    "                    carry = {\"line_no\": group_lines[0], \"pair_id\": int(base[\"pair_id\"]),\n",
    "                             \"sent_pt_br\": br, \"sent_pt_pt\": pt}\n",
    "                    break\n",
    "\n",
    "                if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                    br = _capitalize_first_alpha(br)\n",
    "                if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                    pt = _capitalize_first_alpha(pt)\n",
    "\n",
    "                head = group_lines[0]\n",
    "                if br != base[\"sent_pt_br\"] or pt != base[\"sent_pt_pt\"] or len(group_lines) > 1:\n",
    "                    updates.append({\"line_no\": head, \"sent_pt_br\": br, \"sent_pt_pt\": pt})\n",
    "                for ln in group_lines[1:]:\n",
    "                    deletes.append({\"line_no\": ln})\n",
    "\n",
    "                last_br, last_pt = br, pt\n",
    "\n",
    "            if updates:\n",
    "                con.register(\"upd\", pd.DataFrame(updates))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                    SELECT line_no, sent_pt_br, sent_pt_pt FROM upd\n",
    "                    ON CONFLICT(line_no) DO UPDATE SET\n",
    "                        sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                        sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "                \"\"\")\n",
    "\n",
    "                con.unregister(\"upd\")\n",
    "            if deletes:\n",
    "                con.register(\"del\", pd.DataFrame(deletes))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_delete (line_no)\n",
    "                    SELECT DISTINCT line_no FROM del\n",
    "                    ON CONFLICT(line_no) DO NOTHING\n",
    "                \"\"\")\n",
    "\n",
    "                con.unregister(\"del\")\n",
    "\n",
    "            del df, rows, updates, deletes\n",
    "            gc.collect()\n",
    "\n",
    "            # advance + persist resume point\n",
    "            cur += win\n",
    "            con.execute(\"UPDATE opus_ops_progress SET done_through = ?\", [cur - 1])\n",
    "\n",
    "        # flush final carry (on the same connection)\n",
    "        if carry is not None:\n",
    "            if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                carry[\"sent_pt_br\"] = _capitalize_first_alpha(carry[\"sent_pt_br\"])\n",
    "            if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                carry[\"sent_pt_pt\"] = _capitalize_first_alpha(carry[\"sent_pt_pt\"])\n",
    "            con.register(\"tail_upd\", pd.DataFrame([{\n",
    "                \"line_no\": int(carry[\"line_no\"]),\n",
    "                \"sent_pt_br\": carry[\"sent_pt_br\"],\n",
    "                \"sent_pt_pt\": carry[\"sent_pt_pt\"],\n",
    "            }]))\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                SELECT line_no, sent_pt_br, sent_pt_pt FROM tail_upd\n",
    "                ON CONFLICT(line_no) DO UPDATE SET\n",
    "                    sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                    sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "            \"\"\")\n",
    "            con.unregister(\"tail_upd\")\n",
    "\n",
    "\n",
    "# def apply_ops_to_opus_moses():\n",
    "#     with duckdb.connect(str(DB)) as con:\n",
    "#         # sanity: no overlap between updates and deletes\n",
    "#         overlap = con.execute(\"\"\"\n",
    "#             SELECT COUNT(*) FROM opus_ops_update u\n",
    "#             INNER JOIN opus_ops_delete d USING (line_no)\n",
    "#         \"\"\").fetchone()[0]\n",
    "#         if overlap:\n",
    "#             raise RuntimeError(f\"{overlap} lines in BOTH update & delete; fix plan_ops first.\")\n",
    "\n",
    "#         con.execute(\"BEGIN\")\n",
    "\n",
    "#         # 1) delete merged-away tails FIRST (avoids transient duplicates)\n",
    "#         con.execute(\"\"\"\n",
    "#             DELETE FROM opus_moses\n",
    "#             WHERE line_no IN (SELECT line_no FROM opus_ops_delete)\n",
    "#         \"\"\")\n",
    "\n",
    "#         # 2) then update heads with their merged/cleaned text\n",
    "#         con.execute(\"\"\"\n",
    "#             UPDATE opus_moses AS o\n",
    "#             SET sent_pt_br = u.sent_pt_br,\n",
    "#                 sent_pt_pt = u.sent_pt_pt\n",
    "#             FROM opus_ops_update AS u\n",
    "#             WHERE o.line_no = u.line_no\n",
    "#         \"\"\")\n",
    "\n",
    "#         con.execute(\"COMMIT\")\n",
    "#         con.execute(\"CHECKPOINT\")\n",
    "\n",
    "def apply_ops_ctas_swap(force_checkpoint=True):\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        # If a previous tx is half-open on this connection, close it\n",
    "        try:\n",
    "            con.execute(\"ROLLBACK\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # updates win over deletes\n",
    "        con.execute(\"\"\"\n",
    "            DELETE FROM opus_ops_delete\n",
    "            WHERE line_no IN (SELECT line_no FROM opus_ops_update)\n",
    "        \"\"\")\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE opus_moses_new AS\n",
    "                SELECT\n",
    "                    o.line_no,\n",
    "                    o.pair_id,\n",
    "                    COALESCE(u.sent_pt_br, o.sent_pt_br) AS sent_pt_br,\n",
    "                    COALESCE(u.sent_pt_pt, o.sent_pt_pt) AS sent_pt_pt\n",
    "                FROM opus_moses o\n",
    "                LEFT JOIN opus_ops_update u USING (line_no)\n",
    "                WHERE o.line_no NOT IN (SELECT line_no FROM opus_ops_delete)\n",
    "                ORDER BY o.line_no\n",
    "            \"\"\")\n",
    "\n",
    "            con.execute(\"DROP TABLE opus_moses\")\n",
    "            con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "            con.execute(\"COMMIT\")\n",
    "        except:\n",
    "            con.execute(\"ROLLBACK\")\n",
    "            raise\n",
    "\n",
    "        if force_checkpoint:\n",
    "            # waits for other write transactions to finish\n",
    "            con.execute(\"FORCE CHECKPOINT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan_ops_over_corpus(block_size=50_000, reset=True)\n",
    "# apply_ops_ctas_swap()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7cdf8",
   "metadata": {},
   "source": [
    "**//AFTER PREPROCESSING, RUN THE ALIGNER THROUGH THE WHOLE CORPUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1da5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, gc\n",
    "\n",
    "def _run_moves_df(df: pd.DataFrame, margin=0.04, max_clause_chars=60, max_iters=5) -> pd.DataFrame:\n",
    "    \"\"\"Repeat apply_neighbor_moves on a DataFrame until no more moves or max_iters.\"\"\"\n",
    "    cur = df.loc[:, [\"line_no\",\"pair_id\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "    for _ in range(int(max_iters)):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "    return cur\n",
    "\n",
    "def apply_neighbor_moves_corpus_inplace(\n",
    "    block_size: int = 50_000,\n",
    "    overlap: int = 3,                 # rows kept between blocks so moves can cross the seam\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream over opus_moses and apply your neighbor-move heuristic in-place.\n",
    "    - Processes in blocks with 'overlap' rows carried forward.\n",
    "    - Only updates rows that actually changed.\n",
    "    - No row-count changes (this pass only moves clauses).\n",
    "    \"\"\"\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        lo, hi = con.execute(\"SELECT min(line_no), max(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        cur_start = lo\n",
    "        carry_df = None  # last 'overlap' rows of the previous processed block (already moved)\n",
    "\n",
    "        while cur_start <= hi:\n",
    "            # choose fetch start/count so we include the carried rows\n",
    "            if carry_df is None:\n",
    "                fetch_start = cur_start\n",
    "                fetch_count = min(block_size, hi - fetch_start + 1)\n",
    "            else:\n",
    "                fetch_start = int(carry_df[\"line_no\"].iloc[0])\n",
    "                fetch_count = min(block_size + overlap, hi - fetch_start + 1)\n",
    "\n",
    "            # load from DB\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [fetch_start, fetch_start + fetch_count - 1]).df()\n",
    "\n",
    "            # overlay carried texts onto the front (so we start from the already-moved boundary)\n",
    "            if carry_df is not None and not carry_df.empty:\n",
    "                df = df.merge(carry_df[[\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]],\n",
    "                              on=\"line_no\", how=\"left\", suffixes=(\"\",\"_car\"))\n",
    "                for col in (\"sent_pt_br\",\"sent_pt_pt\"):\n",
    "                    rep = df[col + \"_car\"]\n",
    "                    df[col] = rep.where(rep.notna(), df[col])\n",
    "                    df.drop(columns=[col + \"_car\"], inplace=True)\n",
    "\n",
    "            # keep a copy for diffing\n",
    "            orig = df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "\n",
    "            # run your move heuristic on this combined block\n",
    "            moved = _run_moves_df(df, margin=margin, max_clause_chars=max_clause_chars, max_iters=max_iters)\n",
    "\n",
    "            # decide how many rows to flush now (keep the last 'overlap' rows for the next block)\n",
    "            is_last_block = (fetch_start + len(df) - 1) >= hi\n",
    "            flush_n = len(moved) if is_last_block else max(0, len(moved) - overlap)\n",
    "\n",
    "            if flush_n:\n",
    "                out = moved.iloc[:flush_n]\n",
    "                base = orig.iloc[:flush_n]\n",
    "\n",
    "                # diffs → only update changed rows\n",
    "                changed = (out[\"sent_pt_br\"] != base[\"sent_pt_br\"]) | (out[\"sent_pt_pt\"] != base[\"sent_pt_pt\"])\n",
    "                upd = out.loc[changed, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]]\n",
    "\n",
    "                if not upd.empty:\n",
    "                    con.register(\"upd\", upd)\n",
    "                    con.execute(\"\"\"\n",
    "                        UPDATE opus_moses AS o\n",
    "                        SET sent_pt_br = u.sent_pt_br,\n",
    "                            sent_pt_pt = u.sent_pt_pt\n",
    "                        FROM upd AS u\n",
    "                        WHERE o.line_no = u.line_no\n",
    "                    \"\"\")\n",
    "                    con.unregister(\"upd\")\n",
    "\n",
    "                # next fetch should begin right after the last flushed line\n",
    "                cur_start = int(out[\"line_no\"].iloc[-1]) + 1\n",
    "            else:\n",
    "                # nothing flushed (tiny last block)\n",
    "                cur_start = fetch_start + len(df)\n",
    "\n",
    "            # carry the tail (overlap) forward (already moved)\n",
    "            carry_df = moved.iloc[flush_n:].copy()\n",
    "\n",
    "            # tidy memory\n",
    "            del df, orig, moved\n",
    "            gc.collect()\n",
    "\n",
    "        # flush any leftover carried rows (end of file)\n",
    "        if carry_df is not None and not carry_df.empty:\n",
    "            con.register(\"upd_tail\", carry_df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]])\n",
    "            con.execute(\"\"\"\n",
    "                UPDATE opus_moses AS o\n",
    "                SET sent_pt_br = u.sent_pt_br,\n",
    "                    sent_pt_pt = u.sent_pt_pt\n",
    "                FROM upd_tail AS u\n",
    "                WHERE o.line_no = u.line_no\n",
    "            \"\"\")\n",
    "            con.unregister(\"upd_tail\")\n",
    "\n",
    "        # reclaim disk space\n",
    "        con.execute(\"CHECKPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a4cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_neighbor_moves_corpus_inplace(\n",
    "#     block_size=50_000,   # tune for your RAM\n",
    "#     overlap=3,           # 2–3 is plenty for neighbor moves\n",
    "#     margin=0.04,\n",
    "#     max_clause_chars=60,\n",
    "#     max_iters=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bc9c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# B) REPEATED-PREFIX CLEANER\n",
    "# ==============================\n",
    "def _split_sents(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def _norm_for_match(s: str) -> str:\n",
    "    s = _strip_accents(s.lower())\n",
    "    s = re.sub(r\"[^\\w]+\", \" \", s, flags=re.UNICODE)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _tokens(s: str) -> List[str]:\n",
    "    return _norm_for_match(s).split()\n",
    "\n",
    "def _jaccard(a: set, b: set) -> float:\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def _looks_like_sentence_start(s: str) -> bool:\n",
    "    t = (s or \"\").lstrip()\n",
    "    while t and t[0] in \"«“\\\"([{'’”»\": t = t[1:].lstrip()\n",
    "    return (not t) or t[0].isupper()\n",
    "\n",
    "def _adjacent_dedup(sents: List[str], jacc=0.96) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for s in sents:\n",
    "        if out:\n",
    "            a = set(_tokens(out[-1])); b = set(_tokens(s))\n",
    "            if _jaccard(a, b) >= jacc:\n",
    "                continue\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def dedup_repeated_prefix_block(\n",
    "    br_prev: str, pt_prev: str,\n",
    "    br_here: str, pt_here: str,\n",
    "    *,\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True\n",
    ") -> Tuple[str, str, bool, int]:\n",
    "    \"\"\"\n",
    "    Trim from the START of (br_here, pt_here) the longest sentence-aligned prefix\n",
    "    whose tokens are largely contained in the TAIL of (br_prev, pt_prev).\n",
    "    Returns: (br_trimmed, pt_trimmed, applied, n_sentences_removed)\n",
    "    \"\"\"\n",
    "    def _count_to_remove(prev: str, nxt: str) -> int:\n",
    "        prev_s = _split_sents(prev); nxt_s = _split_sents(nxt)\n",
    "        if not prev_s or not nxt_s: return 0\n",
    "        tail = \" \".join(prev_s[-prev_window:]) if prev_window > 0 else \" \".join(prev_s)\n",
    "        tail_tok = set(_tokens(tail))\n",
    "        best_k = 0\n",
    "        for k in range(1, len(nxt_s) + 1):\n",
    "            pref = \" \".join(nxt_s[:k])\n",
    "            toks = _tokens(pref)\n",
    "            if len(toks) < min_prefix_tokens: continue\n",
    "            cov = len(set(toks) & tail_tok) / max(1, len(set(toks)))\n",
    "            if cov >= coverage_thresh: best_k = k\n",
    "        return best_k\n",
    "\n",
    "    k_br = _count_to_remove(br_prev, br_here)\n",
    "    k_pt = _count_to_remove(pt_prev, pt_here)\n",
    "    k = min(k_br, k_pt) if require_both else max(k_br, k_pt)\n",
    "    if k <= 0: return br_here, pt_here, False, 0\n",
    "\n",
    "    br_s = _split_sents(br_here)[k:]; pt_s = _split_sents(pt_here)[k:]\n",
    "    if collapse_adjacent_dups:\n",
    "        br_s = _adjacent_dedup(br_s); pt_s = _adjacent_dedup(pt_s)\n",
    "\n",
    "    br_out = \" \".join(br_s).strip(); pt_out = \" \".join(pt_s).strip()\n",
    "    if br_out and not _looks_like_sentence_start(br_out): return br_here, pt_here, False, 0\n",
    "    if pt_out and not _looks_like_sentence_start(pt_out): return br_here, pt_here, False, 0\n",
    "    return br_out, pt_out, True, k\n",
    "\n",
    "def run_repeated_prefix_cleaner_chunked(\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    table: str = \"opus_moses\",\n",
    "    order_col: str = \"line_no\",\n",
    "    text_br_col: str = \"sent_pt_br\",\n",
    "    text_pt_col: str = \"sent_pt_pt\",\n",
    "    id_pair_col: str = \"pair_id\",\n",
    "    # knobs (forwarded)\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True,\n",
    "    # deletion policy\n",
    "    delete_on_trigger: bool = True,\n",
    "    delete_if_empty_only: bool = False,\n",
    "    # execution\n",
    "    chunk_size: int = 50_000,\n",
    "    start_line: Optional[int] = None,\n",
    "    end_line: Optional[int] = None,\n",
    "    apply_changes: bool = False,\n",
    "    print_updates: bool = False,\n",
    "    trace_lines: Optional[Iterable[int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Walk rows; if dedup applies (either language unless require_both=True):\n",
    "      - delete whole row (default), or\n",
    "      - update with trimmed text.\n",
    "    Prints deleted (line_no, pair_id). Processes in chunks.\n",
    "    \"\"\"\n",
    "    assert not (delete_on_trigger and delete_if_empty_only), \\\n",
    "        \"Choose delete_on_trigger=True OR delete_if_empty_only=True (not both).\"\n",
    "\n",
    "    where = []; args = []\n",
    "    if start_line is not None: where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "    if end_line   is not None: where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "    WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        mn, mx = con.execute(\n",
    "            f\"SELECT min({order_col}), max({order_col}) FROM {table} {WHERE}\", args\n",
    "        ).fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"No rows match selection.\"); return\n",
    "\n",
    "        prev_br, prev_pt = \"\", \"\"\n",
    "        cur = int(mn)\n",
    "        while cur <= int(mx):\n",
    "            hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "            df = con.execute(f\"\"\"\n",
    "                SELECT {order_col} AS line_no,\n",
    "                       {id_pair_col} AS pair_id,\n",
    "                       {text_br_col} AS br,\n",
    "                       {text_pt_col} AS pt\n",
    "                FROM {table}\n",
    "                WHERE {order_col} BETWEEN ? AND ?\n",
    "                ORDER BY {order_col}\n",
    "            \"\"\", [cur, hi]).df()\n",
    "\n",
    "            updates = []; deletes = []\n",
    "            deleted_ids_print = []; updated_ids_print = []\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                line_no = int(df.line_no.iloc[i])\n",
    "                pair_id = int(df.pair_id.iloc[i]) if \"pair_id\" in df.columns else None\n",
    "                br_here = df.br.iloc[i] or \"\"; pt_here = df.pt.iloc[i] or \"\"\n",
    "\n",
    "                br_new, pt_new, applied, k = dedup_repeated_prefix_block(\n",
    "                    prev_br, prev_pt, br_here, pt_here,\n",
    "                    prev_window=prev_window,\n",
    "                    min_prefix_tokens=min_prefix_tokens,\n",
    "                    coverage_thresh=coverage_thresh,\n",
    "                    require_both=require_both,\n",
    "                    collapse_adjacent_dups=collapse_adjacent_dups\n",
    "                )\n",
    "\n",
    "                if trace_lines and (line_no in set(trace_lines)):\n",
    "                    print(f\"[trace {line_no}] applied={applied} k={k}\")\n",
    "\n",
    "                will_delete = False\n",
    "                if applied:\n",
    "                    if delete_on_trigger:\n",
    "                        will_delete = True\n",
    "                    elif delete_if_empty_only and (not br_new.strip() and not pt_new.strip()):\n",
    "                        will_delete = True\n",
    "\n",
    "                if will_delete:\n",
    "                    deletes.append((line_no,))\n",
    "                    deleted_ids_print.append((line_no, pair_id))\n",
    "                    # don't advance prev_* on deletion (use last kept row)\n",
    "                else:\n",
    "                    if applied and (br_new != br_here or pt_new != pt_here):\n",
    "                        updates.append((br_new, pt_new, line_no))\n",
    "                        if print_updates: updated_ids_print.append((line_no, pair_id))\n",
    "                        prev_br, prev_pt = br_new, pt_new\n",
    "                    else:\n",
    "                        prev_br, prev_pt = br_here, pt_here\n",
    "\n",
    "            if apply_changes and (updates or deletes):\n",
    "                con.execute(\"BEGIN TRANSACTION\")\n",
    "                if updates:\n",
    "                    con.executemany(\n",
    "                        f\"UPDATE {table} SET {text_br_col} = ?, {text_pt_col} = ? WHERE {order_col} = ?\",\n",
    "                        updates\n",
    "                    )\n",
    "                if deletes:\n",
    "                    con.executemany(\n",
    "                        f\"DELETE FROM {table} WHERE {order_col} = ?\",\n",
    "                        deletes\n",
    "                    )\n",
    "                con.execute(\"COMMIT\")\n",
    "\n",
    "            print(f\"[{cur}..{hi}] updates={len(updates)} deletes={len(deletes)}\")\n",
    "            if deleted_ids_print:\n",
    "                print(\"  Deleted rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(deleted_ids_print), 1000):\n",
    "                    block = deleted_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "            if print_updates and updated_ids_print:\n",
    "                print(\"  Updated rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(updated_ids_print), 1000):\n",
    "                    block = updated_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "\n",
    "            cur = hi + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77219d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) repeated-prefix dedup only\n",
    "# run_repeated_prefix_cleaner_chunked(\n",
    "#     db_path=DB,\n",
    "#     prev_window=6,\n",
    "#     min_prefix_tokens=6,\n",
    "#     coverage_thresh=0.92,\n",
    "#     require_both=False,             # either side may trigger\n",
    "#     collapse_adjacent_dups=True,\n",
    "#     delete_on_trigger=True,         # delete whenever dedup applies\n",
    "#     delete_if_empty_only=False,\n",
    "#     chunk_size=50_000,\n",
    "#     start_line=None, end_line=None,\n",
    "#     apply_changes=True,            # DRY RUN\n",
    "#     print_updates=False,\n",
    "#     trace_lines= {18}               # e.g., {18} to debug that row\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe66a3",
   "metadata": {},
   "source": [
    "**//SIMALIGN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 13:05:28,341 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# PURE SimAlign LINKS — Filter & Preview (self-contained)\n",
    "# ==============================\n",
    "# What this provides:\n",
    "#  - SimAlign setup (XLM-R, word-level)\n",
    "#  - Feature extractor using ONLY raw SimAlign word links (optionally with prev/here/next window)\n",
    "#  - Flag policy (threshold-based; tuneable)\n",
    "#  - Preview helpers (numbers-only + [[interior]] / <edge> highlights)\n",
    "#  - No trimming/mutation logic included\n",
    "# ==============================\n",
    "\n",
    "# ---------- tokenization / basics ----------\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    return WORD.findall(s or \"\")\n",
    "\n",
    "def ali_tokenize(s: str) -> list[str]:\n",
    "    \"\"\"Tokens fed to SimAlign (word-level). Keep consistent with ali_char_spans().\"\"\"\n",
    "    return tokenize(s)\n",
    "\n",
    "def ali_char_spans(text: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Char spans aligned with ali_tokenize().\"\"\"\n",
    "    return [m.span() for m in WORD.finditer(text or \"\")]\n",
    "\n",
    "def token_overlap(a: str, b: str) -> float:\n",
    "    A = set(w.lower() for w in WORD.findall(a or \"\"))\n",
    "    B = set(w.lower() for w in WORD.findall(b or \"\"))\n",
    "    return (len(A & B) / len(A | B)) if (A and B) else 0.0\n",
    "\n",
    "\n",
    "def sim2(a: str, b: str, *, method: str = ALIGN_METHOD,\n",
    "        smooth_small_gaps: int = 1, content_only: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Similarity = average coverage of aligned tokens on both sides.\n",
    "    - method: 'itermax' | 'mwmf' | 'inter' (you set ALIGN_METHOD outside)\n",
    "    - smooth_small_gaps: fill 1-token pinholes if >0\n",
    "    - content_only: measure coverage over content tokens only (ignores stopwords)\n",
    "    \"\"\"\n",
    "    a = (a or \"\").strip()\n",
    "    b = (b or \"\").strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    L = ali_tokenize(a)\n",
    "    R = ali_tokenize(b)\n",
    "    if not L and not R:\n",
    "        return 1.0\n",
    "    if not L or not R:\n",
    "        return 0.0\n",
    "\n",
    "    # raw SimAlign links\n",
    "    out = aligner.get_word_aligns(L, R)\n",
    "    pairs = out.get(method, out.get(\"inter\", []))  # fall back if needed\n",
    "\n",
    "    covered_L = {i for i, _ in pairs}\n",
    "    covered_R = {j for _, j in pairs}\n",
    "\n",
    "    covL = len(covered_L) / max(1, len(L))\n",
    "    covR = len(covered_R) / max(1, len(R))\n",
    "\n",
    "    return 0.5 * (covL + covR)\n",
    "\n",
    "def is_content_token(tok: str) -> bool:\n",
    "    t = (tok or \"\").lower()\n",
    "    return (t not in PT_STOPWORDS) and (len(t) > 1)\n",
    "\n",
    "\n",
    "def _split_sents(s: str) -> list[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "# ---------- SimAlign setup ----------\n",
    "aligner = SentenceAligner(model=\"xlmr\", token_type=\"word\", matching_methods=\"a\")\n",
    "\n",
    "# ---------- raw pairs + utilities ----------\n",
    "def _raw_pairs(l_tokens, r_tokens, method=ALIGN_METHOD):\n",
    "    out = aligner.get_word_aligns(l_tokens, r_tokens)  # keys: \"inter\", \"itermax\", \"mwmf\"\n",
    "    if method == \"itermax\":\n",
    "        return out[\"itermax\"]\n",
    "    elif method == \"union\":  # inter ∪ itermax (often a sweet spot)\n",
    "        return list({*out[\"inter\"], *out[\"itermax\"]})\n",
    "    else:  # \"inter\" or \"mwmf\"\n",
    "        return out[method]\n",
    "\n",
    "def spans_from_uncovered(tokens: list[str], covered_idx: set[int]) -> list[tuple[int,int]]:\n",
    "    spans, cur = [], []\n",
    "    for i in range(len(tokens)):\n",
    "        if i not in covered_idx:\n",
    "            cur.append(i)\n",
    "        elif cur:\n",
    "            spans.append((cur[0], cur[-1])); cur = []\n",
    "    if cur:\n",
    "        spans.append((cur[0], cur[-1]))\n",
    "    return spans\n",
    "\n",
    "def _smooth_small_gaps(covered: set[int], n_tokens: int, max_gap: int = 1) -> set[int]:\n",
    "    \"\"\"Fill tiny uncovered holes (≤ max_gap) surrounded by covered tokens (function-word pinholes).\"\"\"\n",
    "    C = set(covered)\n",
    "    i = 0\n",
    "    while i < n_tokens:\n",
    "        if i not in C:\n",
    "            j = i\n",
    "            while j < n_tokens and j not in C:\n",
    "                j += 1\n",
    "            gap = j - i\n",
    "            if 0 < gap <= max_gap and i > 0 and j < n_tokens:\n",
    "                for k in range(i, j):\n",
    "                    C.add(k)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return C\n",
    "\n",
    "def _interior_uncovered(tokens: list[str], covered: set[int]) -> list[tuple[int,int]]:\n",
    "    \"\"\"Uncovered runs strictly inside (not touching edges).\"\"\"\n",
    "    runs = spans_from_uncovered(tokens, covered)\n",
    "    n = len(tokens)\n",
    "    return [(i0, i1) for (i0, i1) in runs if i0 > 0 and i1 < n - 1]\n",
    "\n",
    "def _content_count(tokens: list[str]) -> int:\n",
    "    return sum(1 for t in tokens if is_content_token(t))\n",
    "\n",
    "# ---------- highlighting ----------\n",
    "def _split_edge_vs_interior(runs: list[tuple[int,int]], n_tokens: int):\n",
    "    interior, edges = [], []\n",
    "    for i0, i1 in runs:\n",
    "        if i0 > 0 and i1 < n_tokens - 1:\n",
    "            interior.append((i0, i1))\n",
    "        else:\n",
    "            edges.append((i0, i1))\n",
    "    return interior, edges\n",
    "\n",
    "def _to_char_spans(token_runs: list[tuple[int,int]], token_char: list[tuple[int,int]]):\n",
    "    char_runs = []\n",
    "    for i0, i1 in token_runs:\n",
    "        if not token_char:\n",
    "            continue\n",
    "        i0 = max(0, min(i0, len(token_char) - 1))\n",
    "        i1 = max(0, min(i1, len(token_char) - 1))\n",
    "        L = token_char[i0][0]; R = token_char[i1][1]\n",
    "        char_runs.append((L, R))\n",
    "    # merge\n",
    "    char_runs.sort()\n",
    "    merged = []\n",
    "    for L, R in char_runs:\n",
    "        if not merged or L > merged[-1][1]:\n",
    "            merged.append([L, R])\n",
    "        else:\n",
    "            merged[-1][1] = max(merged[-1][1], R)\n",
    "    return [(L, R) for L, R in merged]\n",
    "\n",
    "def _apply_highlights(text: str,\n",
    "                      interior_char: list[tuple[int,int]],\n",
    "                      edge_char: list[tuple[int,int]],\n",
    "                      marks=(\"[[\", \"]]\"), edge_marks=(\"<\", \">\")) -> str:\n",
    "    \"\"\"Insert [[...]] (interior) and <...> (edge) highlights without breaking indices.\"\"\"\n",
    "    tags = []\n",
    "    for L, R in interior_char:\n",
    "        tags.append((L, \"open_i\")); tags.append((R, \"close_i\"))\n",
    "    for L, R in edge_char:\n",
    "        tags.append((L, \"open_e\")); tags.append((R, \"close_e\"))\n",
    "    tags.sort(key=lambda x: (x[0], x[1].startswith(\"close\")))  # close before open at same pos\n",
    "\n",
    "    out, last = [], 0\n",
    "    stack = []\n",
    "    for pos, kind in tags:\n",
    "        pos = max(0, min(pos, len(text)))\n",
    "        if pos > last:\n",
    "            out.append(text[last:pos])\n",
    "            last = pos\n",
    "        if kind == \"open_i\":\n",
    "            out.append(marks[0]); stack.append(\"i\")\n",
    "        elif kind == \"close_i\":\n",
    "            if stack and stack[-1] == \"i\":\n",
    "                stack.pop()\n",
    "                out.append(marks[1])\n",
    "        elif kind == \"open_e\":\n",
    "            out.append(edge_marks[0]); stack.append(\"e\")\n",
    "        elif kind == \"close_e\":\n",
    "            if stack and stack[-1] == \"e\":\n",
    "                stack.pop()\n",
    "                out.append(edge_marks[1])\n",
    "    if last < len(text):\n",
    "        out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _alignment_uncovered_highlights(\n",
    "    left_text: str, right_prev: str, right_here: str, right_next: str,\n",
    "    *, use_window: bool\n",
    ") -> tuple[str, float, float]:\n",
    "    \"\"\"\n",
    "    Highlight uncovered tokens on the left_text using raw SimAlign links.\n",
    "    Returns (highlighted_text, coverage_ratio, interior_content_ratio).\n",
    "    \"\"\"\n",
    "    left_toks = ali_tokenize(left_text)\n",
    "    if use_window:\n",
    "        right_win = ali_tokenize(\" \".join(x for x in [right_prev, right_here, right_next] if x))\n",
    "        pairs = _raw_pairs(left_toks, right_win)\n",
    "    else:\n",
    "        pairs = _raw_pairs(left_toks, ali_tokenize(right_here))\n",
    "\n",
    "    covered = {i for i, _ in pairs}\n",
    "    covered = _smooth_small_gaps(covered, len(left_toks), max_gap=1)\n",
    "\n",
    "    n = len(left_toks)\n",
    "    cov = len(covered) / max(1, n)\n",
    "\n",
    "    runs_all = spans_from_uncovered(left_toks, covered)\n",
    "    interior_runs, edge_runs = _split_edge_vs_interior(runs_all, n)\n",
    "\n",
    "    # interior content ratio\n",
    "    total_content = sum(1 for t in left_toks if is_content_token(t))\n",
    "    interior_content = sum(\n",
    "        1 for i0, i1 in interior_runs for t in left_toks[i0:i1+1] if is_content_token(t)\n",
    "    )\n",
    "    interior_content_ratio = interior_content / max(1, total_content)\n",
    "\n",
    "    token_chars = ali_char_spans(left_text)\n",
    "    interior_char = _to_char_spans(interior_runs, token_chars)\n",
    "    edge_char     = _to_char_spans(edge_runs, token_chars)\n",
    "    hi = _apply_highlights(left_text, interior_char, edge_char)\n",
    "    return hi, cov, interior_content_ratio\n",
    "\n",
    "# ---------- feature extractor (pure SimAlign) ----------\n",
    "def alignment_quality_features(\n",
    "    br_prev: str, br_here: str, br_next: str,\n",
    "    pt_prev: str, pt_here: str, pt_next: str,\n",
    "    *, use_window: bool = True, sim_fn=None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute alignment metrics using ONLY raw SimAlign word links (+ optional prev/next window).\n",
    "    \"\"\"\n",
    "    if sim_fn is None:\n",
    "        sim_fn = sim\n",
    "\n",
    "    br_toks = ali_tokenize(br_here)\n",
    "    pt_toks = ali_tokenize(pt_here)\n",
    "\n",
    "    if use_window:\n",
    "        pt_win = ali_tokenize(\" \".join(x for x in [pt_prev, pt_here, pt_next] if x))\n",
    "        br_pairs = _raw_pairs(br_toks, pt_win)\n",
    "        br_cov = {i for i, _ in br_pairs}\n",
    "\n",
    "        br_win = ali_tokenize(\" \".join(x for x in [br_prev, br_here, br_next] if x))\n",
    "        pt_pairs = _raw_pairs(pt_toks, br_win)\n",
    "        pt_cov = {i for i, _ in pt_pairs}  # left indices of PT tokens\n",
    "    else:\n",
    "        pairs = _raw_pairs(br_toks, pt_toks)\n",
    "        br_cov = {i for i, _ in pairs}\n",
    "        pt_cov = {j for _, j in pairs}     # approximate right coverage\n",
    "\n",
    "    # smooth 1-token pinholes\n",
    "    br_cov = _smooth_small_gaps(br_cov, len(br_toks), max_gap=1)\n",
    "    pt_cov = _smooth_small_gaps(pt_cov, len(pt_toks), max_gap=1)\n",
    "\n",
    "    br_cov_ratio = len(br_cov) / max(1, len(br_toks))\n",
    "    pt_cov_ratio = len(pt_cov) / max(1, len(pt_toks))\n",
    "    cov_min = min(br_cov_ratio, pt_cov_ratio)\n",
    "    cov_gap = abs(br_cov_ratio - pt_cov_ratio)\n",
    "\n",
    "    # interior uncovered runs + content ratio\n",
    "    br_int_spans = _interior_uncovered(br_toks, br_cov)\n",
    "    pt_int_spans = _interior_uncovered(pt_toks, pt_cov)\n",
    "\n",
    "    def _content_in_runs(tokens, runs):\n",
    "        return sum(1 for i0, i1 in runs for t in tokens[i0:i1+1] if is_content_token(t))\n",
    "\n",
    "    br_content_total = _content_count(br_toks)\n",
    "    pt_content_total = _content_count(pt_toks)\n",
    "    br_int_content = _content_in_runs(br_toks, br_int_spans)\n",
    "    pt_int_content = _content_in_runs(pt_toks, pt_int_spans)\n",
    "\n",
    "    br_int_content_ratio = br_int_content / max(1, br_content_total)\n",
    "    pt_int_content_ratio = pt_int_content / max(1, pt_content_total)\n",
    "\n",
    "    br_max_int = max((j - i + 1) for i, j in br_int_spans) if br_int_spans else 0\n",
    "    pt_max_int = max((j - i + 1) for i, j in pt_int_spans) if pt_int_spans else 0\n",
    "\n",
    "    # “spillover” vs extra-info (same-language neighbors)\n",
    "    def _span_text(tokens, sp): i0, i1 = sp; return \" \".join(tokens[i0:i1+1])\n",
    "    br_int_text = \" \".join(_span_text(br_toks, sp) for sp in br_int_spans)\n",
    "    pt_int_text = \" \".join(_span_text(pt_toks, sp) for sp in pt_int_spans)\n",
    "    br_spill = max(token_overlap(br_int_text, br_prev), token_overlap(br_int_text, br_next)) if br_int_text else 0.0\n",
    "    pt_spill = max(token_overlap(pt_int_text, pt_prev), token_overlap(pt_int_text, pt_next)) if pt_int_text else 0.0\n",
    "\n",
    "    sent_diff = abs(len(_split_sents(br_here)) - len(_split_sents(pt_here)))\n",
    "    base_sim = sim_fn(br_here, pt_here)\n",
    "\n",
    "    return {\n",
    "        \"br_cov\": br_cov_ratio, \"pt_cov\": pt_cov_ratio,\n",
    "        \"cov_min\": cov_min, \"cov_gap\": cov_gap,\n",
    "        \"br_int_content_ratio\": br_int_content_ratio,\n",
    "        \"pt_int_content_ratio\": pt_int_content_ratio,\n",
    "        \"br_max_int\": br_max_int, \"pt_max_int\": pt_max_int,\n",
    "        \"br_spill\": br_spill, \"pt_spill\": pt_spill,\n",
    "        \"sent_diff\": sent_diff,\n",
    "        \"base_sim\": float(base_sim),\n",
    "        \"br_content_total\": br_content_total,\n",
    "        \"pt_content_total\": pt_content_total,\n",
    "    }\n",
    "\n",
    "# ---------- flag policy (tune to taste) ----------\n",
    "def alignment_quality_flag(\n",
    "    feats: dict,\n",
    "    *,\n",
    "    min_cov_ok: float = 0.50,\n",
    "    max_cov_gap: float = 0.35,\n",
    "    max_int_ratio: float = 0.33,\n",
    "    max_max_int: int = 9,\n",
    "    max_sent_diff: int = 1,\n",
    "    min_sim_ok: float = 0.30,\n",
    "    spill_tolerance: float = 0.60,\n",
    "    min_row_content: int = 6,\n",
    "    min_interior_content_for_flag: int = 3\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Decide whether to activate the \"alignment-quality\" filter.\n",
    "    Uses only SimAlign-derived features (+ same-language spill check).\n",
    "    \"\"\"\n",
    "    # guards: short rows or already good-enough\n",
    "    if feats[\"base_sim\"] >= 0.70 and feats[\"cov_min\"] >= 0.60:\n",
    "        return False, \"ok\"\n",
    "    if feats[\"br_content_total\"] < min_row_content and feats[\"pt_content_total\"] < min_row_content:\n",
    "        return False, \"too_short\"\n",
    "\n",
    "    reasons = []\n",
    "    if feats[\"cov_min\"] < min_cov_ok:\n",
    "        reasons.append(\"low_coverage\")\n",
    "    if feats[\"cov_gap\"] > max_cov_gap:\n",
    "        reasons.append(\"coverage_asymmetry\")\n",
    "    if feats[\"br_int_content_ratio\"] > max_int_ratio or feats[\"pt_int_content_ratio\"] > max_int_ratio:\n",
    "        reasons.append(\"big_interior_unaligned_content\")\n",
    "    if feats[\"br_max_int\"] >= max_max_int or feats[\"pt_max_int\"] >= max_max_int:\n",
    "        reasons.append(\"long_interior_gap\")\n",
    "    if feats[\"sent_diff\"] > max_sent_diff:\n",
    "        reasons.append(\"sentence_mismatch\")\n",
    "    if feats[\"base_sim\"] < min_sim_ok:\n",
    "        reasons.append(\"low_similarity\")\n",
    "\n",
    "    spillish = (feats[\"br_spill\"] >= spill_tolerance) or (feats[\"pt_spill\"] >= spill_tolerance)\n",
    "    strong = {\"low_coverage\",\"coverage_asymmetry\",\"big_interior_unaligned_content\",\"long_interior_gap\"}\n",
    "    strong_hits = len([r for r in reasons if r in strong])\n",
    "\n",
    "    # need some actual interior content if we accuse \"content\" reasons\n",
    "    if {\"big_interior_unaligned_content\",\"long_interior_gap\"} & set(reasons):\n",
    "        enough_interior = (feats[\"br_int_content_ratio\"]*feats[\"br_content_total\"] >= min_interior_content_for_flag) or \\\n",
    "                          (feats[\"pt_int_content_ratio\"]*feats[\"pt_content_total\"] >= min_interior_content_for_flag)\n",
    "        if not enough_interior:\n",
    "            reasons = [r for r in reasons if r not in {\"big_interior_unaligned_content\",\"long_interior_gap\"}]\n",
    "\n",
    "    activate = False\n",
    "    if strong_hits >= 2:\n",
    "        activate = True\n",
    "    elif strong_hits >= 1 and not spillish:\n",
    "        activate = True\n",
    "    elif len(reasons) >= 3 and not spillish:\n",
    "        activate = True\n",
    "\n",
    "    return bool(activate), (\",\".join(reasons) if reasons else \"ok\")\n",
    "\n",
    "# ---------- previews ----------\n",
    "def preview_alignment_quality_window(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    sim_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Numbers-only preview (no highlights). No DB writes.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {}\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or sim\n",
    "        )\n",
    "        activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            **feats,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def preview_alignment_quality_window_with_highlights(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    show_when: str = \"flagged\",   # \"flagged\" | \"all\"\n",
    "    sim_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preview with [[INTERIOR]] and <EDGE> highlights using pure SimAlign links.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = dict(\n",
    "            min_cov_ok=0.50,\n",
    "            max_cov_gap=0.35,\n",
    "            max_int_ratio=0.33,\n",
    "            max_max_int=9,\n",
    "            max_sent_diff=1,\n",
    "            min_sim_ok=0.30,\n",
    "            spill_tolerance=0.60\n",
    "        )\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or sim\n",
    "        )\n",
    "        activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "        if show_when == \"flagged\" and not activate:\n",
    "            continue\n",
    "\n",
    "        br_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            br_here, pt_prev, pt_here, pt_next, use_window=use_window\n",
    "        )\n",
    "        pt_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            pt_here, br_prev, br_here, br_next, use_window=use_window\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            \"base_sim\": feats[\"base_sim\"],\n",
    "            \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "            \"cov_gap\": feats[\"cov_gap\"],\n",
    "            \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "            \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "            \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "            \"br_highlight\": br_hi,   # [[INTERIOR]] and <EDGE>\n",
    "            \"pt_highlight\": pt_hi,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hq = preview_alignment_quality_window_with_highlights(\n",
    "    start_line=15, window=30,\n",
    "    use_window=True,           # align against prev+here+next window\n",
    "    show_when=\"all\",\n",
    "    thresholds=dict(\n",
    "        min_cov_ok=0.50,\n",
    "        max_cov_gap=0.35,\n",
    "        max_int_ratio=0.25,\n",
    "        max_max_int=9,\n",
    "        max_sent_diff=1,\n",
    "        min_sim_ok=0.30,\n",
    "        spill_tolerance=0.60\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dc0874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>activate_filter</th>\n",
       "      <th>reason</th>\n",
       "      <th>base_sim</th>\n",
       "      <th>br_cov</th>\n",
       "      <th>pt_cov</th>\n",
       "      <th>cov_gap</th>\n",
       "      <th>br_int_content_ratio</th>\n",
       "      <th>pt_int_content_ratio</th>\n",
       "      <th>spillish</th>\n",
       "      <th>br_highlight</th>\n",
       "      <th>pt_highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>big_interior_unaligned_content,long_interior_gap,sentence_mismatch</td>\n",
       "      <td>0.388742</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.149206</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>Não, mas esses buracos são grandes o suficiente para as mãos deslizarem, com luvas e [[punho anéis afixados a]] ele, pode ser hermético, uma caixa [[de biossegurança improvisada, certifique-se de que]] entra [[em um caso]] biológico, se houver algum esporos deixados lá, eu quero [[ter certeza]] que eles não ser sacudido durante um acidentado carona para [[o laboratório]], sim senhor, entendido, [[nós cuidaremos disso, tome cuidado, certo, k-6, vamos para k]]-6, odeio diga que eu disse [[a você]], bem, nós vamos conseguir para testá-[[lo imediatamente]], saberemos [[em breve, você foi]] puxado na equipe de sykes também? [[Parecia evitar]] o sétimo círculo do inferno por mais um dia, eu pensei [[em você estavam]] seguindo ivins, eu sou, o que [[que diabos ele]] está fazendo aqui?</td>\n",
       "      <td>Não. Mas esses buracos têm tamanho suficiente para mãos. Com luvas e [[material próprio]], podia ser estanque. [[Ponham isso]] numa caixa biológica. Se tiver esporos, não quero que se soltem durante a viagem. Sim, entendido. [[Para trás. 86436. Detesto dizer-te]] \"eu bem te disse\". Vamos testá-la e ficaremos [[a saber]]. Também vieste para a equipa [[do Sykes? Evitei]] o Sétimo Círculo do Inferno por mais um dia. Não [[andavas a]] seguir o Ivins? Ando. Que raio faz ele aqui?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>big_interior_unaligned_content,sentence_mismatch</td>\n",
       "      <td>0.402956</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.209048</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>Não pode ser coincidência, temos um não seguro pessoa na tenda do voluntário, casaco marrom, chapéu preto, remova-[[o imediatamente]], entendido, reabastecer para um café? Por aqui, não [[não, isto é um erro]], isso, [[uh, eu não entendo]], eu [[tenho o, eu tenho o alerta]] de [[alta liberação, isso significa]] que eu trabalho [[para o governo, eu trabalho]] para o exército, O que você está filmando? Isto é um erro, [[eu estou, senhor, eu]] preciso te colocar no [[carro, senhor, no]] carro [[agora, ok]], então você estava lá?</td>\n",
       "      <td>Não pode ser coincidência. Temos uma pessoa não autorizada na tenda de voluntários. Casaco castanho, chapéu preto. [[Querem um]] café? Isto é um engano. Não [[percebo. Tenho autorização]] elevada. O que está a filmar? [[Isto é]] um engano, eu... Preciso que entre no carro. [[Entre já no]] carro. Está bem. Então esteve lá?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.404880</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>Não posso [[divulgue quaisquer]] detalhes, mas eles me trouxeram para ajudar por causa da minha experiência, isso foi [[tudo sobre]] as notícias de hoje, uh, você viu alguma coisa? Eles tinham alguma nova &lt;evidência&gt;?</td>\n",
       "      <td>Não posso falar de pormenores, mas pediram-me ajuda devido às minhas habilitações. Passou nas notícias. Viu alguma coisa, têm provas novas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "      <td>big_interior_unaligned_content,long_interior_gap,sentence_mismatch</td>\n",
       "      <td>0.388779</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.265057</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>&lt;Eu&gt; não posso divulgar isso, não alcancei [[o nível de]] liberação eu tenho por sendo um tagarela, [[soa como você estava realmente]] essencial, [[bruce, você se]] sentiu [[necessário? Você]] sabe, dr, halstrom, [[eu comecei]] a perceber no meu vida que existem dois tipos de pessoas, [[há pessoas cujo trabalho é é sentar em uma cadeira]] o dia [[todo e]] fazer perguntas, e então há alguns poucos habilidosos de nós que encontram respostas para eles, um é claramente mais necessário do que o outro, [[bom para]] ir, obrigada, [[eu aprecio isso, meu]] amigo, vejo [[você em]] breve, bem aqui, por favor, sim [[senhor, obrigada, tudo bem]], pare [[aí, ok, faça isso novamente]] com o outro, [[sim, use]] a &lt;luva, sim, uh-hmm, sim, o que você quiser, uma caneta ou&gt;,?</td>\n",
       "      <td>&lt;Não&gt; posso dizer. Não tenho uma autorização elevada por [[falar de mais. Parece que]] foi essencial, Bruce. Sentiu [[que precisavam]] de si? Sabe, Dra. Halstrom? Já percebi que há dois tipos de pessoas. [[Há as]] que passam o dia sentadas a fazer perguntas e há alguns de nós, mais habilitados, [[que lhes arranjamos]] respostas. Um [[dos tipos]] é mais necessário que o outro. Podemos ir. Obrigado, amigo. Até breve. Aqui, por favor. Pare. Repita com a outra. Sim.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_no  pair_id  activate_filter  \\\n",
       "0       21       21             True   \n",
       "1       27       27             True   \n",
       "2       36       36            False   \n",
       "3       38       38             True   \n",
       "\n",
       "                                                               reason  \\\n",
       "0  big_interior_unaligned_content,long_interior_gap,sentence_mismatch   \n",
       "1                    big_interior_unaligned_content,sentence_mismatch   \n",
       "2                                                                  ok   \n",
       "3  big_interior_unaligned_content,long_interior_gap,sentence_mismatch   \n",
       "\n",
       "   base_sim    br_cov    pt_cov   cov_gap  br_int_content_ratio  \\\n",
       "0  0.388742  0.650794  0.800000  0.149206              0.333333   \n",
       "1  0.402956  0.602273  0.811321  0.209048              0.423729   \n",
       "2  0.404880  0.852941  1.000000  0.147059              0.153846   \n",
       "3  0.388779  0.542636  0.807692  0.265057              0.387097   \n",
       "\n",
       "   pt_int_content_ratio  spillish  \\\n",
       "0              0.207547  0.081395   \n",
       "1              0.194444  0.075472   \n",
       "2              0.000000  0.010638   \n",
       "3              0.134615  0.075758   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           br_highlight  \\\n",
       "0  Não, mas esses buracos são grandes o suficiente para as mãos deslizarem, com luvas e [[punho anéis afixados a]] ele, pode ser hermético, uma caixa [[de biossegurança improvisada, certifique-se de que]] entra [[em um caso]] biológico, se houver algum esporos deixados lá, eu quero [[ter certeza]] que eles não ser sacudido durante um acidentado carona para [[o laboratório]], sim senhor, entendido, [[nós cuidaremos disso, tome cuidado, certo, k-6, vamos para k]]-6, odeio diga que eu disse [[a você]], bem, nós vamos conseguir para testá-[[lo imediatamente]], saberemos [[em breve, você foi]] puxado na equipe de sykes também? [[Parecia evitar]] o sétimo círculo do inferno por mais um dia, eu pensei [[em você estavam]] seguindo ivins, eu sou, o que [[que diabos ele]] está fazendo aqui?   \n",
       "1                                                                                                                                                                                                                                                                   Não pode ser coincidência, temos um não seguro pessoa na tenda do voluntário, casaco marrom, chapéu preto, remova-[[o imediatamente]], entendido, reabastecer para um café? Por aqui, não [[não, isto é um erro]], isso, [[uh, eu não entendo]], eu [[tenho o, eu tenho o alerta]] de [[alta liberação, isso significa]] que eu trabalho [[para o governo, eu trabalho]] para o exército, O que você está filmando? Isto é um erro, [[eu estou, senhor, eu]] preciso te colocar no [[carro, senhor, no]] carro [[agora, ok]], então você estava lá?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Não posso [[divulgue quaisquer]] detalhes, mas eles me trouxeram para ajudar por causa da minha experiência, isso foi [[tudo sobre]] as notícias de hoje, uh, você viu alguma coisa? Eles tinham alguma nova <evidência>?   \n",
       "3                       <Eu> não posso divulgar isso, não alcancei [[o nível de]] liberação eu tenho por sendo um tagarela, [[soa como você estava realmente]] essencial, [[bruce, você se]] sentiu [[necessário? Você]] sabe, dr, halstrom, [[eu comecei]] a perceber no meu vida que existem dois tipos de pessoas, [[há pessoas cujo trabalho é é sentar em uma cadeira]] o dia [[todo e]] fazer perguntas, e então há alguns poucos habilidosos de nós que encontram respostas para eles, um é claramente mais necessário do que o outro, [[bom para]] ir, obrigada, [[eu aprecio isso, meu]] amigo, vejo [[você em]] breve, bem aqui, por favor, sim [[senhor, obrigada, tudo bem]], pare [[aí, ok, faça isso novamente]] com o outro, [[sim, use]] a <luva, sim, uh-hmm, sim, o que você quiser, uma caneta ou>,?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      pt_highlight  \n",
       "0  Não. Mas esses buracos têm tamanho suficiente para mãos. Com luvas e [[material próprio]], podia ser estanque. [[Ponham isso]] numa caixa biológica. Se tiver esporos, não quero que se soltem durante a viagem. Sim, entendido. [[Para trás. 86436. Detesto dizer-te]] \"eu bem te disse\". Vamos testá-la e ficaremos [[a saber]]. Também vieste para a equipa [[do Sykes? Evitei]] o Sétimo Círculo do Inferno por mais um dia. Não [[andavas a]] seguir o Ivins? Ando. Que raio faz ele aqui?  \n",
       "1                                                                                                                                                              Não pode ser coincidência. Temos uma pessoa não autorizada na tenda de voluntários. Casaco castanho, chapéu preto. [[Querem um]] café? Isto é um engano. Não [[percebo. Tenho autorização]] elevada. O que está a filmar? [[Isto é]] um engano, eu... Preciso que entre no carro. [[Entre já no]] carro. Está bem. Então esteve lá?  \n",
       "2                                                                                                                                                                                                                                                                                                                                                      Não posso falar de pormenores, mas pediram-me ajuda devido às minhas habilitações. Passou nas notícias. Viu alguma coisa, têm provas novas?  \n",
       "3               <Não> posso dizer. Não tenho uma autorização elevada por [[falar de mais. Parece que]] foi essencial, Bruce. Sentiu [[que precisavam]] de si? Sabe, Dra. Halstrom? Já percebi que há dois tipos de pessoas. [[Há as]] que passam o dia sentadas a fazer perguntas e há alguns de nós, mais habilitados, [[que lhes arranjamos]] respostas. Um [[dos tipos]] é mais necessário que o outro. Podemos ir. Obrigado, amigo. Até breve. Aqui, por favor. Pare. Repita com a outra. Sim.  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c58838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ali_tokenize(s: str) -> list[str]:\n",
    "    return [m.group(0) for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "def ali_char_spans(s: str) -> list[tuple[int,int]]:\n",
    "    return [m.span() for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "def _raw_pairs(left_tokens: list[str], right_tokens: list[str], method: str = ALIGN_METHOD):\n",
    "    \"\"\"Call SimAlign safely; fall back on truncation and available methods.\"\"\"\n",
    "    if not left_tokens or not right_tokens:\n",
    "        return []\n",
    "    try:\n",
    "        out = aligner.get_word_aligns(left_tokens, right_tokens)\n",
    "    except Exception:\n",
    "        lt, rt = left_tokens[:300], right_tokens[:300]  # rare long-line guard\n",
    "        try:\n",
    "            out = aligner.get_word_aligns(lt, rt)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if method in out:\n",
    "        return out[method]\n",
    "    for m in (\"itermax\",\"mwmf\",\"inter\"):\n",
    "        if m in out:\n",
    "            return out[m]\n",
    "    return []\n",
    "\n",
    "\n",
    "# -------- reservoir sampling (keeps a bounded, uniform-ish sample) -------\n",
    "def _reservoir_add(reservoir: list, item: dict, cap: int, seen_counter: int):\n",
    "    if cap <= 0:\n",
    "        return seen_counter + 1\n",
    "    if len(reservoir) < cap:\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        j = random.randint(0, seen_counter)\n",
    "        if j < cap:\n",
    "            reservoir[j] = item\n",
    "    return seen_counter + 1\n",
    "\n",
    "\n",
    "# -------- main audit: fixed thresholds, no adaptive policy ----------------\n",
    "def audit_alignment_filter_chunked_fixed(\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    table: str = \"opus_moses\",\n",
    "    order_col: str = \"line_no\",\n",
    "    id_col: str = \"pair_id\",\n",
    "    br_col: str = \"sent_pt_br\",\n",
    "    pt_col: str = \"sent_pt_pt\",\n",
    "    start_line: Optional[int] = None,\n",
    "    end_line:   Optional[int] = None,\n",
    "    chunk_size: int = 50_000,\n",
    "    # filter policy (FIXED per run)\n",
    "    thresholds: Dict[str, Any] = dict(\n",
    "        min_cov_ok=0.50,\n",
    "        max_cov_gap=0.35,\n",
    "        max_int_ratio=0.33,\n",
    "        max_max_int=9,\n",
    "        max_sent_diff=1,\n",
    "        min_sim_ok=0.30,\n",
    "        spill_tolerance=0.60\n",
    "    ),\n",
    "    use_window: bool = True,\n",
    "    sim_fn=None,\n",
    "    # how many examples to keep in memory\n",
    "    max_store_flagged: int = 1000,\n",
    "    max_store_passed: int  = 1000,\n",
    "    seed: int = 13,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Iterate in chunks, compute alignment_quality_features + alignment_quality_flag\n",
    "    with FIXED thresholds. Print per-chunk stats.\n",
    "    Returns (flagged_df, passed_df, summary_df). No highlights, no DB writes.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        # build range\n",
    "        where, args = [], []\n",
    "        if start_line is not None:\n",
    "            where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "        if end_line is not None:\n",
    "            where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "        WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "        mn, mx = con.execute(\n",
    "            f\"SELECT MIN({order_col}), MAX({order_col}) FROM {table} {WHERE}\", args\n",
    "        ).fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"No rows to audit.\")\n",
    "            return (pd.DataFrame(), pd.DataFrame(), pd.DataFrame())\n",
    "\n",
    "        total_rows = 0\n",
    "        total_flagged = 0\n",
    "        summaries = []\n",
    "\n",
    "        # sample reservoirs\n",
    "        flagged_res, passed_res = [], []\n",
    "        seen_flagged = seen_passed = 0\n",
    "\n",
    "        cur = int(mn)\n",
    "        while cur <= int(mx):\n",
    "            hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "            df = con.execute(f\"\"\"\n",
    "                SELECT {order_col} AS line_no, {id_col} AS pair_id,\n",
    "                       {br_col} AS br, {pt_col} AS pt\n",
    "                FROM {table}\n",
    "                WHERE {order_col} BETWEEN ? AND ?\n",
    "                ORDER BY {order_col}\n",
    "            \"\"\", [cur, hi]).df()\n",
    "\n",
    "            if df.empty:\n",
    "                cur = hi + 1\n",
    "                continue\n",
    "\n",
    "            chunk_rows = len(df)\n",
    "            chunk_flagged = 0\n",
    "\n",
    "            for i in range(chunk_rows):\n",
    "                br_prev = df.br.iloc[i-1] if i > 0 else \"\"\n",
    "                br_here = df.br.iloc[i]\n",
    "                br_next = df.br.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "                pt_prev = df.pt.iloc[i-1] if i > 0 else \"\"\n",
    "                pt_here = df.pt.iloc[i]\n",
    "                pt_next = df.pt.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "                feats = alignment_quality_features(\n",
    "                    br_prev, br_here, br_next,\n",
    "                    pt_prev, pt_here, pt_next,\n",
    "                    use_window=use_window, sim_fn=sim_fn or sim\n",
    "                )\n",
    "\n",
    "                # alignment_quality_flag may return (activate, reason) OR (activate, reason, flags)\n",
    "                res = alignment_quality_flag(feats, **thresholds)\n",
    "                if isinstance(res, tuple) and len(res) == 3:\n",
    "                    activate, reason_str, flags = res\n",
    "                else:\n",
    "                    activate, reason_str = res\n",
    "                    flags = {}\n",
    "\n",
    "                row_info = {\n",
    "                    \"line_no\": int(df.line_no.iloc[i]),\n",
    "                    \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "                    \"activate_filter\": bool(activate),\n",
    "                    \"reason\": reason_str,\n",
    "                    \"base_sim\": feats[\"base_sim\"],\n",
    "                    \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "                    \"cov_gap\": feats[\"cov_gap\"],\n",
    "                    \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "                    \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "                    \"br_max_int\": feats[\"br_max_int\"], \"pt_max_int\": feats[\"pt_max_int\"],\n",
    "                    \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "                    # include sentence mismatch if available\n",
    "                    \"sent_diff\": feats.get(\"sent_diff\", None),\n",
    "                }\n",
    "                # keep reason flags if your policy returns them\n",
    "                row_info.update({k: v for k, v in flags.items()})\n",
    "\n",
    "                if activate:\n",
    "                    chunk_flagged += 1\n",
    "                    seen_flagged = _reservoir_add(flagged_res, row_info, max_store_flagged, seen_flagged)\n",
    "                else:\n",
    "                    seen_passed = _reservoir_add(passed_res, row_info, max_store_passed, seen_passed)\n",
    "\n",
    "            total_rows    += chunk_rows\n",
    "            total_flagged += chunk_flagged\n",
    "            frac = (chunk_flagged / chunk_rows) if chunk_rows else 0.0\n",
    "            print(f\"[{cur}..{hi}] rows={chunk_rows} flagged={chunk_flagged} ({frac:.1%})\")\n",
    "\n",
    "            summaries.append({\n",
    "                \"chunk_start\": cur, \"chunk_end\": hi,\n",
    "                \"rows\": chunk_rows, \"flagged\": chunk_flagged, \"ratio\": frac\n",
    "            })\n",
    "\n",
    "            cur = hi + 1\n",
    "\n",
    "        overall = (total_flagged / total_rows) if total_rows else 0.0\n",
    "        print(f\"\\nTOTAL rows={total_rows} flagged={total_flagged} ({overall:.1%})\")\n",
    "\n",
    "        flagged_df = pd.DataFrame(flagged_res)\n",
    "        passed_df  = pd.DataFrame(passed_res)\n",
    "        summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "        return flagged_df, passed_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1..50000] rows=41582 flagged=3583 (8.6%)\n",
      "[50001..100000] rows=40318 flagged=3655 (9.1%)\n",
      "[100001..150000] rows=40464 flagged=3481 (8.6%)\n",
      "[150001..200000] rows=39129 flagged=4551 (11.6%)\n",
      "\n",
      "TOTAL rows=161493 flagged=15270 (9.5%)\n"
     ]
    }
   ],
   "source": [
    "flagged_df, passed_df, summary = audit_alignment_filter_chunked_fixed(\n",
    "    start_line=1,            # or None to scan all\n",
    "    end_line=200000,\n",
    "    chunk_size=50_000,\n",
    "    thresholds=dict(         # your fixed policy\n",
    "        min_cov_ok=0.50,\n",
    "        max_cov_gap=0.35,\n",
    "        max_int_ratio=0.25,\n",
    "        max_max_int=9,\n",
    "        max_sent_diff=1,\n",
    "        min_sim_ok=0.30,\n",
    "        spill_tolerance=0.60\n",
    "    ),\n",
    "    use_window=True,\n",
    "    max_store_flagged=1500,  # how many examples to keep for inspection\n",
    "    max_store_passed=1500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cdef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>reason</th>\n",
       "      <th>br_cov</th>\n",
       "      <th>pt_cov</th>\n",
       "      <th>cov_gap</th>\n",
       "      <th>base_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118086</td>\n",
       "      <td>118086</td>\n",
       "      <td>coverage_asymmetry</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.585207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147682</td>\n",
       "      <td>147682</td>\n",
       "      <td>low_coverage,coverage_asymmetry,big_interior_unaligned_content</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.360213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71103</td>\n",
       "      <td>71103</td>\n",
       "      <td>low_coverage</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.443878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92292</td>\n",
       "      <td>92292</td>\n",
       "      <td>low_coverage,coverage_asymmetry,big_interior_unaligned_content</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.355494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101935</td>\n",
       "      <td>101935</td>\n",
       "      <td>coverage_asymmetry</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.394440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>168486</td>\n",
       "      <td>168486</td>\n",
       "      <td>low_coverage,coverage_asymmetry,big_interior_unaligned_content,sentence_mismatch</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.477645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33603</td>\n",
       "      <td>33603</td>\n",
       "      <td>low_coverage,sentence_mismatch,low_similarity</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.291908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85756</td>\n",
       "      <td>85756</td>\n",
       "      <td>low_coverage,big_interior_unaligned_content,sentence_mismatch</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.300416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>167771</td>\n",
       "      <td>167771</td>\n",
       "      <td>low_coverage,big_interior_unaligned_content,sentence_mismatch,low_similarity</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.049342</td>\n",
       "      <td>0.281618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162093</td>\n",
       "      <td>162093</td>\n",
       "      <td>big_interior_unaligned_content</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.610884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45895</td>\n",
       "      <td>45895</td>\n",
       "      <td>big_interior_unaligned_content</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.134921</td>\n",
       "      <td>0.342582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>182858</td>\n",
       "      <td>182858</td>\n",
       "      <td>low_coverage,low_similarity</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.268889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>big_interior_unaligned_content,sentence_mismatch</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.136618</td>\n",
       "      <td>0.473450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>97230</td>\n",
       "      <td>97230</td>\n",
       "      <td>big_interior_unaligned_content</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.425505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>183776</td>\n",
       "      <td>183776</td>\n",
       "      <td>coverage_asymmetry</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.444409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>178054</td>\n",
       "      <td>178054</td>\n",
       "      <td>low_coverage,coverage_asymmetry,big_interior_unaligned_content</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.360593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>153108</td>\n",
       "      <td>153108</td>\n",
       "      <td>big_interior_unaligned_content</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.344179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57518</td>\n",
       "      <td>57518</td>\n",
       "      <td>coverage_asymmetry,big_interior_unaligned_content</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.535485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26676</td>\n",
       "      <td>26676</td>\n",
       "      <td>low_coverage,big_interior_unaligned_content,low_similarity</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.171179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "      <td>big_interior_unaligned_content,sentence_mismatch</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.482181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  \\\n",
       "0    118086   118086   \n",
       "1    147682   147682   \n",
       "2     71103    71103   \n",
       "3     92292    92292   \n",
       "4    101935   101935   \n",
       "5    168486   168486   \n",
       "6     33603    33603   \n",
       "7     85756    85756   \n",
       "8    167771   167771   \n",
       "9    162093   162093   \n",
       "10    45895    45895   \n",
       "11   182858   182858   \n",
       "12       81       81   \n",
       "13    97230    97230   \n",
       "14   183776   183776   \n",
       "15   178054   178054   \n",
       "16   153108   153108   \n",
       "17    57518    57518   \n",
       "18    26676    26676   \n",
       "19      124      124   \n",
       "\n",
       "                                                                              reason  \\\n",
       "0                                                                 coverage_asymmetry   \n",
       "1                     low_coverage,coverage_asymmetry,big_interior_unaligned_content   \n",
       "2                                                                       low_coverage   \n",
       "3                     low_coverage,coverage_asymmetry,big_interior_unaligned_content   \n",
       "4                                                                 coverage_asymmetry   \n",
       "5   low_coverage,coverage_asymmetry,big_interior_unaligned_content,sentence_mismatch   \n",
       "6                                      low_coverage,sentence_mismatch,low_similarity   \n",
       "7                      low_coverage,big_interior_unaligned_content,sentence_mismatch   \n",
       "8       low_coverage,big_interior_unaligned_content,sentence_mismatch,low_similarity   \n",
       "9                                                     big_interior_unaligned_content   \n",
       "10                                                    big_interior_unaligned_content   \n",
       "11                                                       low_coverage,low_similarity   \n",
       "12                                  big_interior_unaligned_content,sentence_mismatch   \n",
       "13                                                    big_interior_unaligned_content   \n",
       "14                                                                coverage_asymmetry   \n",
       "15                    low_coverage,coverage_asymmetry,big_interior_unaligned_content   \n",
       "16                                                    big_interior_unaligned_content   \n",
       "17                                 coverage_asymmetry,big_interior_unaligned_content   \n",
       "18                        low_coverage,big_interior_unaligned_content,low_similarity   \n",
       "19                                  big_interior_unaligned_content,sentence_mismatch   \n",
       "\n",
       "      br_cov    pt_cov   cov_gap  base_sim  \n",
       "0   1.000000  0.571429  0.428571  0.585207  \n",
       "1   0.468750  1.000000  0.531250  0.360213  \n",
       "2   0.428571  0.625000  0.196429  0.443878  \n",
       "3   1.000000  0.400000  0.600000  0.355494  \n",
       "4   1.000000  0.625000  0.375000  0.394440  \n",
       "5   0.368421  1.000000  0.631579  0.477645  \n",
       "6   0.555556  0.250000  0.305556  0.291908  \n",
       "7   0.363636  0.300000  0.063636  0.300416  \n",
       "8   0.263158  0.312500  0.049342  0.281618  \n",
       "9   0.636364  0.600000  0.036364  0.610884  \n",
       "10  0.642857  0.777778  0.134921  0.342582  \n",
       "11  0.333333  0.250000  0.083333  0.268889  \n",
       "12  0.622642  0.759259  0.136618  0.473450  \n",
       "13  0.714286  0.727273  0.012987  0.425505  \n",
       "14  1.000000  0.636364  0.363636  0.444409  \n",
       "15  0.833333  0.357143  0.476190  0.360593  \n",
       "16  0.600000  0.833333  0.233333  0.344179  \n",
       "17  1.000000  0.500000  0.500000  0.535485  \n",
       "18  0.333333  0.250000  0.083333  0.171179  \n",
       "19  0.795455  0.794118  0.001337  0.482181  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.head(), summary[\"ratio\"].describe()\n",
    "\n",
    "# samples of IDs that flagged vs passed\n",
    "passed_df.head(20)[[\"line_no\",\"pair_id\",\"br_cov\",\"pt_cov\",\"cov_gap\",\"base_sim\"]]\n",
    "flagged_df.head(20)[[\"line_no\",\"pair_id\",\"reason\",\"br_cov\",\"pt_cov\",\"cov_gap\",\"base_sim\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>activate_filter</th>\n",
       "      <th>reason</th>\n",
       "      <th>base_sim</th>\n",
       "      <th>br_cov</th>\n",
       "      <th>pt_cov</th>\n",
       "      <th>cov_gap</th>\n",
       "      <th>br_int_content_ratio</th>\n",
       "      <th>pt_int_content_ratio</th>\n",
       "      <th>spillish</th>\n",
       "      <th>br_highlight</th>\n",
       "      <th>pt_highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118115</td>\n",
       "      <td>118115</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.636280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sinto muito não poder compartilhar da sua fé na...</td>\n",
       "      <td>&lt;Me&gt; desculpe, não posso compartilhar sua fé em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118116</td>\n",
       "      <td>118116</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Wells e Fargo.</td>\n",
       "      <td>Wells e Fargo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118117</td>\n",
       "      <td>118117</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sim, sim, Wells e Fargo.</td>\n",
       "      <td>Sim, sim, Wells e Fargo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118118</td>\n",
       "      <td>118118</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.743698</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Realmente,&gt; não vejo uma maneira de servi-lo, senhor Pryor. Bem, pensei que fossem [[capazes de]] abrir uma exceção.</td>\n",
       "      <td>&lt;Mas, eu realmente&gt; não vejo, uma maneira de atendê-lo, Sr. Pryor. Bem, pensei que você pudesse abrir uma exceção.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118120</td>\n",
       "      <td>118120</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.984487</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Será um empréstimo tão pequeno e minha reputação aqui nesta comunidade...</td>\n",
       "      <td>Será um empréstimo tão pequeno e minha reputação, aqui nesta comunidade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118121</td>\n",
       "      <td>118121</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.644963</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Desculpe, senhor Pryor. Bom dia, &lt;cavalheiros.&gt;</td>\n",
       "      <td>Desculpe, Sr. Pryor. Bom Dia senhores.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>118125</td>\n",
       "      <td>118125</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Senhor Pryor?</td>\n",
       "      <td>Senhor Pryor?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>118126</td>\n",
       "      <td>118126</td>\n",
       "      <td>False</td>\n",
       "      <td>too_short</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Sim,&gt; Hank.</td>\n",
       "      <td>&lt;Sim,&gt; Hank.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>118127</td>\n",
       "      <td>118127</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.531060</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Eu tenho recusado [[uma porção de]] empregos esperando a saída de sua expedição.</td>\n",
       "      <td>Recusei muitos empregos que aguardavam, a partida de sua expedição.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118128</td>\n",
       "      <td>118128</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.812936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Eu sabia que o senhor iria quer que eu e Pawnee fossemos como no ano passado.</td>\n",
       "      <td>Eu sabia que você gostaria que eu e Pawnee fôssemos como no ano passado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>118129</td>\n",
       "      <td>118129</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Não há ninguém que eu prefira a você, Hank, mas...</td>\n",
       "      <td>Não há ninguém que eu prefira que você, Hank, mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>118130</td>\n",
       "      <td>118130</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.750890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>É possível que eu não mande ninguém este ano, é melhor você pegar o primeiro emprego que aparecer.</td>\n",
       "      <td>É possível que eu não envie ninguém este ano, é melhor você conseguir, o primeiro bom trabalho que surgir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118131</td>\n",
       "      <td>118131</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.753497</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Ao&gt; escritório, Sam.</td>\n",
       "      <td>Para o escritório, Sam.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>118132</td>\n",
       "      <td>118132</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Papai.</td>\n",
       "      <td>Papai.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>118133</td>\n",
       "      <td>118133</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Por que não está na cama?</td>\n",
       "      <td>Por que você não está na cama?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118134</td>\n",
       "      <td>118134</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.547454</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Eu&gt; queria dizer boa noite para você.</td>\n",
       "      <td>Queria te dizer boa noite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>118135</td>\n",
       "      <td>118135</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boa noite.</td>\n",
       "      <td>Boa noite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>118136</td>\n",
       "      <td>118136</td>\n",
       "      <td>False</td>\n",
       "      <td>too_short</td>\n",
       "      <td>0.485680</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Minha&gt; querida.</td>\n",
       "      <td>Boa noite, minha querida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>118137</td>\n",
       "      <td>118137</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.807227</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Esperamos você para jantar durante quase uma hora, Nicholas. Papai... Papai, você comeu alguma coisa? Oh, sim.</td>\n",
       "      <td>Esperamos por você para jantar, por quase uma hora, Nicholas. Pai, você comeu alguma coisa? Oh sim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>118139</td>\n",
       "      <td>118139</td>\n",
       "      <td>False</td>\n",
       "      <td>too_short</td>\n",
       "      <td>0.718144</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;Eu jantei&gt; no hotel.</td>\n",
       "      <td>&lt;Jantei no&gt; hotel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>118140</td>\n",
       "      <td>118140</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.757089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Como está, Talbot?</td>\n",
       "      <td>Como você está, Talbot?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>118141</td>\n",
       "      <td>118141</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.754750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Muito bem, senhor Pryor. Alguma notícia?</td>\n",
       "      <td>Muito bem, Sr. Pryor. Alguma novidade?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>118143</td>\n",
       "      <td>118143</td>\n",
       "      <td>False</td>\n",
       "      <td>too_short</td>\n",
       "      <td>0.650702</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ainda não. [[Foi o]] que pensei.</td>\n",
       "      <td>Ainda não. Eu pensei assim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>118144</td>\n",
       "      <td>118144</td>\n",
       "      <td>False</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.687796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E não tem motivo algum para supor que haverá amanhã ou depois.</td>\n",
       "      <td>E ele não tem razão para supor, que haverá amanhã ou [[depois de]] amanhã.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  activate_filter     reason  base_sim    br_cov  \\\n",
       "0    118115   118115            False         ok  0.636280  1.000000   \n",
       "1    118116   118116            False         ok  1.000000  1.000000   \n",
       "2    118117   118117            False         ok  1.000000  1.000000   \n",
       "3    118118   118118            False         ok  0.743698  0.833333   \n",
       "4    118120   118120            False         ok  0.984487  1.000000   \n",
       "5    118121   118121            False         ok  0.644963  0.833333   \n",
       "6    118125   118125            False         ok  1.000000  1.000000   \n",
       "7    118126   118126            False  too_short  1.000000  0.500000   \n",
       "8    118127   118127            False         ok  0.531060  0.769231   \n",
       "9    118128   118128            False         ok  0.812936  1.000000   \n",
       "10   118129   118129            False         ok  0.933025  1.000000   \n",
       "11   118130   118130            False         ok  0.750890  1.000000   \n",
       "12   118131   118131            False         ok  0.753497  0.666667   \n",
       "13   118132   118132            False         ok  1.000000  1.000000   \n",
       "14   118133   118133            False         ok  0.812500  1.000000   \n",
       "15   118134   118134            False         ok  0.547454  0.857143   \n",
       "16   118135   118135            False         ok  0.812500  1.000000   \n",
       "17   118136   118136            False  too_short  0.485680  0.500000   \n",
       "18   118137   118137            False         ok  0.807227  1.000000   \n",
       "19   118139   118139            False  too_short  0.718144  0.500000   \n",
       "20   118140   118140            False         ok  0.757089  1.000000   \n",
       "21   118141   118141            False         ok  0.754750  1.000000   \n",
       "22   118143   118143            False  too_short  0.650702  0.666667   \n",
       "23   118144   118144            False         ok  0.687796  1.000000   \n",
       "\n",
       "      pt_cov   cov_gap  br_int_content_ratio  pt_int_content_ratio  spillish  \\\n",
       "0   0.875000  0.125000              0.000000                 0.000       0.0   \n",
       "1   1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "2   1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "3   0.842105  0.008772              0.083333                 0.000       0.0   \n",
       "4   1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "5   1.000000  0.166667              0.000000                 0.000       0.0   \n",
       "6   1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "7   0.500000  0.000000              0.000000                 0.000       0.0   \n",
       "8   1.000000  0.230769              0.111111                 0.000       0.0   \n",
       "9   1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "10  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "11  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "12  1.000000  0.333333              0.000000                 0.000       0.0   \n",
       "13  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "14  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "15  1.000000  0.142857              0.000000                 0.000       0.0   \n",
       "16  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "17  1.000000  0.500000              0.000000                 0.000       0.0   \n",
       "18  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "19  0.333333  0.166667              0.000000                 0.000       0.0   \n",
       "20  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "21  1.000000  0.000000              0.000000                 0.000       0.0   \n",
       "22  1.000000  0.333333              0.250000                 0.000       0.0   \n",
       "23  0.857143  0.142857              0.000000                 0.125       0.0   \n",
       "\n",
       "                                                                                                             br_highlight  \\\n",
       "0                                                                      Sinto muito não poder compartilhar da sua fé na...   \n",
       "1                                                                                                          Wells e Fargo.   \n",
       "2                                                                                                Sim, sim, Wells e Fargo.   \n",
       "3   <Realmente,> não vejo uma maneira de servi-lo, senhor Pryor. Bem, pensei que fossem [[capazes de]] abrir uma exceção.   \n",
       "4                                               Será um empréstimo tão pequeno e minha reputação aqui nesta comunidade...   \n",
       "5                                                                         Desculpe, senhor Pryor. Bom dia, <cavalheiros.>   \n",
       "6                                                                                                           Senhor Pryor?   \n",
       "7                                                                                                            <Sim,> Hank.   \n",
       "8                                        Eu tenho recusado [[uma porção de]] empregos esperando a saída de sua expedição.   \n",
       "9                                           Eu sabia que o senhor iria quer que eu e Pawnee fossemos como no ano passado.   \n",
       "10                                                                     Não há ninguém que eu prefira a você, Hank, mas...   \n",
       "11                     É possível que eu não mande ninguém este ano, é melhor você pegar o primeiro emprego que aparecer.   \n",
       "12                                                                                                  <Ao> escritório, Sam.   \n",
       "13                                                                                                                 Papai.   \n",
       "14                                                                                              Por que não está na cama?   \n",
       "15                                                                                 <Eu> queria dizer boa noite para você.   \n",
       "16                                                                                                             Boa noite.   \n",
       "17                                                                                                       <Minha> querida.   \n",
       "18         Esperamos você para jantar durante quase uma hora, Nicholas. Papai... Papai, você comeu alguma coisa? Oh, sim.   \n",
       "19                                                                                                  <Eu jantei> no hotel.   \n",
       "20                                                                                                     Como está, Talbot?   \n",
       "21                                                                               Muito bem, senhor Pryor. Alguma notícia?   \n",
       "22                                                                                       Ainda não. [[Foi o]] que pensei.   \n",
       "23                                                         E não tem motivo algum para supor que haverá amanhã ou depois.   \n",
       "\n",
       "                                                                                                          pt_highlight  \n",
       "0                                                                   <Me> desculpe, não posso compartilhar sua fé em...  \n",
       "1                                                                                                       Wells e Fargo.  \n",
       "2                                                                                             Sim, sim, Wells e Fargo.  \n",
       "3   <Mas, eu realmente> não vejo, uma maneira de atendê-lo, Sr. Pryor. Bem, pensei que você pudesse abrir uma exceção.  \n",
       "4                                           Será um empréstimo tão pequeno e minha reputação, aqui nesta comunidade...  \n",
       "5                                                                               Desculpe, Sr. Pryor. Bom Dia senhores.  \n",
       "6                                                                                                        Senhor Pryor?  \n",
       "7                                                                                                         <Sim,> Hank.  \n",
       "8                                                  Recusei muitos empregos que aguardavam, a partida de sua expedição.  \n",
       "9                                             Eu sabia que você gostaria que eu e Pawnee fôssemos como no ano passado.  \n",
       "10                                                                Não há ninguém que eu prefira que você, Hank, mas...  \n",
       "11          É possível que eu não envie ninguém este ano, é melhor você conseguir, o primeiro bom trabalho que surgir.  \n",
       "12                                                                                             Para o escritório, Sam.  \n",
       "13                                                                                                              Papai.  \n",
       "14                                                                                      Por que você não está na cama?  \n",
       "15                                                                                          Queria te dizer boa noite.  \n",
       "16                                                                                                        Boa noite...  \n",
       "17                                                                                           Boa noite, minha querida.  \n",
       "18                 Esperamos por você para jantar, por quase uma hora, Nicholas. Pai, você comeu alguma coisa? Oh sim.  \n",
       "19                                                                                                  <Jantei no> hotel.  \n",
       "20                                                                                             Como você está, Talbot?  \n",
       "21                                                                              Muito bem, Sr. Pryor. Alguma novidade?  \n",
       "22                                                                                         Ainda não. Eu pensei assim.  \n",
       "23                                          E ele não tem razão para supor, que haverá amanhã ou [[depois de]] amanhã.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq = preview_alignment_quality_window_with_highlights(\n",
    "    start_line=118115, window=30,\n",
    "    use_window=True,           # align against prev+here+next window\n",
    "    show_when=\"all\",\n",
    "    thresholds=dict(\n",
    "        min_cov_ok=0.50,\n",
    "        max_cov_gap=0.35,\n",
    "        max_int_ratio=0.25,\n",
    "        max_max_int=9,\n",
    "        max_sent_diff=1,\n",
    "        min_sim_ok=0.30,\n",
    "        spill_tolerance=0.60\n",
    "    )\n",
    ")\n",
    "\n",
    "hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdd92c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>activate_filter</th>\n",
       "      <th>reason</th>\n",
       "      <th>base_sim</th>\n",
       "      <th>br_cov</th>\n",
       "      <th>pt_cov</th>\n",
       "      <th>cov_gap</th>\n",
       "      <th>br_int_content_ratio</th>\n",
       "      <th>pt_int_content_ratio</th>\n",
       "      <th>br_max_int</th>\n",
       "      <th>pt_max_int</th>\n",
       "      <th>spillish</th>\n",
       "      <th>sent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>97231</td>\n",
       "      <td>97231</td>\n",
       "      <td>True</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.437037</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     line_no  pair_id  activate_filter reason  base_sim    br_cov  pt_cov  \\\n",
       "309    97231    97231             True     ok  0.437037  0.727273    0.75   \n",
       "\n",
       "      cov_gap  br_int_content_ratio  pt_int_content_ratio  br_max_int  \\\n",
       "309  0.022727              0.285714              0.142857           3   \n",
       "\n",
       "     pt_max_int  spillish  sent_diff  \n",
       "309           3       0.1          1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flagged_df[flagged_df['line_no'] == 97231]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27286c3e",
   "metadata": {},
   "source": [
    "**//GIZA++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24584d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>pair_id</b>=123456 → <b>orig0</b>=101728 → <b>clean_idx</b>=101607 | window [101597..101616]<br>Align file: <code>../work_clean/model/aligned.intersect</code> (diagonal band active)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>clean_idx</th>\n",
       "      <th>orig0_idx</th>\n",
       "      <th>★</th>\n",
       "      <th>BR (clean)</th>\n",
       "      <th>BR highlight (unaligned near-diagonal)</th>\n",
       "      <th>PT (clean)</th>\n",
       "      <th>PT highlight (unaligned near-diagonal)</th>\n",
       "      <th>suggest</th>\n",
       "      <th>DB BR (raw)</th>\n",
       "      <th>DB PT (raw)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>101597</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Deixa ao Soapy e aos outros . Não volte a lhes dar dinheiro .</td>\n",
       "      <td>Deixa ao Soapy e aos <span style=\"background:#ffe08a\">outros</span> . Não <span style=\"background:#ffe08a\">volte</span> a lhes <span style=\"background:#ffe08a\">dar</span> <span style=\"background:#ffe08a\">dinheiro</span> .</td>\n",
       "      <td>Soapy e os outros miúdos ... Afasta-te deles , sim ? Não lhes oferecas mais dinheiro . ,</td>\n",
       "      <td>Soapy e os outros <span style=\"background:#ffe08a\">miúdos</span> ... <span style=\"background:#ffe08a\">Afasta-te</span> <span style=\"background:#ffe08a\">deles</span> , <span style=\"background:#ffe08a\">sim</span> ? Não lhes <span style=\"background:#ffe08a\">oferecas</span> <span style=\"background:#ffe08a\">mais</span> <span style=\"background:#ffe08a\">dinheiro</span> . ,</td>\n",
       "      <td></td>\n",
       "      <td>Deixa ao Soapy e aos outros. Não volte a lhes dar dinheiro.</td>\n",
       "      <td>Soapy e os outros miúdos... Afasta-te deles, sim? Não lhes oferecas mais dinheiro.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101598</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Não os faça ...</td>\n",
       "      <td>Não os faça ...</td>\n",
       "      <td>Não os encorajes ...</td>\n",
       "      <td>Não os encorajes ...</td>\n",
       "      <td></td>\n",
       "      <td>Não os faça...</td>\n",
       "      <td>Não os encorajes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101599</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O admirar .</td>\n",
       "      <td>O admirar .</td>\n",
       "      <td>A admirar-te .</td>\n",
       "      <td>A admirar-te .</td>\n",
       "      <td></td>\n",
       "      <td>O admirar.</td>\n",
       "      <td>A admirar-te.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101600</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Está bem . Vou fazer isso . Claro que o fará .</td>\n",
       "      <td>Está bem . Vou fazer isso . Claro que o <span style=\"background:#ffe08a\">fará</span> .</td>\n",
       "      <td>Está bem , garanto-te . Claro que garantes .</td>\n",
       "      <td>Está bem , garanto-te . Claro que garantes .</td>\n",
       "      <td></td>\n",
       "      <td>Está bem. Vou fazer isso. Claro que o fará.</td>\n",
       "      <td>Está bem, garanto-te. Claro que garantes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101601</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Editor executivo de o boletim</td>\n",
       "      <td>Editor executivo de o <span style=\"background:#ffe08a\">boletim</span></td>\n",
       "      <td>Editor-chefe</td>\n",
       "      <td>Editor-chefe</td>\n",
       "      <td></td>\n",
       "      <td>Editor executivo de o boletim</td>\n",
       "      <td>Editor-chefe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101602</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Padre Connelly , queria lhe ajudar , sério . Mas não podemos .</td>\n",
       "      <td>Padre Connelly , queria lhe ajudar , <span style=\"background:#ffe08a\">sério</span> . Mas não <span style=\"background:#ffe08a\">podemos</span> .</td>\n",
       "      <td>Padre Connelly , gostaria de ajudá-lo . Acredite que sim . Mas é impossível .</td>\n",
       "      <td>Padre Connelly , gostaria de ajudá-lo . <span style=\"background:#ffe08a\">Acredite</span> que <span style=\"background:#ffe08a\">sim</span> . Mas é <span style=\"background:#ffe08a\">impossível</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Padre Connelly, queria lhe ajudar, sério. Mas não podemos.</td>\n",
       "      <td>Padre Connelly, gostaria de ajudá-lo. Acredite que sim. Mas é impossível.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101603</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>É uma organização muito forte e muito dura .</td>\n",
       "      <td>É uma organização <span style=\"background:#ffe08a\">muito</span> <span style=\"background:#ffe08a\">forte</span> e <span style=\"background:#ffe08a\">muito</span> <span style=\"background:#ffe08a\">dura</span> .</td>\n",
       "      <td>Não se luta com uma organização tão poderosa .</td>\n",
       "      <td>Não se luta com uma <span style=\"background:#ffe08a\">organização</span> <span style=\"background:#ffe08a\">tão</span> <span style=\"background:#ffe08a\">poderosa</span> .</td>\n",
       "      <td></td>\n",
       "      <td>É uma organização muito forte e muito dura.</td>\n",
       "      <td>Não se luta com uma organização tão poderosa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101604</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Norton J. WHITE EDITOR - IMPRENSA MATUTINA</td>\n",
       "      <td>Norton J. WHITE EDITOR - <span style=\"background:#ffe08a\">IMPRENSA</span> <span style=\"background:#ffe08a\">MATUTINA</span></td>\n",
       "      <td>Editor</td>\n",
       "      <td>Editor</td>\n",
       "      <td></td>\n",
       "      <td>Norton J. WHITE EDITOR - IMPRENSA MATUTINA</td>\n",
       "      <td>Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101605</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Sabe o que me está pedindo ?</td>\n",
       "      <td>Sabe o que me está pedindo ?</td>\n",
       "      <td>Sabe o que está a pedir-me ?</td>\n",
       "      <td>Sabe o que está a pedir-me ?</td>\n",
       "      <td></td>\n",
       "      <td>Sabe o que me está pedindo?</td>\n",
       "      <td>Sabe o que está a pedir-me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101606</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Sim , Sr. White .</td>\n",
       "      <td>Sim , Sr. White .</td>\n",
       "      <td>Sim , Mr . White .</td>\n",
       "      <td>Sim , Mr . <span style=\"background:#ffe08a\">White</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Sim, Sr. White.</td>\n",
       "      <td>Sim, Mr. White.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101607</td>\n",
       "      <td>101728</td>\n",
       "      <td>★</td>\n",
       "      <td>Nos outros jornais lhe explicaram os perigos .</td>\n",
       "      <td>Nos outros jornais lhe <span style=\"background:#ffe08a\">explicaram</span> os <span style=\"background:#ffe08a\">perigos</span> .</td>\n",
       "      <td>Os outros jornais esmeraram-se a explicar-me os riscos .</td>\n",
       "      <td>Os outros jornais <span style=\"background:#ffe08a\">esmeraram-se</span> a <span style=\"background:#ffe08a\">explicar-me</span> os <span style=\"background:#ffe08a\">riscos</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Nos outros jornais lhe explicaram os perigos.</td>\n",
       "      <td>Os outros jornais esmeraram-se a explicar-me os riscos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101608</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Quer lutar contra eles ?</td>\n",
       "      <td>Quer lutar contra eles ?</td>\n",
       "      <td>Travará o combate pessoalmente ?</td>\n",
       "      <td>Travará o combate pessoalmente ?</td>\n",
       "      <td></td>\n",
       "      <td>Quer lutar contra eles?</td>\n",
       "      <td>Travará o combate pessoalmente?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101609</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Dedicar todo seu tempo ?</td>\n",
       "      <td>Dedicar todo seu tempo ?</td>\n",
       "      <td>Dedicar-lhe-á todo o seu tempo ?</td>\n",
       "      <td>Dedicar-lhe-á todo o seu tempo ?</td>\n",
       "      <td></td>\n",
       "      <td>Dedicar todo seu tempo?</td>\n",
       "      <td>Dedicar-lhe-á todo o seu tempo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101610</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Me creia , nada eu gostaria mais .</td>\n",
       "      <td>Me creia , nada eu gostaria mais .</td>\n",
       "      <td>Acredite que nada me daria mais prazer .</td>\n",
       "      <td>Acredite que nada me daria mais prazer .</td>\n",
       "      <td></td>\n",
       "      <td>Me creia, nada eu gostaria mais.</td>\n",
       "      <td>Acredite que nada me daria mais prazer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101611</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Muito bem , Padre Connelly , estou com você .</td>\n",
       "      <td>Muito bem , Padre Connelly , estou com você .</td>\n",
       "      <td>Muito bem , Padre Connelly , estou do seu lado .</td>\n",
       "      <td>Muito bem , Padre Connelly , estou do seu <span style=\"background:#ffe08a\">lado</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Muito bem, Padre Connelly, estou com você.</td>\n",
       "      <td>Muito bem, Padre Connelly, estou do seu lado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101612</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O jornais o respaldará .</td>\n",
       "      <td>O jornais o respaldará .</td>\n",
       "      <td>A imprensa apoiá-Io-á até ao fim .</td>\n",
       "      <td>A imprensa apoiá-Io-á até ao fim .</td>\n",
       "      <td></td>\n",
       "      <td>O jornais o respaldará.</td>\n",
       "      <td>A imprensa apoiá-Io-á até ao fim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101613</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Obrigado , o agradeço . Padre declara guerra ao mundo do vício O Padre Connelly Dirigirá Um Movimento de Reforma \" Investiguem o rocky ! \" Exigem os cidadãos</td>\n",
       "      <td>Obrigado , o agradeço . Padre declara <span style=\"background:#ffe08a\">guerra</span> ao <span style=\"background:#ffe08a\">mundo</span> do <span style=\"background:#ffe08a\">vício</span> O <span style=\"background:#ffe08a\">Padre</span> <span style=\"background:#ffe08a\">Connelly</span> <span style=\"background:#ffe08a\">Dirigirá</span> Um <span style=\"background:#ffe08a\">Movimento</span> de <span style=\"background:#ffe08a\">Reforma</span> &quot; <span style=\"background:#ffe08a\">Investiguem</span> o <span style=\"background:#ffe08a\">rocky</span> ! &quot; <span style=\"background:#ffe08a\">Exigem</span> os <span style=\"background:#ffe08a\">cidadãos</span></td>\n",
       "      <td>Obrigado . Agradeço-lhe . Padre declara guerra ao submundo do crime ! Padre Connelly Diz que Conduzirá uma Verdadeira Reforma \" Investiguem rocky ! \" É exigência dos cidadãos</td>\n",
       "      <td>Obrigado . Agradeço-lhe . Padre declara guerra ao <span style=\"background:#ffe08a\">submundo</span> do <span style=\"background:#ffe08a\">crime</span> ! <span style=\"background:#ffe08a\">Padre</span> <span style=\"background:#ffe08a\">Connelly</span> <span style=\"background:#ffe08a\">Diz</span> que <span style=\"background:#ffe08a\">Conduzirá</span> uma <span style=\"background:#ffe08a\">Verdadeira</span> <span style=\"background:#ffe08a\">Reforma</span> &quot; <span style=\"background:#ffe08a\">Investiguem</span> <span style=\"background:#ffe08a\">rocky</span> ! &quot; É <span style=\"background:#ffe08a\">exigência</span> dos <span style=\"background:#ffe08a\">cidadãos</span></td>\n",
       "      <td></td>\n",
       "      <td>Obrigado, o agradeço. Padre declara guerra ao mundo do vício O Padre Connelly Dirigirá Um Movimento de Reforma \"Investiguem o rocky!\"Exigem os cidadãos</td>\n",
       "      <td>Obrigado. Agradeço-lhe. Padre declara guerra ao submundo do crime! Padre Connelly Diz que Conduzirá uma Verdadeira Reforma \"Investiguem rocky!\"É exigência dos cidadãos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101614</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Se denunciará Poder dos Gângsters Sobre os Funcionários</td>\n",
       "      <td>Se denunciará Poder dos Gângsters <span style=\"background:#ffe08a\">Sobre</span> os <span style=\"background:#ffe08a\">Funcionários</span></td>\n",
       "      <td>Poder do Bandido Sobre Autoridades Municipais Exposto Dentro de Dias !</td>\n",
       "      <td>Poder do Bandido Sobre Autoridades <span style=\"background:#ffe08a\">Municipais</span> <span style=\"background:#ffe08a\">Exposto</span> <span style=\"background:#ffe08a\">Dentro</span> de <span style=\"background:#ffe08a\">Dias</span> !</td>\n",
       "      <td></td>\n",
       "      <td>Se denunciará Poder dos Gângsters Sobre os Funcionários</td>\n",
       "      <td>Poder do Bandido Sobre Autoridades Municipais Exposto Dentro de Dias!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101615</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Quero a verdade sobre as contas do Frazier e Keefer .</td>\n",
       "      <td>Quero a verdade sobre as contas do Frazier e <span style=\"background:#ffe08a\">Keefer</span> .</td>\n",
       "      <td>Quero saber das contas bancárias do Frazier e do Keefer .</td>\n",
       "      <td>Quero saber das contas bancárias do Frazier e do <span style=\"background:#ffe08a\">Keefer</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Quero a verdade sobre as contas do Frazier e Keefer.</td>\n",
       "      <td>Quero saber das contas bancárias do Frazier e do Keefer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101616</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Não importa como , mas averiguem .</td>\n",
       "      <td>Não importa como , mas <span style=\"background:#ffe08a\">averiguem</span> .</td>\n",
       "      <td>Não me interessa onde arranjam a informação .</td>\n",
       "      <td>Não me interessa onde arranjam a <span style=\"background:#ffe08a\">informação</span> .</td>\n",
       "      <td></td>\n",
       "      <td>Não importa como, mas averiguem.</td>\n",
       "      <td>Não me interessa onde arranjam a informação.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== BOTH-SIDE PREVIEW with DIAGONAL-BAND FILTER (catches edge spillovers) ====\n",
    "import io, html, duckdb, pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# paths\n",
    "BR_PATH    = \"../data/corpus.clean.br\"\n",
    "PT_PATH    = \"../data/corpus.clean.pt\"\n",
    "ALIGN_PATH = \"../work_clean/model/aligned.intersect\"   # stricter file recommended\n",
    "PAIR2IDX   = \"../data/pairid_to_idx.tsv\"               # optional\n",
    "KEEP_IDX   = \"../data/keep.idx\"                        # optional\n",
    "DB_PATH    = \"../data/duckdb/subs.duckdb\"\n",
    "DB_TABLE   = \"opus_moses\"\n",
    "\n",
    "PAIR_ID = 123456   # <--- set your pair_id\n",
    "WINDOW  = 20\n",
    "\n",
    "# content heuristics\n",
    "PT_STOP = {\"de\",\"do\",\"da\",\"dos\",\"das\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\"com\",\"para\",\"por\",\"a\",\"ao\",\"à\",\"às\",\"aos\",\n",
    "           \"o\",\"a\",\"os\",\"as\",\"um\",\"uma\",\"uns\",\"umas\",\"este\",\"esta\",\"estes\",\"estas\",\"esse\",\"essa\",\"esses\",\"essas\",\n",
    "           \"aquele\",\"aquela\",\"aqueles\",\"aquelas\",\"me\",\"te\",\"se\",\"lhe\",\"nos\",\"vos\",\"lhes\",\"e\",\"ou\",\"mas\",\"nem\",\n",
    "           \"que\",\"porque\",\"pois\",\"porém\",\"porem\",\"não\",\"nao\"}\n",
    "DISFL = {\"uh\",\"uhm\",\"uhmm\",\"hã\",\"aham\",\"é\",\"uh-hmm\",\"uh-hum\",\"hum\",\"humm\"}\n",
    "def is_punct(t): return all(ch in \",.;:!?…—–-()[]{}\\\"'«»“”\" for ch in t)\n",
    "def is_content(t):\n",
    "    t=t.lower(); return (t not in PT_STOP) and (t not in DISFL) and (not is_punct(t)) and (len(t)>1)\n",
    "\n",
    "def parse_links(line):\n",
    "    out=set()\n",
    "    for z in (line or \"\").split():\n",
    "        if \"-\" in z:\n",
    "            i,j=z.split(\"-\"); out.add((int(i),int(j)))\n",
    "    return out\n",
    "\n",
    "def get_line(path, k):\n",
    "    with io.open(path,\"r\",encoding=\"utf8\") as f:\n",
    "        for i,ln in enumerate(f):\n",
    "            if i==k: return ln.rstrip(\"\\n\")\n",
    "    return \"\"\n",
    "\n",
    "# diagonal-band gating: keep only links close to the diagonal\n",
    "def band_filter_links(links, len_s, len_t, frac=0.20, abs_px=3):\n",
    "    \"\"\"Keep (i,j) if | j - (i*len_t/len_s) | <= max(abs_px, frac*len_t).\"\"\"\n",
    "    if len_s==0 or len_t==0: return set()\n",
    "    out=set()\n",
    "    band = max(abs_px, int(round(frac*len_t)))\n",
    "    ratio = len_t/len_s\n",
    "    for i,j in links:\n",
    "        target_on_diag = i*ratio\n",
    "        if abs(j - target_on_diag) <= band:\n",
    "            out.add((i,j))\n",
    "    return out\n",
    "\n",
    "# mappings\n",
    "pair2orig={}\n",
    "if Path(PAIR2IDX).exists():\n",
    "    with io.open(PAIR2IDX,\"r\",encoding=\"utf8\") as f:\n",
    "        for ln in f:\n",
    "            pid, idx = ln.rstrip(\"\\n\").split(\"\\t\")\n",
    "            pair2orig[int(pid)] = int(idx)\n",
    "orig0_to_clean=None; clean_to_orig0=None\n",
    "if Path(KEEP_IDX).exists():\n",
    "    keep=[int(x.strip()) for x in io.open(KEEP_IDX,\"r\",encoding=\"utf8\")]\n",
    "    orig0_to_clean={orig1-1: c for c,orig1 in enumerate(keep)}\n",
    "    clean_to_orig0=[orig1-1 for orig1 in keep]\n",
    "\n",
    "def map_pair_to_clean(pid):\n",
    "    orig0 = pair2orig.get(pid, pid)\n",
    "    if orig0_to_clean is None: return orig0, orig0\n",
    "    return orig0_to_clean.get(orig0), orig0\n",
    "\n",
    "def highlight(tokens, covered_idx):\n",
    "    bits=[]\n",
    "    for i,tok in enumerate(tokens):\n",
    "        txt=html.escape(tok)\n",
    "        if (i not in covered_idx) and is_content(tok):\n",
    "            bits.append(f'<span style=\"background:#ffe08a\">{txt}</span>')\n",
    "        else:\n",
    "            bits.append(txt)\n",
    "    return \" \".join(bits)\n",
    "\n",
    "def edge_runs(tokens, covered_idx):\n",
    "    n=len(tokens)\n",
    "    pre=suf=0\n",
    "    for i in range(n):\n",
    "        if (i not in covered_idx) and is_content(tokens[i]): pre+=1\n",
    "        else: break\n",
    "    for i in range(n-1,-1,-1):\n",
    "        if (i not in covered_idx) and is_content(tokens[i]): suf+=1\n",
    "        else: break\n",
    "    return pre, suf\n",
    "\n",
    "# build the window\n",
    "clean_idx, orig0 = map_pair_to_clean(PAIR_ID)\n",
    "assert clean_idx is not None, f\"Original {orig0} missing in clean corpus\"\n",
    "half=max(1,WINDOW//2); start=max(0,clean_idx-half); end=clean_idx+(WINDOW-half-1)\n",
    "\n",
    "# optional DB slice\n",
    "db_map={}\n",
    "if Path(DB_PATH).exists():\n",
    "    con=duckdb.connect(DB_PATH, read_only=True)\n",
    "    o_start = clean_to_orig0[start] if clean_to_orig0 else start\n",
    "    o_end   = clean_to_orig0[min(end,len(clean_to_orig0)-1)] if clean_to_orig0 else end\n",
    "    q=f\"\"\"\n",
    "    WITH t AS (SELECT row_number() OVER ()-1 AS orig0, sent_pt_br, sent_pt_pt FROM {DB_TABLE})\n",
    "    SELECT * FROM t WHERE orig0 BETWEEN ? AND ? ORDER BY orig0\n",
    "    \"\"\"\n",
    "    df_db=con.execute(q,[int(o_start),int(o_end)]).df()\n",
    "    db_map={int(r.orig0):(r.sent_pt_br, r.sent_pt_pt) for _,r in df_db.iterrows()}\n",
    "\n",
    "rows=[]\n",
    "for k in range(start, end+1):\n",
    "    br = (get_line(BR_PATH,k) or \"\").split()\n",
    "    pt = (get_line(PT_PATH,k) or \"\").split()\n",
    "    raw_links = parse_links(get_line(ALIGN_PATH,k))\n",
    "    links = band_filter_links(raw_links, len(br), len(pt), frac=0.20, abs_px=3)\n",
    "    br_cov = {i for (i,_) in links}\n",
    "    pt_cov = {j for (_,j) in links}\n",
    "    o0=clean_to_orig0[k] if clean_to_orig0 else k\n",
    "    raw_br, raw_pt = db_map.get(o0, (\"\",\"\"))\n",
    "    pre_br, suf_br = edge_runs(br, br_cov)\n",
    "    pre_pt, suf_pt = edge_runs(pt, pt_cov)\n",
    "    suggest=[]\n",
    "    if pre_br>=3: suggest.append(f\"cut BR prefix {pre_br}T\")\n",
    "    if suf_br>=3: suggest.append(f\"cut BR suffix {suf_br}T\")\n",
    "    if pre_pt>=3: suggest.append(f\"cut PT prefix {pre_pt}T\")\n",
    "    if suf_pt>=3: suggest.append(f\"cut PT suffix {suf_pt}T\")\n",
    "\n",
    "    rows.append({\n",
    "        \"clean_idx\": k,\n",
    "        \"orig0_idx\": o0 if k==clean_idx else \"\",\n",
    "        \"★\": \"★\" if k==clean_idx else \"\",\n",
    "        \"BR (clean)\": \" \".join(br),\n",
    "        \"BR highlight (unaligned near-diagonal)\": highlight(br, br_cov),\n",
    "        \"PT (clean)\": \" \".join(pt),\n",
    "        \"PT highlight (unaligned near-diagonal)\": highlight(pt, pt_cov),\n",
    "        \"suggest\": \" | \".join(suggest),\n",
    "        \"DB BR (raw)\": raw_br,\n",
    "        \"DB PT (raw)\": raw_pt\n",
    "    })\n",
    "\n",
    "display(HTML(f\"<p><b>pair_id</b>={PAIR_ID} → <b>orig0</b>={orig0} → <b>clean_idx</b>={clean_idx} | window [{start}..{end}]<br>\"\n",
    "             f\"Align file: <code>{ALIGN_PATH}</code> (diagonal band active)</p>\"))\n",
    "display(HTML(pd.DataFrame(rows).to_html(escape=False, index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b54b5998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>sent_pt_br</th>\n",
       "      <th>sent_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118130</td>\n",
       "      <td>118130</td>\n",
       "      <td>É possível que eu não mande ninguém este ano, é melhor você pegar o primeiro emprego que aparecer.</td>\n",
       "      <td>É possível que eu não envie ninguém este ano, é melhor você conseguir, o primeiro bom trabalho que surgir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118131</td>\n",
       "      <td>118131</td>\n",
       "      <td>Ao escritório, Sam.</td>\n",
       "      <td>Para o escritório, Sam.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118132</td>\n",
       "      <td>118132</td>\n",
       "      <td>Papai.</td>\n",
       "      <td>Papai.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118133</td>\n",
       "      <td>118133</td>\n",
       "      <td>Por que não está na cama?</td>\n",
       "      <td>Por que você não está na cama?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118134</td>\n",
       "      <td>118134</td>\n",
       "      <td>Eu queria dizer boa noite para você.</td>\n",
       "      <td>Queria te dizer boa noite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118135</td>\n",
       "      <td>118135</td>\n",
       "      <td>Boa noite.</td>\n",
       "      <td>Boa noite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>118136</td>\n",
       "      <td>118136</td>\n",
       "      <td>Minha querida.</td>\n",
       "      <td>Boa noite, minha querida.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>118137</td>\n",
       "      <td>118137</td>\n",
       "      <td>Esperamos você para jantar durante quase uma hora, Nicholas. Papai... Papai, você comeu alguma coisa? Oh, sim.</td>\n",
       "      <td>Esperamos por você para jantar, por quase uma hora, Nicholas. Pai, você comeu alguma coisa? Oh sim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>118139</td>\n",
       "      <td>118139</td>\n",
       "      <td>Eu jantei no hotel.</td>\n",
       "      <td>Jantei no hotel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118140</td>\n",
       "      <td>118140</td>\n",
       "      <td>Como está, Talbot?</td>\n",
       "      <td>Como você está, Talbot?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>118141</td>\n",
       "      <td>118141</td>\n",
       "      <td>Muito bem, senhor Pryor. Alguma notícia?</td>\n",
       "      <td>Muito bem, Sr. Pryor. Alguma novidade?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>118143</td>\n",
       "      <td>118143</td>\n",
       "      <td>Ainda não. Foi o que pensei.</td>\n",
       "      <td>Ainda não. Eu pensei assim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118144</td>\n",
       "      <td>118144</td>\n",
       "      <td>E não tem motivo algum para supor que haverá amanhã ou depois.</td>\n",
       "      <td>E ele não tem razão para supor, que haverá amanhã ou depois de amanhã.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>118145</td>\n",
       "      <td>118145</td>\n",
       "      <td>Não precisa falar assim, tenho muita confiança nos meus agentes.</td>\n",
       "      <td>Não precisa falar assim, tenho muita confiança nos meus agentes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>118146</td>\n",
       "      <td>118146</td>\n",
       "      <td>Mas eu não tenho e de qualquer maneira ficar aqui sentado esperando... não vai adiantar nada.</td>\n",
       "      <td>Mas eu não preciso e eu só fico aqui sentado esperanda... não vai adiantar nada.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  \\\n",
       "0    118130   118130   \n",
       "1    118131   118131   \n",
       "2    118132   118132   \n",
       "3    118133   118133   \n",
       "4    118134   118134   \n",
       "5    118135   118135   \n",
       "6    118136   118136   \n",
       "7    118137   118137   \n",
       "8    118139   118139   \n",
       "9    118140   118140   \n",
       "10   118141   118141   \n",
       "11   118143   118143   \n",
       "12   118144   118144   \n",
       "13   118145   118145   \n",
       "14   118146   118146   \n",
       "\n",
       "                                                                                                        sent_pt_br  \\\n",
       "0               É possível que eu não mande ninguém este ano, é melhor você pegar o primeiro emprego que aparecer.   \n",
       "1                                                                                              Ao escritório, Sam.   \n",
       "2                                                                                                           Papai.   \n",
       "3                                                                                        Por que não está na cama?   \n",
       "4                                                                             Eu queria dizer boa noite para você.   \n",
       "5                                                                                                       Boa noite.   \n",
       "6                                                                                                   Minha querida.   \n",
       "7   Esperamos você para jantar durante quase uma hora, Nicholas. Papai... Papai, você comeu alguma coisa? Oh, sim.   \n",
       "8                                                                                              Eu jantei no hotel.   \n",
       "9                                                                                               Como está, Talbot?   \n",
       "10                                                                        Muito bem, senhor Pryor. Alguma notícia?   \n",
       "11                                                                                    Ainda não. Foi o que pensei.   \n",
       "12                                                  E não tem motivo algum para supor que haverá amanhã ou depois.   \n",
       "13                                                Não precisa falar assim, tenho muita confiança nos meus agentes.   \n",
       "14                   Mas eu não tenho e de qualquer maneira ficar aqui sentado esperando... não vai adiantar nada.   \n",
       "\n",
       "                                                                                                    sent_pt_pt  \n",
       "0   É possível que eu não envie ninguém este ano, é melhor você conseguir, o primeiro bom trabalho que surgir.  \n",
       "1                                                                                      Para o escritório, Sam.  \n",
       "2                                                                                                       Papai.  \n",
       "3                                                                               Por que você não está na cama?  \n",
       "4                                                                                   Queria te dizer boa noite.  \n",
       "5                                                                                                 Boa noite...  \n",
       "6                                                                                    Boa noite, minha querida.  \n",
       "7          Esperamos por você para jantar, por quase uma hora, Nicholas. Pai, você comeu alguma coisa? Oh sim.  \n",
       "8                                                                                             Jantei no hotel.  \n",
       "9                                                                                      Como você está, Talbot?  \n",
       "10                                                                      Muito bem, Sr. Pryor. Alguma novidade?  \n",
       "11                                                                                 Ainda não. Eu pensei assim.  \n",
       "12                                      E ele não tem razão para supor, que haverá amanhã ou depois de amanhã.  \n",
       "13                                            Não precisa falar assim, tenho muita confiança nos meus agentes.  \n",
       "14                            Mas eu não preciso e eu só fico aqui sentado esperanda... não vai adiantar nada.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(str(DB))\n",
    "df = con.execute(\"\"\"\n",
    "  SELECT *\n",
    "  FROM opus_moses\n",
    "  WHERE pair_id >= ?\n",
    "  ORDER BY pair_id\n",
    "  LIMIT 15\n",
    "\"\"\", [118130]).df()\n",
    "con.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3542d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
