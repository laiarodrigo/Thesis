{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84263414",
   "metadata": {},
   "source": [
    "**//IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facf6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiarodrigo/repos/Thesis/thesis/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/laiarodrigo/repos/Thesis/eda's/data/duckdb/subs.duckdb\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd, pathlib, itertools, textwrap, re, gc\n",
    "import numpy as np\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from rapidfuzz import fuzz, distance\n",
    "from simalign import SentenceAligner\n",
    "from typing import Optional, Dict, Any, Tuple, List, Iterable, Union\n",
    "import os; print(os.path.abspath(\"data/duckdb/subs.duckdb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951770de",
   "metadata": {},
   "source": [
    "**//CONFIGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4085ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = '../data/duckdb/subs.duckdb'\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>|\\{[^}]+\\}')\n",
    "NL_RE  = re.compile(r'\\s*\\n\\s*')\n",
    "SENT_SPLIT_RE = re.compile(r'(?<=[\\.\\?\\!…])\\s+')\n",
    "\n",
    "ABBREVS = {\"dr\",\"dra\",\"sr\",\"sra\",\"srta\",\"prof\",\"profa\",\"etc\",\"av\",\"nº\",\"n.º\",\"vs\",\"p.ex\"}\n",
    "ABBR_RX = re.compile(r'\\b(' + '|'.join(re.escape(x) for x in ABBREVS) + r')\\.', re.IGNORECASE)\n",
    "\n",
    "# PREPROCESSING\n",
    "QUOTE_PAIRS = [\n",
    "    ('\"', '\"'), (\"'\", \"'\"),\n",
    "    ('“', '”'), ('‘', '’'),\n",
    "    ('«', '»'), ('„', '“'), ('‹', '›')\n",
    "]\n",
    "QUOTE_CHARS = set(ch for L,R in QUOTE_PAIRS for ch in (L,R))\n",
    "\n",
    "HARD_STOPS = \".?!…\"\n",
    "SOFT_TAILS = \",;:—–-\"\n",
    "DASHES     = \"-–—\"\n",
    "\n",
    "LETTER      = r\"[^\\W\\d_]\"                                   # any letter, no digits/underscore\n",
    "NAME_TOKEN  = rf\"{LETTER}(?:{LETTER}|[.'\\-])*\"              # e.g., Steve, O'Neill, João-Pedro\n",
    "NAME_PHRASE = rf\"{NAME_TOKEN}(?:\\s+{NAME_TOKEN}){{0,2}}\"    # up to 3-word names\n",
    "\n",
    "WS = r\"(?:\\s|\\u00A0|\\u202F)*\"                               # normal/narrow/nbsp\n",
    "SPEAKER_LABEL_DROP = re.compile(\n",
    "    rf\"(^|[^\\w]){NAME_PHRASE}{WS}[:\\uFF1A]{WS}\",            # keep boundary, drop label\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "WORD = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
    "# Lightweight PT stopword set for \"content-token\" accounting\n",
    "PT_PREPS = {\"de\",\"do\",\"da\",\"dos\",\"das\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\"com\",\"para\",\"por\",\"a\",\"ao\",\"à\",\"às\",\"aos\"}\n",
    "PT_DETS  = {\"o\",\"a\",\"os\",\"as\",\"um\",\"uma\",\"uns\",\"umas\",\"este\",\"esta\",\"estes\",\"estas\",\"esse\",\"essa\",\"esses\",\"essas\",\"aquele\",\"aquela\",\"aqueles\",\"aquelas\"}\n",
    "PT_CLITICS={\"me\",\"te\",\"se\",\"lhe\",\"nos\",\"vos\",\"lhes\"}\n",
    "PT_CONJ  = {\"e\",\"ou\",\"mas\",\"nem\",\"que\",\"porque\",\"pois\",\"porém\",\"porem\"}\n",
    "PT_NEG   = {\"não\",\"nao\"}\n",
    "PT_STOPWORDS = (PT_PREPS | PT_DETS | PT_CLITICS | PT_CONJ | PT_NEG)\n",
    "\n",
    "# sentence splitter (no look-behind) for light stats only\n",
    "_SENT_RE = re.compile(r'.*?[.!?…]+(?:[\"”»\\'\\)\\]\\}]+)?(?=\\s|$)|.+?(?=\\s|$)', re.UNICODE)\n",
    "\n",
    "ALIGN_METHOD = \"inter\"  # alternatives: \"inter\" (↑recall), \"mwmf\", \"itermax\", \"union\"\n",
    "\n",
    "# === alignment tokenizer (use ONLY for SimAlign) ===\n",
    "WS_TOKEN = re.compile(r\"\\S+\", re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623fca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- cleaning + similarity ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return TAG_RE.sub('', NL_RE.sub(' ', s)).strip()\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    a = clean_text(a); b = clean_text(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    # edit-distance core signals (all penalize insertions/deletions)\n",
    "    s_edit  = fuzz.ratio(a, b) / 100.0\n",
    "    s_sort  = fuzz.token_sort_ratio(a, b) / 100.0         # order-insensitive but still length-aware\n",
    "    s_lev   = distance.Levenshtein.normalized_similarity(a, b)  # 0..1\n",
    "\n",
    "    # penalize big length mismatches (e.g., a is much longer than b)\n",
    "    lp = min(len(a), len(b)) / max(len(a), len(b))  # 0..1\n",
    "\n",
    "    # blend; weights are tame and easy to tune\n",
    "    base = 0.5*s_edit + 0.2*s_sort + 0.3*s_lev\n",
    "    return base * (0.5 + 0.5*lp)   # shrink score when lengths differ a lot\n",
    "\n",
    "def new_sim():\n",
    "    pass\n",
    "\n",
    "# ---------- clause split (sentences first, comma/dash fallback) ----------\n",
    "def mask_abbrevs(t: str) -> str: return ABBR_RX.sub(lambda m: m.group(1)+\"§\", t or \"\")\n",
    "def unmask_abbrevs(t: str) -> str: return (t or \"\").replace(\"§\",\".\")\n",
    "\n",
    "def sentence_split(t: str):\n",
    "    tt = mask_abbrevs(t or \"\")\n",
    "    parts = [unmask_abbrevs(p).strip() for p in SENT_SPLIT_RE.split(tt.strip()) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def split_tail_clause(text: str, max_tail_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = ' '.join(parts[:-1]).strip(); tail = parts[-1].strip()\n",
    "        if head and tail: return head, tail\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.rfind(token)\n",
    "        if k != -1 and 1 <= len(t) - (k+len(token)) <= max_tail_chars:\n",
    "            return t[:k].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def split_head_clause(text: str, max_head_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = parts[0].strip(); rest = ' '.join(parts[1:]).strip()\n",
    "        if head and rest: return head, rest\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.find(token)\n",
    "        if k != -1 and 1 <= k+1 <= max_head_chars:\n",
    "            return t[:k+len(token)].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def ok_piece(seg: str, min_chars=6, min_tokens=2):\n",
    "    toks = [w for w in re.findall(r'\\b\\w+\\b', seg or \"\", flags=re.UNICODE) if any(c.isalpha() for c in w)]\n",
    "    return bool(seg) and len(seg) >= min_chars and len(toks) >= min_tokens\n",
    "\n",
    "def _py_int(x):\n",
    "    # robust cast for numpy/pandas scalars and plain ints\n",
    "    if isinstance(x, (np.generic,)):  # np.int64, np.int32, etc.\n",
    "        return int(x.item())\n",
    "    return int(x)\n",
    "\n",
    "def load_opus_window(start_line: int, window: int = 600) -> pd.DataFrame:\n",
    "    start_line = _py_int(start_line)\n",
    "    window     = _py_int(window)\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [start_line, start_line + window - 1]).df()\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "\n",
    "# ---------- PASS A: neighbor MOVES (choose tail→next or head←next if it increases sum) ----------\n",
    "def apply_neighbor_moves(df: pd.DataFrame, margin=0.04, max_clause_chars=60):\n",
    "    df2 = df.copy()\n",
    "    log = []\n",
    "    n = len(df2)\n",
    "    for i in range(n-1):\n",
    "        for lang, other in ((\"sent_pt_pt\",\"sent_pt_br\"), (\"sent_pt_br\",\"sent_pt_pt\")):\n",
    "            L_i,  L_ip1  = df2.at[i,lang],     df2.at[i+1,lang]\n",
    "            R_i,  R_ip1  = df2.at[i,other],    df2.at[i+1,other]\n",
    "            keep_sum = sim(L_i,R_i) + sim(L_ip1,R_ip1)\n",
    "\n",
    "            # option 1: move tail of i -> front of i+1\n",
    "            head, tail = split_tail_clause(L_i, max_tail_chars=max_clause_chars)\n",
    "            gain1 = -1e9\n",
    "            if ok_piece(tail) and ok_piece(head, min_chars=4):\n",
    "                move_sum1 = sim(head, R_i) + sim((tail + \" \" + (L_ip1 or \"\")).strip(), R_ip1)\n",
    "                gain1 = move_sum1 - keep_sum\n",
    "\n",
    "            # option 2: move head of i+1 -> end of i\n",
    "            head2, rest2 = split_head_clause(L_ip1, max_head_chars=max_clause_chars)\n",
    "            gain2 = -1e9\n",
    "            if ok_piece(head2) and ok_piece(rest2, min_chars=4):\n",
    "                move_sum2 = sim(((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip(), R_i) + sim(rest2, R_ip1)\n",
    "                gain2 = move_sum2 - keep_sum\n",
    "\n",
    "            # apply the better positive option\n",
    "            if gain1 > margin and gain1 >= gain2:\n",
    "                df2.at[i,lang]     = head\n",
    "                df2.at[i+1,lang]   = (tail + \" \" + (L_ip1 or \"\")).strip()\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"tail_to_next\", \"gain\": float(gain1)})\n",
    "            elif gain2 > margin and gain2 > gain1:\n",
    "                df2.at[i,lang]     = (((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip())\n",
    "                df2.at[i+1,lang]   = rest2\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"head_from_next\", \"gain\": float(gain2)})\n",
    "            # else: no move\n",
    "    return df2, pd.DataFrame(log)\n",
    "\n",
    "# # ---------- example run on a tiny window ----------\n",
    "# df  = load_opus_window(start_line=13580016, window=10)\n",
    "\n",
    "# # A) move commas/clauses across neighbors when it helps the two-row sum\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "\n",
    "# print(\"Moves:\", len(move_log))\n",
    "# print(move_log.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f006fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _moved_piece(bi, ai, bip1, aip1, op):\n",
    "    \"\"\"Best-effort extract of the moved fragment from before/after strings.\"\"\"\n",
    "    if op == \"tail_to_next\":\n",
    "        # ai = head; piece = suffix removed from bi\n",
    "        if bi.startswith(ai):\n",
    "            return bi[len(ai):].strip()\n",
    "        # fallback: prefix added to next\n",
    "        added = max(0, len(aip1) - len(bip1))\n",
    "        return aip1[:added].strip()\n",
    "    else:  # \"head_from_next\"\n",
    "        # aip1 = rest; piece = prefix removed from bip1\n",
    "        if len(bip1) > len(aip1):\n",
    "            return bip1[:len(bip1) - len(aip1)].strip()\n",
    "        # fallback: suffix added to i\n",
    "        if ai.startswith(bi):\n",
    "            return ai[len(bi):].strip()\n",
    "        return \"\"\n",
    "\n",
    "def preview_moves(df_before, df_after, move_log, k=8):\n",
    "    \"\"\"\n",
    "    Show top-k moves with before/after texts, moved fragment, and score deltas.\n",
    "    Assumes df_before/df_after are the same window (same line_no order).\n",
    "    \"\"\"\n",
    "    if move_log is None or move_log.empty:\n",
    "        print(\"No moves to preview.\")\n",
    "        return\n",
    "\n",
    "    log = move_log.sort_values(\"gain\", ascending=False).head(k)\n",
    "\n",
    "    for _, r in log.iterrows():\n",
    "        i   = int(r[\"i\"])\n",
    "        op  = r[\"op\"]\n",
    "        lang = r[\"lang\"]\n",
    "        other = \"sent_pt_pt\" if lang == \"sent_pt_br\" else \"sent_pt_br\"\n",
    "\n",
    "        # pull rows\n",
    "        bi   = df_before.at[i,   lang]\n",
    "        bip1 = df_before.at[i+1, lang]\n",
    "        ai   = df_after.at[i,    lang]\n",
    "        aip1 = df_after.at[i+1,  lang]\n",
    "\n",
    "        Ri   = df_before.at[i,   other]\n",
    "        Rip1 = df_before.at[i+1, other]  # other side doesn't change during move\n",
    "\n",
    "        piece = _moved_piece(bi, ai, bip1, aip1, op)\n",
    "\n",
    "        keep_sum = sim(bi, Ri) + sim(bip1, Rip1)\n",
    "        new_sum  = sim(ai, Ri) + sim(aip1, Rip1)\n",
    "        d_i   = sim(ai, Ri)   - sim(bi, Ri)\n",
    "        d_ip1 = sim(aip1, Rip1) - sim(bip1, Rip1)\n",
    "\n",
    "        line_i   = int(df_before.at[i,   \"line_no\"])\n",
    "        line_ip1 = int(df_before.at[i+1, \"line_no\"])\n",
    "\n",
    "        print(\"\\n────────────────────────────────────────\")\n",
    "        print(f\"lines {line_i} → {line_ip1} | {lang} | {op} | gain {float(r['gain']):.3f}\")\n",
    "        print(f\"moved piece: [{piece}]\")\n",
    "        print(f\"sum sim: {keep_sum:.3f} → {new_sum:.3f}  (Δi={d_i:+.3f}, Δi+1={d_ip1:+.3f})\")\n",
    "\n",
    "        print(\"\\n— BEFORE —\")\n",
    "        print(f\"i   ({lang}): {bi}\")\n",
    "        print(f\"i+1 ({lang}): {bip1}\")\n",
    "        print(f\"i   ({other}): {Ri}\")\n",
    "        print(f\"i+1 ({other}): {Rip1}\")\n",
    "\n",
    "        print(\"\\n— AFTER —\")\n",
    "        print(f\"i   ({lang}): {ai}\")\n",
    "        print(f\"i+1 ({lang}): {aip1}\")\n",
    "\n",
    "# # Usage example (with what you already computed):\n",
    "# df = load_opus_window(start_line=1750340, window=50)\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "# preview_moves(df, moved_df, move_log, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937373b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>sent_pt_br</th>\n",
       "      <th>sent_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6766357</td>\n",
       "      <td>6766358</td>\n",
       "      <td>Muito bem, OK. Ok? Ok.</td>\n",
       "      <td>Está. Bem, tudo bem. Está bem?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6766360</td>\n",
       "      <td>6766361</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Tudo bem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6766361</td>\n",
       "      <td>6766362</td>\n",
       "      <td>Não, não. Rosa, escute.</td>\n",
       "      <td>Adeusinho. Não, não. Rosa, ouça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6766363</td>\n",
       "      <td>6766364</td>\n",
       "      <td>Preciso encontrar a senhora Lieberman. OK.</td>\n",
       "      <td>Preciso de encontrar a Sra. Lieberman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6766365</td>\n",
       "      <td>6766366</td>\n",
       "      <td>Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.</td>\n",
       "      <td>Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6766369</td>\n",
       "      <td>6766370</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Pronto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6766370</td>\n",
       "      <td>6766371</td>\n",
       "      <td>Gracias.</td>\n",
       "      <td>Adeusinho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6766371</td>\n",
       "      <td>6766372</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Gracias.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6766372</td>\n",
       "      <td>6766373</td>\n",
       "      <td>Devolva a bola de gude!</td>\n",
       "      <td>Dá-me esse berlinde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6766373</td>\n",
       "      <td>6766374</td>\n",
       "      <td>É do meu pai! Me dê!</td>\n",
       "      <td>É do meu pai! Dá-mo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6766374</td>\n",
       "      <td>6766375</td>\n",
       "      <td>É a bola de gude do meu pai.</td>\n",
       "      <td>É o berlinde do meu pai.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  \\\n",
       "0   6766357  6766358   \n",
       "1   6766360  6766361   \n",
       "2   6766361  6766362   \n",
       "3   6766363  6766364   \n",
       "4   6766365  6766366   \n",
       "5   6766369  6766370   \n",
       "6   6766370  6766371   \n",
       "7   6766371  6766372   \n",
       "8   6766372  6766373   \n",
       "9   6766373  6766374   \n",
       "10  6766374  6766375   \n",
       "\n",
       "                                                                       sent_pt_br  \\\n",
       "0                                                          Muito bem, OK. Ok? Ok.   \n",
       "1                                                                          Adeus.   \n",
       "2                                                         Não, não. Rosa, escute.   \n",
       "3                                      Preciso encontrar a senhora Lieberman. OK.   \n",
       "4   Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.   \n",
       "5                                                                          Adeus.   \n",
       "6                                                                        Gracias.   \n",
       "7                                                                          Adeus.   \n",
       "8                                                         Devolva a bola de gude!   \n",
       "9                                                            É do meu pai! Me dê!   \n",
       "10                                                   É a bola de gude do meu pai.   \n",
       "\n",
       "                                                                                 sent_pt_pt  \n",
       "0                                                            Está. Bem, tudo bem. Está bem?  \n",
       "1                                                                                 Tudo bem.  \n",
       "2                                                          Adeusinho. Não, não. Rosa, ouça.  \n",
       "3                                                    Preciso de encontrar a Sra. Lieberman.  \n",
       "4   Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".  \n",
       "5                                                                                   Pronto.  \n",
       "6                                                                                Adeusinho.  \n",
       "7                                                                                  Gracias.  \n",
       "8                                                                      Dá-me esse berlinde.  \n",
       "9                                                                      É do meu pai! Dá-mo!  \n",
       "10                                                                 É o berlinde do meu pai.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preview_window_final_df(\n",
    "    start_line: int,\n",
    "    window: int = 50,\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run apply_neighbor_moves repeatedly (moves only) on a window until no more moves\n",
    "    or max_iters is reached. Return a DF with the same columns as opus_moses\n",
    "    (line_no, pair_id, sent_pt_br, sent_pt_pt) reflecting the FINAL subtitles.\n",
    "    \"\"\"\n",
    "    # load the original window\n",
    "    cur = load_opus_window(start_line=start_line, window=window)\n",
    "\n",
    "    # iterate moves to convergence\n",
    "    for _ in range(max_iters):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "\n",
    "    # return only the opus_moses columns, in order\n",
    "    return cur.loc[:, [\"line_no\", \"pair_id\", \"sent_pt_br\", \"sent_pt_pt\"]].copy()\n",
    "\n",
    "final_df = preview_window_final_df(start_line=6766355, window=20, margin=0.04, max_clause_chars=60, max_iters=5)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569388ce",
   "metadata": {},
   "source": [
    "**//PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa95ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_wrapping_quotes_once(s: str) -> str:\n",
    "    \"\"\"Remove exactly one balanced pair of wrapping quotes if present.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    t = s.strip()\n",
    "    for left, right in QUOTE_PAIRS:\n",
    "        if t.startswith(left) and t.endswith(right):\n",
    "            inner = t[len(left):-len(right)].strip()\n",
    "            # keep only if there's some non-quote content inside\n",
    "            if inner and any(c not in QUOTE_CHARS for c in inner):\n",
    "                return inner\n",
    "    return t\n",
    "\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    \"\"\"Peel multiple layers, e.g., “ 'foo' ” -> foo.\"\"\"\n",
    "    prev, cur = None, s\n",
    "    while cur != prev:\n",
    "        prev = cur\n",
    "        cur = _strip_wrapping_quotes_once(cur)\n",
    "    return cur\n",
    "\n",
    "def _strip_edge_quotes_unbalanced(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Also remove lone leading/trailing quotes if they remain (unbalanced).\n",
    "    Repeats until no edge quote remains or only quotes are left.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    t = s.strip()\n",
    "    lefts  = {L for L,_ in QUOTE_PAIRS}\n",
    "    rights = {R for _,R in QUOTE_PAIRS}\n",
    "\n",
    "    changed = True\n",
    "    while changed and t:\n",
    "        changed = False\n",
    "        if t and t[0] in lefts:\n",
    "            t = t[1:].lstrip(); changed = True\n",
    "        if t and t[-1] in rights:\n",
    "            t = t[:-1].rstrip(); changed = True\n",
    "        # stop if the remainder is only quotes/spaces\n",
    "        if t and all((c in QUOTE_CHARS) or c.isspace() for c in t):\n",
    "            break\n",
    "    return t\n",
    "\n",
    "\n",
    "def _drop_speaker_labels_keep_content(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    # normalize space variants first\n",
    "    s = (s.replace(\"\\u00A0\", \" \")\n",
    "           .replace(\"\\u202F\", \" \")\n",
    "           .replace(\"\\u2007\", \" \")\n",
    "           .replace(\"\\u2009\", \" \"))\n",
    "\n",
    "    # keep the boundary, drop the label\n",
    "    s = SPEAKER_LABEL_DROP.sub(r\"\\1\", s)\n",
    "\n",
    "    # tidy spacing\n",
    "    s = re.sub(r\"\\s+([,.;:!?…)\\]\\}}])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([(\\[\\{{«“\\\"'])\\s+\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _rstrip_quotes(s): return re.sub(r'[\\s\"\\']+$', '', s or \"\")\n",
    "def _last_char(s): \n",
    "    t = _rstrip_quotes(s or \"\").rstrip()\n",
    "    return t[-1:] if t else \"\"\n",
    "def _first_alpha_case(s):\n",
    "    for ch in (s or \"\"):\n",
    "        if ch.isalpha(): return \"upper\" if ch.isupper() else \"lower\"\n",
    "    return None\n",
    "\n",
    "def _starts_with_dash(s): return bool(re.match(r'^\\s*['+re.escape(DASHES)+r']\\s*', s or \"\"))\n",
    "def _strip_leading_dash(s): return re.sub(r'^\\s*['+re.escape(DASHES)+r']\\s*', '', s or \"\")\n",
    "def _remove_dash_after_punct(s): return re.sub(r'([,\\.!\\?])\\s*['+re.escape(DASHES)+r']\\s*', r'\\1 ', s or \"\")\n",
    "\n",
    "def _normalize_spaces(s):\n",
    "    s = (s or \"\").replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\s+([,.;:?!…])', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "def _is_all_caps_alpha(s):\n",
    "    letters = [c for c in (s or \"\") if c.isalpha()]\n",
    "    return bool(letters) and all(c.isupper() for c in letters)\n",
    "\n",
    "def _sentence_case_from_lower(s):\n",
    "    t, out, cap = (s or \"\").lower(), [], True\n",
    "    for ch in t:\n",
    "        if cap and ch.isalpha(): out.append(ch.upper()); cap=False\n",
    "        else: out.append(ch)\n",
    "        if ch in HARD_STOPS: cap=True\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _capitalize_first_alpha(s):\n",
    "    if not s: return s\n",
    "    chars=list(s)\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch.isalpha(): chars[i]=ch.upper(); break\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def _normalize_line_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = _strip_leading_dash(s)\n",
    "    s = _drop_speaker_labels_keep_content(s)\n",
    "    s = _remove_dash_after_punct(s)\n",
    "    s = _normalize_spaces(s)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_edge_quotes_unbalanced(s)\n",
    "    if _is_all_caps_alpha(s):\n",
    "        s = _sentence_case_from_lower(s)\n",
    "    return s\n",
    "\n",
    "def _join_text(a,b):\n",
    "    b2 = _strip_leading_dash(b).lstrip()\n",
    "    if not a: return b2\n",
    "    if not b2: return a\n",
    "    return (a.rstrip() + \" \" + b2).strip()\n",
    "\n",
    "def _has_inner_hard_stop(s):  # two sentences in one row\n",
    "    return bool(re.search(r'[.?!…].+\\S.*[.?!…]', s or \"\"))\n",
    "\n",
    "def _should_merge_pair(br_a, br_b, pt_a, pt_b):\n",
    "    a_end_br = _last_char(br_a); a_end_pt = _last_char(pt_a)\n",
    "    b_head_br = _first_alpha_case(br_b); b_head_pt = _first_alpha_case(pt_b)\n",
    "    hard_br = a_end_br in HARD_STOPS; hard_pt = a_end_pt in HARD_STOPS\n",
    "\n",
    "    cont_br = ((a_end_br in SOFT_TAILS) or (len(br_a) < 40)) and (b_head_br==\"lower\" or _starts_with_dash(br_b))\n",
    "    cont_pt = ((a_end_pt in SOFT_TAILS) or (len(pt_a) < 40)) and (b_head_pt==\"lower\" or _starts_with_dash(pt_b))\n",
    "\n",
    "    underseg = (_has_inner_hard_stop(pt_a) and not _has_inner_hard_stop(br_a)) or \\\n",
    "               (_has_inner_hard_stop(br_a) and not _has_inner_hard_stop(pt_a))\n",
    "\n",
    "    if hard_br and hard_pt and (b_head_br==\"upper\") and (b_head_pt==\"upper\") and not underseg:\n",
    "        return False\n",
    "    return bool(cont_br or cont_pt or underseg)\n",
    "\n",
    "\n",
    "\n",
    "# def plan_ops_over_corpus(block_size=50_000, reset=False):\n",
    "#     with duckdb.connect(str(DB)) as con:\n",
    "#         lo, hi = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "#         lo, hi = int(lo), int(hi)\n",
    "\n",
    "#         # reset this run (only when you want a fresh start)\n",
    "#         if reset:\n",
    "#             con.execute(\"DELETE FROM opus_ops_update\")\n",
    "#             con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "#             con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "#             con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "#         # resume point\n",
    "#         done = int(con.execute(\"SELECT done_through FROM opus_ops_progress\").fetchone()[0])\n",
    "#         cur  = max(lo, done + 1)\n",
    "\n",
    "#         carry = None\n",
    "#         last_br, last_pt = None, None\n",
    "\n",
    "#         while cur <= hi:\n",
    "#             win = min(block_size, hi - cur + 1)\n",
    "#             df = con.execute(\"\"\"\n",
    "#                 SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#                 FROM opus_moses\n",
    "#                 WHERE line_no BETWEEN ? AND ?\n",
    "#                 ORDER BY line_no\n",
    "#             \"\"\", [cur, cur+win-1]).df()\n",
    "\n",
    "#             rows = df.to_dict(\"records\")\n",
    "#             if carry is not None:\n",
    "#                 rows = [carry] + rows\n",
    "#                 carry = None\n",
    "\n",
    "#             updates, deletes = [], []\n",
    "#             i, n = 0, len(rows)\n",
    "#             while i < n:\n",
    "#                 base = rows[i]; i += 1\n",
    "#                 br = _normalize_line_text(base[\"sent_pt_br\"])\n",
    "#                 pt = _normalize_line_text(base[\"sent_pt_pt\"])\n",
    "#                 group_lines = [int(base[\"line_no\"])]\n",
    "\n",
    "#                 while i < n:\n",
    "#                     nxt = rows[i]\n",
    "#                     br2 = _normalize_line_text(nxt[\"sent_pt_br\"])\n",
    "#                     pt2 = _normalize_line_text(nxt[\"sent_pt_pt\"])\n",
    "#                     if _should_merge_pair(br, br2, pt, pt2):\n",
    "#                         br = _join_text(br, br2)\n",
    "#                         pt = _join_text(pt, pt2)\n",
    "#                         group_lines.append(int(nxt[\"line_no\"]))\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         break\n",
    "\n",
    "#                 if i >= n:\n",
    "#                     carry = {\"line_no\": group_lines[0], \"pair_id\": int(base[\"pair_id\"]),\n",
    "#                              \"sent_pt_br\": br, \"sent_pt_pt\": pt}\n",
    "#                     break\n",
    "\n",
    "#                 if (br.strip() == \"\") or (pt.strip() == \"\"):\n",
    "#                     for ln in group_lines:\n",
    "#                         deletes.append({\"line_no\": ln})\n",
    "#                     # don't emit an update for the head; just skip ahead\n",
    "#                     last_br, last_pt = None, None\n",
    "#                     continue\n",
    "\n",
    "#                 if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "#                     br = _capitalize_first_alpha(br)\n",
    "#                 if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "#                     pt = _capitalize_first_alpha(pt)\n",
    "\n",
    "#                 head = group_lines[0]\n",
    "#                 if br != base[\"sent_pt_br\"] or pt != base[\"sent_pt_pt\"] or len(group_lines) > 1:\n",
    "#                     updates.append({\"line_no\": head, \"sent_pt_br\": br, \"sent_pt_pt\": pt})\n",
    "#                 for ln in group_lines[1:]:\n",
    "#                     deletes.append({\"line_no\": ln})\n",
    "\n",
    "#                 last_br, last_pt = br, pt\n",
    "\n",
    "#             if updates:\n",
    "#                 con.register(\"upd\", pd.DataFrame(updates))\n",
    "#                 con.execute(\"\"\"\n",
    "#                     INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "#                     SELECT line_no, sent_pt_br, sent_pt_pt FROM upd\n",
    "#                     ON CONFLICT(line_no) DO UPDATE SET\n",
    "#                         sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "#                         sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "#                 \"\"\")\n",
    "\n",
    "#                 con.unregister(\"upd\")\n",
    "#             if deletes:\n",
    "#                 con.register(\"del\", pd.DataFrame(deletes))\n",
    "#                 con.execute(\"\"\"\n",
    "#                     INSERT INTO opus_ops_delete (line_no)\n",
    "#                     SELECT DISTINCT line_no FROM del\n",
    "#                     ON CONFLICT(line_no) DO NOTHING\n",
    "#                 \"\"\")\n",
    "\n",
    "#                 con.unregister(\"del\")\n",
    "\n",
    "#             del df, rows, updates, deletes\n",
    "#             gc.collect()\n",
    "\n",
    "#             # advance + persist resume point\n",
    "#             cur += win\n",
    "#             con.execute(\"UPDATE opus_ops_progress SET done_through = ?\", [cur - 1])\n",
    "\n",
    "#         # flush final carry (on the same connection)\n",
    "#         if carry is not None:\n",
    "#             if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "#                 carry[\"sent_pt_br\"] = _capitalize_first_alpha(carry[\"sent_pt_br\"])\n",
    "#             if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "#                 carry[\"sent_pt_pt\"] = _capitalize_first_alpha(carry[\"sent_pt_pt\"])\n",
    "#             con.register(\"tail_upd\", pd.DataFrame([{\n",
    "#                 \"line_no\": int(carry[\"line_no\"]),\n",
    "#                 \"sent_pt_br\": carry[\"sent_pt_br\"],\n",
    "#                 \"sent_pt_pt\": carry[\"sent_pt_pt\"],\n",
    "#             }]))\n",
    "#             con.execute(\"\"\"\n",
    "#                 INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "#                 SELECT line_no, sent_pt_br, sent_pt_pt FROM tail_upd\n",
    "#                 ON CONFLICT(line_no) DO UPDATE SET\n",
    "#                     sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "#                     sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "#             \"\"\")\n",
    "#             con.unregister(\"tail_upd\")\n",
    "\n",
    "\n",
    "# # def apply_ops_to_opus_moses():\n",
    "# #     with duckdb.connect(str(DB)) as con:\n",
    "# #         # sanity: no overlap between updates and deletes\n",
    "# #         overlap = con.execute(\"\"\"\n",
    "# #             SELECT COUNT(*) FROM opus_ops_update u\n",
    "# #             INNER JOIN opus_ops_delete d USING (line_no)\n",
    "# #         \"\"\").fetchone()[0]\n",
    "# #         if overlap:\n",
    "# #             raise RuntimeError(f\"{overlap} lines in BOTH update & delete; fix plan_ops first.\")\n",
    "\n",
    "# #         con.execute(\"BEGIN\")\n",
    "\n",
    "# #         # 1) delete merged-away tails FIRST (avoids transient duplicates)\n",
    "# #         con.execute(\"\"\"\n",
    "# #             DELETE FROM opus_moses\n",
    "# #             WHERE line_no IN (SELECT line_no FROM opus_ops_delete)\n",
    "# #         \"\"\")\n",
    "\n",
    "# #         # 2) then update heads with their merged/cleaned text\n",
    "# #         con.execute(\"\"\"\n",
    "# #             UPDATE opus_moses AS o\n",
    "# #             SET sent_pt_br = u.sent_pt_br,\n",
    "# #                 sent_pt_pt = u.sent_pt_pt\n",
    "# #             FROM opus_ops_update AS u\n",
    "# #             WHERE o.line_no = u.line_no\n",
    "# #         \"\"\")\n",
    "\n",
    "# #         con.execute(\"COMMIT\")\n",
    "# #         con.execute(\"CHECKPOINT\")\n",
    "\n",
    "# def apply_ops_ctas_swap(force_checkpoint=True):\n",
    "#     with duckdb.connect(str(DB)) as con:\n",
    "#         # If a previous tx is half-open on this connection, close it\n",
    "#         try:\n",
    "#             con.execute(\"ROLLBACK\")\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         # updates win over deletes\n",
    "#         con.execute(\"\"\"\n",
    "#             DELETE FROM opus_ops_delete\n",
    "#             WHERE line_no IN (SELECT line_no FROM opus_ops_update)\n",
    "#         \"\"\")\n",
    "\n",
    "#         con.execute(\"BEGIN\")\n",
    "#         try:\n",
    "#             con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "#             con.execute(\"\"\"\n",
    "#                 CREATE TABLE opus_moses_new AS\n",
    "#                 SELECT\n",
    "#                     o.line_no,\n",
    "#                     o.pair_id,\n",
    "#                     COALESCE(u.sent_pt_br, o.sent_pt_br) AS sent_pt_br,\n",
    "#                     COALESCE(u.sent_pt_pt, o.sent_pt_pt) AS sent_pt_pt\n",
    "#                 FROM opus_moses o\n",
    "#                 LEFT JOIN opus_ops_update u USING (line_no)\n",
    "#                 WHERE o.line_no NOT IN (SELECT line_no FROM opus_ops_delete)\n",
    "#                 ORDER BY o.line_no\n",
    "#             \"\"\")\n",
    "\n",
    "#             con.execute(\"DROP TABLE opus_moses\")\n",
    "#             con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "#             con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "#             con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "#             con.execute(\"COMMIT\")\n",
    "#         except:\n",
    "#             con.execute(\"ROLLBACK\")\n",
    "#             raise\n",
    "\n",
    "#         if force_checkpoint:\n",
    "#             # waits for other write transactions to finish\n",
    "#             con.execute(\"FORCE CHECKPOINT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan_ops_over_corpus(block_size=50_000, reset=True)\n",
    "# apply_ops_ctas_swap()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7cdf8",
   "metadata": {},
   "source": [
    "**//AFTER PREPROCESSING, RUN THE ALIGNER THROUGH THE WHOLE CORPUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e1da5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, gc\n",
    "\n",
    "def _run_moves_df(df: pd.DataFrame, margin=0.04, max_clause_chars=60, max_iters=5) -> pd.DataFrame:\n",
    "    \"\"\"Repeat apply_neighbor_moves on a DataFrame until no more moves or max_iters.\"\"\"\n",
    "    cur = df.loc[:, [\"line_no\",\"pair_id\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "    for _ in range(int(max_iters)):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "    return cur\n",
    "\n",
    "def apply_neighbor_moves_corpus_inplace(\n",
    "    block_size: int = 50_000,\n",
    "    overlap: int = 3,                 # rows kept between blocks so moves can cross the seam\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream over opus_moses and apply your neighbor-move heuristic in-place.\n",
    "    - Processes in blocks with 'overlap' rows carried forward.\n",
    "    - Only updates rows that actually changed.\n",
    "    - No row-count changes (this pass only moves clauses).\n",
    "    \"\"\"\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        lo, hi = con.execute(\"SELECT min(line_no), max(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        cur_start = lo\n",
    "        carry_df = None  # last 'overlap' rows of the previous processed block (already moved)\n",
    "\n",
    "        while cur_start <= hi:\n",
    "            # choose fetch start/count so we include the carried rows\n",
    "            if carry_df is None:\n",
    "                fetch_start = cur_start\n",
    "                fetch_count = min(block_size, hi - fetch_start + 1)\n",
    "            else:\n",
    "                fetch_start = int(carry_df[\"line_no\"].iloc[0])\n",
    "                fetch_count = min(block_size + overlap, hi - fetch_start + 1)\n",
    "\n",
    "            # load from DB\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [fetch_start, fetch_start + fetch_count - 1]).df()\n",
    "\n",
    "            # overlay carried texts onto the front (so we start from the already-moved boundary)\n",
    "            if carry_df is not None and not carry_df.empty:\n",
    "                df = df.merge(carry_df[[\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]],\n",
    "                              on=\"line_no\", how=\"left\", suffixes=(\"\",\"_car\"))\n",
    "                for col in (\"sent_pt_br\",\"sent_pt_pt\"):\n",
    "                    rep = df[col + \"_car\"]\n",
    "                    df[col] = rep.where(rep.notna(), df[col])\n",
    "                    df.drop(columns=[col + \"_car\"], inplace=True)\n",
    "\n",
    "            # keep a copy for diffing\n",
    "            orig = df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "\n",
    "            # run your move heuristic on this combined block\n",
    "            moved = _run_moves_df(df, margin=margin, max_clause_chars=max_clause_chars, max_iters=max_iters)\n",
    "\n",
    "            # decide how many rows to flush now (keep the last 'overlap' rows for the next block)\n",
    "            is_last_block = (fetch_start + len(df) - 1) >= hi\n",
    "            flush_n = len(moved) if is_last_block else max(0, len(moved) - overlap)\n",
    "\n",
    "            if flush_n:\n",
    "                out = moved.iloc[:flush_n]\n",
    "                base = orig.iloc[:flush_n]\n",
    "\n",
    "                # diffs → only update changed rows\n",
    "                changed = (out[\"sent_pt_br\"] != base[\"sent_pt_br\"]) | (out[\"sent_pt_pt\"] != base[\"sent_pt_pt\"])\n",
    "                upd = out.loc[changed, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]]\n",
    "\n",
    "                if not upd.empty:\n",
    "                    con.register(\"upd\", upd)\n",
    "                    con.execute(\"\"\"\n",
    "                        UPDATE opus_moses AS o\n",
    "                        SET sent_pt_br = u.sent_pt_br,\n",
    "                            sent_pt_pt = u.sent_pt_pt\n",
    "                        FROM upd AS u\n",
    "                        WHERE o.line_no = u.line_no\n",
    "                    \"\"\")\n",
    "                    con.unregister(\"upd\")\n",
    "\n",
    "                # next fetch should begin right after the last flushed line\n",
    "                cur_start = int(out[\"line_no\"].iloc[-1]) + 1\n",
    "            else:\n",
    "                # nothing flushed (tiny last block)\n",
    "                cur_start = fetch_start + len(df)\n",
    "\n",
    "            # carry the tail (overlap) forward (already moved)\n",
    "            carry_df = moved.iloc[flush_n:].copy()\n",
    "\n",
    "            # tidy memory\n",
    "            del df, orig, moved\n",
    "            gc.collect()\n",
    "\n",
    "        # flush any leftover carried rows (end of file)\n",
    "        if carry_df is not None and not carry_df.empty:\n",
    "            con.register(\"upd_tail\", carry_df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]])\n",
    "            con.execute(\"\"\"\n",
    "                UPDATE opus_moses AS o\n",
    "                SET sent_pt_br = u.sent_pt_br,\n",
    "                    sent_pt_pt = u.sent_pt_pt\n",
    "                FROM upd_tail AS u\n",
    "                WHERE o.line_no = u.line_no\n",
    "            \"\"\")\n",
    "            con.unregister(\"upd_tail\")\n",
    "\n",
    "        # reclaim disk space\n",
    "        con.execute(\"CHECKPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a4cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_neighbor_moves_corpus_inplace(\n",
    "#     block_size=50_000,   # tune for your RAM\n",
    "#     overlap=3,           # 2–3 is plenty for neighbor moves\n",
    "#     margin=0.04,\n",
    "#     max_clause_chars=60,\n",
    "#     max_iters=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc9c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# B) REPEATED-PREFIX CLEANER\n",
    "# ==============================\n",
    "def _split_sents(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def _norm_for_match(s: str) -> str:\n",
    "    s = _strip_accents(s.lower())\n",
    "    s = re.sub(r\"[^\\w]+\", \" \", s, flags=re.UNICODE)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _tokens(s: str) -> List[str]:\n",
    "    return _norm_for_match(s).split()\n",
    "\n",
    "def _jaccard(a: set, b: set) -> float:\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def _looks_like_sentence_start(s: str) -> bool:\n",
    "    t = (s or \"\").lstrip()\n",
    "    while t and t[0] in \"«“\\\"([{'’”»\": t = t[1:].lstrip()\n",
    "    return (not t) or t[0].isupper()\n",
    "\n",
    "def _adjacent_dedup(sents: List[str], jacc=0.96) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for s in sents:\n",
    "        if out:\n",
    "            a = set(_tokens(out[-1])); b = set(_tokens(s))\n",
    "            if _jaccard(a, b) >= jacc:\n",
    "                continue\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def dedup_repeated_prefix_block(\n",
    "    br_prev: str, pt_prev: str,\n",
    "    br_here: str, pt_here: str,\n",
    "    *,\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True\n",
    ") -> Tuple[str, str, bool, int]:\n",
    "    \"\"\"\n",
    "    Trim from the START of (br_here, pt_here) the longest sentence-aligned prefix\n",
    "    whose tokens are largely contained in the TAIL of (br_prev, pt_prev).\n",
    "    Returns: (br_trimmed, pt_trimmed, applied, n_sentences_removed)\n",
    "    \"\"\"\n",
    "    def _count_to_remove(prev: str, nxt: str) -> int:\n",
    "        prev_s = _split_sents(prev); nxt_s = _split_sents(nxt)\n",
    "        if not prev_s or not nxt_s: return 0\n",
    "        tail = \" \".join(prev_s[-prev_window:]) if prev_window > 0 else \" \".join(prev_s)\n",
    "        tail_tok = set(_tokens(tail))\n",
    "        best_k = 0\n",
    "        for k in range(1, len(nxt_s) + 1):\n",
    "            pref = \" \".join(nxt_s[:k])\n",
    "            toks = _tokens(pref)\n",
    "            if len(toks) < min_prefix_tokens: continue\n",
    "            cov = len(set(toks) & tail_tok) / max(1, len(set(toks)))\n",
    "            if cov >= coverage_thresh: best_k = k\n",
    "        return best_k\n",
    "\n",
    "    k_br = _count_to_remove(br_prev, br_here)\n",
    "    k_pt = _count_to_remove(pt_prev, pt_here)\n",
    "    k = min(k_br, k_pt) if require_both else max(k_br, k_pt)\n",
    "    if k <= 0: return br_here, pt_here, False, 0\n",
    "\n",
    "    br_s = _split_sents(br_here)[k:]; pt_s = _split_sents(pt_here)[k:]\n",
    "    if collapse_adjacent_dups:\n",
    "        br_s = _adjacent_dedup(br_s); pt_s = _adjacent_dedup(pt_s)\n",
    "\n",
    "    br_out = \" \".join(br_s).strip(); pt_out = \" \".join(pt_s).strip()\n",
    "    if br_out and not _looks_like_sentence_start(br_out): return br_here, pt_here, False, 0\n",
    "    if pt_out and not _looks_like_sentence_start(pt_out): return br_here, pt_here, False, 0\n",
    "    return br_out, pt_out, True, k\n",
    "\n",
    "def run_repeated_prefix_cleaner_chunked(\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    table: str = \"opus_moses\",\n",
    "    order_col: str = \"line_no\",\n",
    "    text_br_col: str = \"sent_pt_br\",\n",
    "    text_pt_col: str = \"sent_pt_pt\",\n",
    "    id_pair_col: str = \"pair_id\",\n",
    "    # knobs (forwarded)\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True,\n",
    "    # deletion policy\n",
    "    delete_on_trigger: bool = True,\n",
    "    delete_if_empty_only: bool = False,\n",
    "    # execution\n",
    "    chunk_size: int = 50_000,\n",
    "    start_line: Optional[int] = None,\n",
    "    end_line: Optional[int] = None,\n",
    "    apply_changes: bool = False,\n",
    "    print_updates: bool = False,\n",
    "    trace_lines: Optional[Iterable[int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Walk rows; if dedup applies (either language unless require_both=True):\n",
    "      - delete whole row (default), or\n",
    "      - update with trimmed text.\n",
    "    Prints deleted (line_no, pair_id). Processes in chunks.\n",
    "    \"\"\"\n",
    "    assert not (delete_on_trigger and delete_if_empty_only), \\\n",
    "        \"Choose delete_on_trigger=True OR delete_if_empty_only=True (not both).\"\n",
    "\n",
    "    where = []; args = []\n",
    "    if start_line is not None: where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "    if end_line   is not None: where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "    WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        mn, mx = con.execute(\n",
    "            f\"SELECT min({order_col}), max({order_col}) FROM {table} {WHERE}\", args\n",
    "        ).fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"No rows match selection.\"); return\n",
    "\n",
    "        prev_br, prev_pt = \"\", \"\"\n",
    "        cur = int(mn)\n",
    "        while cur <= int(mx):\n",
    "            hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "            df = con.execute(f\"\"\"\n",
    "                SELECT {order_col} AS line_no,\n",
    "                       {id_pair_col} AS pair_id,\n",
    "                       {text_br_col} AS br,\n",
    "                       {text_pt_col} AS pt\n",
    "                FROM {table}\n",
    "                WHERE {order_col} BETWEEN ? AND ?\n",
    "                ORDER BY {order_col}\n",
    "            \"\"\", [cur, hi]).df()\n",
    "\n",
    "            updates = []; deletes = []\n",
    "            deleted_ids_print = []; updated_ids_print = []\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                line_no = int(df.line_no.iloc[i])\n",
    "                pair_id = int(df.pair_id.iloc[i]) if \"pair_id\" in df.columns else None\n",
    "                br_here = df.br.iloc[i] or \"\"; pt_here = df.pt.iloc[i] or \"\"\n",
    "\n",
    "                br_new, pt_new, applied, k = dedup_repeated_prefix_block(\n",
    "                    prev_br, prev_pt, br_here, pt_here,\n",
    "                    prev_window=prev_window,\n",
    "                    min_prefix_tokens=min_prefix_tokens,\n",
    "                    coverage_thresh=coverage_thresh,\n",
    "                    require_both=require_both,\n",
    "                    collapse_adjacent_dups=collapse_adjacent_dups\n",
    "                )\n",
    "\n",
    "                if trace_lines and (line_no in set(trace_lines)):\n",
    "                    print(f\"[trace {line_no}] applied={applied} k={k}\")\n",
    "\n",
    "                will_delete = False\n",
    "                if applied:\n",
    "                    if delete_on_trigger:\n",
    "                        will_delete = True\n",
    "                    elif delete_if_empty_only and (not br_new.strip() and not pt_new.strip()):\n",
    "                        will_delete = True\n",
    "\n",
    "                if will_delete:\n",
    "                    deletes.append((line_no,))\n",
    "                    deleted_ids_print.append((line_no, pair_id))\n",
    "                    # don't advance prev_* on deletion (use last kept row)\n",
    "                else:\n",
    "                    if applied and (br_new != br_here or pt_new != pt_here):\n",
    "                        updates.append((br_new, pt_new, line_no))\n",
    "                        if print_updates: updated_ids_print.append((line_no, pair_id))\n",
    "                        prev_br, prev_pt = br_new, pt_new\n",
    "                    else:\n",
    "                        prev_br, prev_pt = br_here, pt_here\n",
    "\n",
    "            if apply_changes and (updates or deletes):\n",
    "                con.execute(\"BEGIN TRANSACTION\")\n",
    "                if updates:\n",
    "                    con.executemany(\n",
    "                        f\"UPDATE {table} SET {text_br_col} = ?, {text_pt_col} = ? WHERE {order_col} = ?\",\n",
    "                        updates\n",
    "                    )\n",
    "                if deletes:\n",
    "                    con.executemany(\n",
    "                        f\"DELETE FROM {table} WHERE {order_col} = ?\",\n",
    "                        deletes\n",
    "                    )\n",
    "                con.execute(\"COMMIT\")\n",
    "\n",
    "            print(f\"[{cur}..{hi}] updates={len(updates)} deletes={len(deletes)}\")\n",
    "            if deleted_ids_print:\n",
    "                print(\"  Deleted rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(deleted_ids_print), 1000):\n",
    "                    block = deleted_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "            if print_updates and updated_ids_print:\n",
    "                print(\"  Updated rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(updated_ids_print), 1000):\n",
    "                    block = updated_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "\n",
    "            cur = hi + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77219d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) repeated-prefix dedup only\n",
    "# run_repeated_prefix_cleaner_chunked(\n",
    "#     db_path=DB,\n",
    "#     prev_window=6,\n",
    "#     min_prefix_tokens=6,\n",
    "#     coverage_thresh=0.92,\n",
    "#     require_both=False,             # either side may trigger\n",
    "#     collapse_adjacent_dups=True,\n",
    "#     delete_on_trigger=True,         # delete whenever dedup applies\n",
    "#     delete_if_empty_only=False,\n",
    "#     chunk_size=50_000,\n",
    "#     start_line=None, end_line=None,\n",
    "#     apply_changes=True,            # DRY RUN\n",
    "#     print_updates=False,\n",
    "#     trace_lines= {18}               # e.g., {18} to debug that row\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe66a3",
   "metadata": {},
   "source": [
    "**//SIMALIGN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4670bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 15:26:24,901 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# PURE SimAlign LINKS — Filter & Preview (self-contained)\n",
    "# ==============================\n",
    "# What this provides:\n",
    "#  - SimAlign setup (XLM-R, word-level)\n",
    "#  - Feature extractor using ONLY raw SimAlign word links (optionally with prev/here/next window)\n",
    "#  - Flag policy (threshold-based; tuneable)\n",
    "#  - Preview helpers (numbers-only + [[interior]] / <edge> highlights)\n",
    "#  - No trimming/mutation logic included\n",
    "# ==============================\n",
    "\n",
    "# ---------- tokenization / basics ----------\n",
    "def tokenize(s: str) -> list[str]:\n",
    "    return WORD.findall(s or \"\")\n",
    "\n",
    "def ali_tokenize(s: str) -> list[str]:\n",
    "    \"\"\"Tokens fed to SimAlign (word-level). Keep consistent with ali_char_spans().\"\"\"\n",
    "    return tokenize(s)\n",
    "\n",
    "def ali_char_spans(text: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Char spans aligned with ali_tokenize().\"\"\"\n",
    "    return [m.span() for m in WORD.finditer(text or \"\")]\n",
    "\n",
    "def token_overlap(a: str, b: str) -> float:\n",
    "    A = set(w.lower() for w in WORD.findall(a or \"\"))\n",
    "    B = set(w.lower() for w in WORD.findall(b or \"\"))\n",
    "    return (len(A & B) / len(A | B)) if (A and B) else 0.0\n",
    "\n",
    "\n",
    "def sim2(a: str, b: str, *, method: str = ALIGN_METHOD,\n",
    "        smooth_small_gaps: int = 1, content_only: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Similarity = average coverage of aligned tokens on both sides.\n",
    "    - method: 'itermax' | 'mwmf' | 'inter' (you set ALIGN_METHOD outside)\n",
    "    - smooth_small_gaps: fill 1-token pinholes if >0\n",
    "    - content_only: measure coverage over content tokens only (ignores stopwords)\n",
    "    \"\"\"\n",
    "    a = (a or \"\").strip()\n",
    "    b = (b or \"\").strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    L = ali_tokenize(a)\n",
    "    R = ali_tokenize(b)\n",
    "    if not L and not R:\n",
    "        return 1.0\n",
    "    if not L or not R:\n",
    "        return 0.0\n",
    "\n",
    "    # raw SimAlign links\n",
    "    out = aligner.get_word_aligns(L, R)\n",
    "    pairs = out.get(method, out.get(\"inter\", []))  # fall back if needed\n",
    "\n",
    "    covered_L = {i for i, _ in pairs}\n",
    "    covered_R = {j for _, j in pairs}\n",
    "\n",
    "    covL = len(covered_L) / max(1, len(L))\n",
    "    covR = len(covered_R) / max(1, len(R))\n",
    "\n",
    "    return 0.5 * (covL + covR)\n",
    "\n",
    "import unicodedata as ud\n",
    "\n",
    "def _form_norm(t: str) -> str:\n",
    "    # Unicode-aware: normalize + casefold\n",
    "    return ud.normalize(\"NFC\", t or \"\").casefold()\n",
    "\n",
    "\n",
    "def new_sim(br: str, pt: str, method: str = ALIGN_METHOD) -> float:\n",
    "    L = ali_tokenize(br or \"\"); R = ali_tokenize(pt or \"\")\n",
    "    if not L and not R: return 1.0\n",
    "    if not L or not R:  return 0.0\n",
    "\n",
    "    Ln = [_form_norm(t) for t in L]\n",
    "    Rn = [_form_norm(t) for t in R]\n",
    "    union = set(Ln) | set(Rn)\n",
    "    if not union: return 1.0\n",
    "\n",
    "    pairs = _raw_pairs(L, R, method=method)\n",
    "\n",
    "    aligned_types = set()\n",
    "    for i, j in pairs:\n",
    "        if 0 <= i < len(Ln): aligned_types.add(Ln[i])   # <-- use normalized\n",
    "        if 0 <= j < len(Rn): aligned_types.add(Rn[j])   # <-- use normalized\n",
    "\n",
    "    return len(aligned_types) / len(union)\n",
    "\n",
    "def _type_sim_from_pairs(L_tokens, R_tokens, pairs):\n",
    "    Ln = [_form_norm(t) for t in L_tokens]\n",
    "    Rn = [_form_norm(t) for t in R_tokens]\n",
    "    union = set(Ln) | set(Rn)\n",
    "    if not union: return 1.0\n",
    "    aligned = set()\n",
    "    for i, j in pairs:\n",
    "        if 0 <= i < len(Ln): aligned.add(Ln[i])\n",
    "        if 0 <= j < len(Rn): aligned.add(Rn[j])\n",
    "    return len(aligned) / len(union)\n",
    "\n",
    "\n",
    "def is_content_token(tok: str) -> bool:\n",
    "    t = (tok or \"\").lower()\n",
    "    return (t not in PT_STOPWORDS) and (len(t) > 1)\n",
    "\n",
    "\n",
    "def _split_sents(s: str) -> list[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "# ---------- SimAlign setup ----------\n",
    "aligner = SentenceAligner(model=\"xlmr\", token_type=\"word\", matching_methods=\"a\")\n",
    "\n",
    "# ---------- raw pairs + utilities ----------\n",
    "def _raw_pairs(l_tokens, r_tokens, method=\"inter\"):\n",
    "    # empty-side guard\n",
    "    if not l_tokens or not r_tokens:\n",
    "        return []\n",
    "    out = aligner.get_word_aligns(l_tokens, r_tokens)  # {\"inter\",\"itermax\",\"mwmf\"}\n",
    "    if method == \"itermax\":\n",
    "        return out.get(\"itermax\", [])\n",
    "    elif method == \"union\":\n",
    "        return list({*out.get(\"inter\", []), *out.get(\"itermax\", [])})\n",
    "    else:  # \"inter\" (default) or \"mwmf\"\n",
    "        return out.get(method, out.get(\"inter\", []))\n",
    "\n",
    "def spans_from_uncovered(tokens: list[str], covered_idx: set[int]) -> list[tuple[int,int]]:\n",
    "    spans, cur = [], []\n",
    "    for i in range(len(tokens)):\n",
    "        if i not in covered_idx:\n",
    "            cur.append(i)\n",
    "        elif cur:\n",
    "            spans.append((cur[0], cur[-1])); cur = []\n",
    "    if cur:\n",
    "        spans.append((cur[0], cur[-1]))\n",
    "    return spans\n",
    "\n",
    "def _smooth_small_gaps(covered: set[int], n_tokens: int, max_gap: int = 1) -> set[int]:\n",
    "    \"\"\"Fill tiny uncovered holes (≤ max_gap) surrounded by covered tokens (function-word pinholes).\"\"\"\n",
    "    C = set(covered)\n",
    "    i = 0\n",
    "    while i < n_tokens:\n",
    "        if i not in C:\n",
    "            j = i\n",
    "            while j < n_tokens and j not in C:\n",
    "                j += 1\n",
    "            gap = j - i\n",
    "            if 0 < gap <= max_gap and i > 0 and j < n_tokens:\n",
    "                for k in range(i, j):\n",
    "                    C.add(k)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return C\n",
    "\n",
    "def _interior_uncovered(tokens: list[str], covered: set[int]) -> list[tuple[int,int]]:\n",
    "    \"\"\"Uncovered runs strictly inside (not touching edges).\"\"\"\n",
    "    runs = spans_from_uncovered(tokens, covered)\n",
    "    n = len(tokens)\n",
    "    return [(i0, i1) for (i0, i1) in runs if i0 > 0 and i1 < n - 1]\n",
    "\n",
    "def _content_count(tokens: list[str]) -> int:\n",
    "    return sum(1 for t in tokens if is_content_token(t))\n",
    "\n",
    "# ---------- highlighting ----------\n",
    "def _split_edge_vs_interior(runs: list[tuple[int,int]], n_tokens: int):\n",
    "    interior, edges = [], []\n",
    "    for i0, i1 in runs:\n",
    "        if i0 > 0 and i1 < n_tokens - 1:\n",
    "            interior.append((i0, i1))\n",
    "        else:\n",
    "            edges.append((i0, i1))\n",
    "    return interior, edges\n",
    "\n",
    "def _to_char_spans(token_runs: list[tuple[int,int]], token_char: list[tuple[int,int]]):\n",
    "    char_runs = []\n",
    "    for i0, i1 in token_runs:\n",
    "        if not token_char:\n",
    "            continue\n",
    "        i0 = max(0, min(i0, len(token_char) - 1))\n",
    "        i1 = max(0, min(i1, len(token_char) - 1))\n",
    "        L = token_char[i0][0]; R = token_char[i1][1]\n",
    "        char_runs.append((L, R))\n",
    "    # merge\n",
    "    char_runs.sort()\n",
    "    merged = []\n",
    "    for L, R in char_runs:\n",
    "        if not merged or L > merged[-1][1]:\n",
    "            merged.append([L, R])\n",
    "        else:\n",
    "            merged[-1][1] = max(merged[-1][1], R)\n",
    "    return [(L, R) for L, R in merged]\n",
    "\n",
    "def _apply_highlights(text: str,\n",
    "                      interior_char: list[tuple[int,int]],\n",
    "                      edge_char: list[tuple[int,int]],\n",
    "                      marks=(\"[[\", \"]]\"), edge_marks=(\"<\", \">\")) -> str:\n",
    "    \"\"\"Insert [[...]] (interior) and <...> (edge) highlights without breaking indices.\"\"\"\n",
    "    tags = []\n",
    "    for L, R in interior_char:\n",
    "        tags.append((L, \"open_i\")); tags.append((R, \"close_i\"))\n",
    "    for L, R in edge_char:\n",
    "        tags.append((L, \"open_e\")); tags.append((R, \"close_e\"))\n",
    "    tags.sort(key=lambda x: (x[0], x[1].startswith(\"close\")))  # close before open at same pos\n",
    "\n",
    "    out, last = [], 0\n",
    "    stack = []\n",
    "    for pos, kind in tags:\n",
    "        pos = max(0, min(pos, len(text)))\n",
    "        if pos > last:\n",
    "            out.append(text[last:pos])\n",
    "            last = pos\n",
    "        if kind == \"open_i\":\n",
    "            out.append(marks[0]); stack.append(\"i\")\n",
    "        elif kind == \"close_i\":\n",
    "            if stack and stack[-1] == \"i\":\n",
    "                stack.pop()\n",
    "                out.append(marks[1])\n",
    "        elif kind == \"open_e\":\n",
    "            out.append(edge_marks[0]); stack.append(\"e\")\n",
    "        elif kind == \"close_e\":\n",
    "            if stack and stack[-1] == \"e\":\n",
    "                stack.pop()\n",
    "                out.append(edge_marks[1])\n",
    "    if last < len(text):\n",
    "        out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _alignment_uncovered_highlights(\n",
    "    left_text: str, right_prev: str, right_here: str, right_next: str,\n",
    "    *, use_window: bool\n",
    ") -> tuple[str, float, float]:\n",
    "    \"\"\"\n",
    "    Highlight uncovered tokens on the left_text using raw SimAlign links.\n",
    "    Returns (highlighted_text, coverage_ratio, interior_content_ratio).\n",
    "    \"\"\"\n",
    "    left_toks = ali_tokenize(left_text)\n",
    "    if use_window:\n",
    "        right_win = ali_tokenize(\" \".join(x for x in [right_prev, right_here, right_next] if x))\n",
    "        pairs = _raw_pairs(left_toks, right_win)\n",
    "    else:\n",
    "        pairs = _raw_pairs(left_toks, ali_tokenize(right_here))\n",
    "\n",
    "    covered = {i for i, _ in pairs}\n",
    "    covered = _smooth_small_gaps(covered, len(left_toks), max_gap=1)\n",
    "\n",
    "    n = len(left_toks)\n",
    "    cov = len(covered) / max(1, n)\n",
    "\n",
    "    runs_all = spans_from_uncovered(left_toks, covered)\n",
    "    interior_runs, edge_runs = _split_edge_vs_interior(runs_all, n)\n",
    "\n",
    "    # interior content ratio\n",
    "    total_content = sum(1 for t in left_toks if is_content_token(t))\n",
    "    interior_content = sum(\n",
    "        1 for i0, i1 in interior_runs for t in left_toks[i0:i1+1] if is_content_token(t)\n",
    "    )\n",
    "    interior_content_ratio = interior_content / max(1, total_content)\n",
    "\n",
    "    token_chars = ali_char_spans(left_text)\n",
    "    interior_char = _to_char_spans(interior_runs, token_chars)\n",
    "    edge_char     = _to_char_spans(edge_runs, token_chars)\n",
    "    hi = _apply_highlights(left_text, interior_char, edge_char)\n",
    "    return hi, cov, interior_content_ratio\n",
    "\n",
    "# ---------- feature extractor (pure SimAlign) ----------\n",
    "def alignment_quality_features(\n",
    "    br_prev: str, br_here: str, br_next: str,\n",
    "    pt_prev: str, pt_here: str, pt_next: str,\n",
    "    *, use_window: bool = True, sim_fn=None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute alignment metrics using ONLY raw SimAlign word links (+ optional prev/next window).\n",
    "    \"\"\"\n",
    "    if sim_fn is None:\n",
    "        sim_fn = new_sim\n",
    "\n",
    "    br_toks = ali_tokenize(br_here)\n",
    "    pt_toks = ali_tokenize(pt_here)\n",
    "    br_len   = len(br_toks)\n",
    "    pt_len   = len(pt_toks)\n",
    "\n",
    "    if len(br_toks) == 10 or len(pt_toks) == 10:\n",
    "        pass\n",
    "\n",
    "    if use_window:\n",
    "        pt_win = ali_tokenize(\" \".join(x for x in [pt_prev, pt_here, pt_next] if x))\n",
    "        br_pairs = _raw_pairs(br_toks, pt_win)\n",
    "        br_cov = {i for i, _ in br_pairs}\n",
    "\n",
    "        br_win = ali_tokenize(\" \".join(x for x in [br_prev, br_here, br_next] if x))\n",
    "        pt_pairs = _raw_pairs(pt_toks, br_win)\n",
    "        pt_cov = {i for i, _ in pt_pairs}  # left indices of PT tokens\n",
    "    else:\n",
    "        pairs = _raw_pairs(br_toks, pt_toks)\n",
    "        br_cov = {i for i, _ in pairs}\n",
    "        pt_cov = {j for _, j in pairs}     # approximate right coverage\n",
    "\n",
    "    # smooth 1-token pinholes\n",
    "    br_cov = _smooth_small_gaps(br_cov, len(br_toks), max_gap=1)\n",
    "    pt_cov = _smooth_small_gaps(pt_cov, len(pt_toks), max_gap=1)\n",
    "\n",
    "    br_cov_ratio = len(br_cov) / max(1, len(br_toks))\n",
    "    pt_cov_ratio = len(pt_cov) / max(1, len(pt_toks))\n",
    "    cov_min = min(br_cov_ratio, pt_cov_ratio)\n",
    "    cov_gap = abs(br_cov_ratio - pt_cov_ratio)\n",
    "\n",
    "    # interior uncovered runs + content ratio\n",
    "    br_int_spans = _interior_uncovered(br_toks, br_cov)\n",
    "    pt_int_spans = _interior_uncovered(pt_toks, pt_cov)\n",
    "\n",
    "    def _content_in_runs(tokens, runs):\n",
    "        return sum(1 for i0, i1 in runs for t in tokens[i0:i1+1] if is_content_token(t))\n",
    "\n",
    "    br_content_total = _content_count(br_toks)\n",
    "    pt_content_total = _content_count(pt_toks)\n",
    "    br_int_content = _content_in_runs(br_toks, br_int_spans)\n",
    "    pt_int_content = _content_in_runs(pt_toks, pt_int_spans)\n",
    "\n",
    "    br_int_content_ratio = br_int_content / max(1, br_content_total)\n",
    "    pt_int_content_ratio = pt_int_content / max(1, pt_content_total)\n",
    "\n",
    "    br_max_int = max((j - i + 1) for i, j in br_int_spans) if br_int_spans else 0\n",
    "    pt_max_int = max((j - i + 1) for i, j in pt_int_spans) if pt_int_spans else 0\n",
    "\n",
    "    # “spillover” vs extra-info (same-language neighbors)\n",
    "    def _span_text(tokens, sp): i0, i1 = sp; return \" \".join(tokens[i0:i1+1])\n",
    "    br_int_text = \" \".join(_span_text(br_toks, sp) for sp in br_int_spans)\n",
    "    pt_int_text = \" \".join(_span_text(pt_toks, sp) for sp in pt_int_spans)\n",
    "    br_spill = max(token_overlap(br_int_text, br_prev), token_overlap(br_int_text, br_next)) if br_int_text else 0.0\n",
    "    pt_spill = max(token_overlap(pt_int_text, pt_prev), token_overlap(pt_int_text, pt_next)) if pt_int_text else 0.0\n",
    "\n",
    "    sent_diff = abs(len(_split_sents(br_here)) - len(_split_sents(pt_here)))\n",
    "    if use_window:\n",
    "        # we already computed br_pairs (BR vs PT window)\n",
    "        base_sim = _type_sim_from_pairs(br_toks, pt_toks, br_pairs)\n",
    "    else:\n",
    "        pairs = _raw_pairs(br_toks, pt_toks)\n",
    "        base_sim = _type_sim_from_pairs(br_toks, pt_toks, pairs)\n",
    "\n",
    "    return {\n",
    "        \"br_cov\": br_cov_ratio, \"pt_cov\": pt_cov_ratio,\n",
    "        \"cov_min\": cov_min, \"cov_gap\": cov_gap,\n",
    "        \"br_int_content_ratio\": br_int_content_ratio,\n",
    "        \"pt_int_content_ratio\": pt_int_content_ratio,\n",
    "        \"br_max_int\": br_max_int, \"pt_max_int\": pt_max_int,\n",
    "        \"br_spill\": br_spill, \"pt_spill\": pt_spill,\n",
    "        \"sent_diff\": sent_diff,\n",
    "        \"base_sim\": float(base_sim),\n",
    "        \"br_content_total\": br_content_total,\n",
    "        \"pt_content_total\": pt_content_total,\n",
    "        \"br_len\": br_len,\n",
    "        \"pt_len\": pt_len\n",
    "    }\n",
    "\n",
    "# ---------- flag policy (tune to taste) ----------\n",
    "def alignment_quality_flag(\n",
    "    feats: dict,\n",
    "    *,\n",
    "    min_cov_ok: float = 0.50,\n",
    "    max_cov_gap: float = 0.35,\n",
    "    max_int_ratio: float = 0.33,\n",
    "    max_max_int: int = 9,\n",
    "    max_sent_diff: int = 1,\n",
    "    min_sim_ok: float = 0.30,\n",
    "    spill_tolerance: float = 0.60,\n",
    "    min_row_content: int = 7,\n",
    "    min_interior_content_for_flag: int = 3,\n",
    "    hard_low_sim: float = 0.20,   # NEW: hard gate — flag if new_sim is this low\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Decide whether to activate the filter using SimAlign features.\n",
    "\n",
    "    Changes:\n",
    "    - Adds a hard similarity gate: if base_sim < hard_low_sim, flag immediately.\n",
    "    - Keeps the very-short logic: tiny pairs pass only when coverage looks sane; else flagged.\n",
    "    - No 'too_short' shortcut anywhere.\n",
    "    \"\"\"\n",
    "    spillish = (feats[\"br_spill\"] >= spill_tolerance) or (feats[\"pt_spill\"] >= spill_tolerance)\n",
    "\n",
    "    br_ct = feats[\"br_content_total\"]\n",
    "    pt_ct = feats[\"pt_content_total\"]\n",
    "    shortish = (br_ct < min_row_content) or (pt_ct < min_row_content)\n",
    "    very_short = (br_ct <= 3) and (pt_ct <= 3)\n",
    "\n",
    "    # ---- HARD gate: extremely low token-type similarity means it's off-topic → flag\n",
    "    if feats[\"base_sim\"] < hard_low_sim:\n",
    "        return True, \"low_similarity\"\n",
    "\n",
    "    # ---- very short handling ----\n",
    "    if very_short:\n",
    "        # let tiny function-word variants through if coverage looks sane\n",
    "        if (feats[\"cov_min\"] >= (2/3) and\n",
    "            feats[\"br_max_int\"] <= 1 and feats[\"pt_max_int\"] <= 1 and\n",
    "            feats[\"sent_diff\"] <= 0 and not spillish):\n",
    "            return False, \"ok_short_high_cov\"\n",
    "        # otherwise be stricter on coverage asymmetry & ignore interior ratio\n",
    "        min_cov_ok  = 0.40\n",
    "        max_cov_gap = max_cov_gap + 0.20\n",
    "        max_int_ratio = 1.00\n",
    "\n",
    "    # gentle nudge for other short lines (not very short)\n",
    "    if shortish and not very_short:\n",
    "        min_cov_ok  = max(min_cov_ok, 0.55)      # was 0.55\n",
    "        max_cov_gap = min(max_cov_gap, 0.20)     # was +0.10 (relaxed) → now stricter\n",
    "        max_int_ratio = max(0.0, max_int_ratio - 0.10)\n",
    "        min_sim_ok  = max(min_sim_ok, 0.60)      # was 0.50\n",
    "\n",
    "\n",
    "    # quick pass if clearly good under (possibly adjusted) thresholds\n",
    "    if feats[\"base_sim\"] >= (min_sim_ok + 0.35) and feats[\"cov_min\"] >= (min_cov_ok + 0.10):\n",
    "        return False, \"ok\"\n",
    "\n",
    "    reasons = []\n",
    "    if feats[\"cov_min\"] < min_cov_ok:\n",
    "        reasons.append(\"low_coverage\")\n",
    "    if feats[\"cov_gap\"] > max_cov_gap:\n",
    "        reasons.append(\"coverage_asymmetry\")\n",
    "    if feats[\"br_int_content_ratio\"] > max_int_ratio or feats[\"pt_int_content_ratio\"] > max_int_ratio:\n",
    "        reasons.append(\"big_interior_unaligned_content\")\n",
    "    if feats[\"br_max_int\"] >= max_max_int or feats[\"pt_max_int\"] >= max_max_int:\n",
    "        reasons.append(\"long_interior_gap\")\n",
    "    if feats[\"sent_diff\"] > max_sent_diff:\n",
    "        reasons.append(\"sentence_mismatch\")\n",
    "    if feats[\"base_sim\"] < min_sim_ok:\n",
    "        reasons.append(\"low_similarity\")\n",
    "\n",
    "    # need some actual interior content if we accuse \"content\" reasons\n",
    "    if {\"big_interior_unaligned_content\",\"long_interior_gap\"} & set(reasons):\n",
    "        enough_interior = (feats[\"br_int_content_ratio\"]*br_ct >= min_interior_content_for_flag) or \\\n",
    "                          (feats[\"pt_int_content_ratio\"]*pt_ct >= min_interior_content_for_flag)\n",
    "        if not enough_interior:\n",
    "            reasons = [r for r in reasons if r not in {\"big_interior_unaligned_content\",\"long_interior_gap\"}]\n",
    "\n",
    "    strong = {\"low_coverage\",\"coverage_asymmetry\",\"big_interior_unaligned_content\",\"long_interior_gap\"}\n",
    "    strong_hits = len([r for r in reasons if r in strong])\n",
    "\n",
    "    activate = False\n",
    "    if strong_hits >= 2:\n",
    "        activate = True\n",
    "    elif strong_hits >= 1 and not spillish:\n",
    "        activate = True\n",
    "    elif len(reasons) >= 3 and not spillish:\n",
    "        activate = True\n",
    "\n",
    "    return bool(activate), (\",\".join(reasons) if reasons else \"ok\")\n",
    "\n",
    "# ---------- previews ----------\n",
    "def preview_alignment_quality_window(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    sim_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Numbers-only preview (no highlights). No DB writes.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {}\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "        )\n",
    "        activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            **feats,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def preview_alignment_quality_window_with_highlights_complex(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    show_when: str = \"flagged\",   # \"flagged\" | \"all\"\n",
    "    sim_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preview with [[INTERIOR]] and <EDGE> highlights using pure SimAlign links.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = dict(\n",
    "            min_cov_ok=0.50,\n",
    "            max_cov_gap=0.35,\n",
    "            max_int_ratio=0.33,\n",
    "            max_max_int=9,\n",
    "            max_sent_diff=1,\n",
    "            min_sim_ok=0.30,\n",
    "            spill_tolerance=0.60\n",
    "        )\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "        )\n",
    "        activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "        if show_when == \"flagged\" and not activate:\n",
    "            continue\n",
    "\n",
    "        br_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            br_here, pt_prev, pt_here, pt_next, use_window=use_window\n",
    "        )\n",
    "        pt_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            pt_here, br_prev, br_here, br_next, use_window=use_window\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            \"base_sim\": feats[\"base_sim\"],\n",
    "            \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "            \"cov_gap\": feats[\"cov_gap\"],\n",
    "            \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "            \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "            \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "            \"br_highlight\": br_hi,   # [[INTERIOR]] and <EDGE>\n",
    "            \"pt_highlight\": pt_hi,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c58838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ali_tokenize(s: str) -> list[str]:\n",
    "#     return [m.group(0) for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "# def ali_char_spans(s: str) -> list[tuple[int,int]]:\n",
    "#     return [m.span() for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "# def _raw_pairs(left_tokens: list[str], right_tokens: list[str], method: str = ALIGN_METHOD):\n",
    "#     \"\"\"Call SimAlign safely; fall back on truncation and available methods.\"\"\"\n",
    "#     if not left_tokens or not right_tokens:\n",
    "#         return []\n",
    "#     try:\n",
    "#         out = aligner.get_word_aligns(left_tokens, right_tokens)\n",
    "#     except Exception:\n",
    "#         lt, rt = left_tokens[:300], right_tokens[:300]  # rare long-line guard\n",
    "#         try:\n",
    "#             out = aligner.get_word_aligns(lt, rt)\n",
    "#         except Exception:\n",
    "#             return []\n",
    "#     if method in out:\n",
    "#         return out[method]\n",
    "#     for m in (\"itermax\",\"mwmf\",\"inter\"):\n",
    "#         if m in out:\n",
    "#             return out[m]\n",
    "#     return []\n",
    "\n",
    "\n",
    "# # -------- reservoir sampling (keeps a bounded, uniform-ish sample) -------\n",
    "# def _reservoir_add(reservoir: list, item: dict, cap: int, seen_counter: int):\n",
    "#     if cap <= 0:\n",
    "#         return seen_counter + 1\n",
    "#     if len(reservoir) < cap:\n",
    "#         reservoir.append(item)\n",
    "#     else:\n",
    "#         j = random.randint(0, seen_counter)\n",
    "#         if j < cap:\n",
    "#             reservoir[j] = item\n",
    "#     return seen_counter + 1\n",
    "\n",
    "\n",
    "# # -------- main audit: fixed thresholds, no adaptive policy ----------------\n",
    "# def audit_alignment_filter_chunked_fixed(\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     table: str = \"opus_moses\",\n",
    "#     order_col: str = \"line_no\",\n",
    "#     id_col: str = \"pair_id\",\n",
    "#     br_col: str = \"sent_pt_br\",\n",
    "#     pt_col: str = \"sent_pt_pt\",\n",
    "#     start_line: Optional[int] = None,\n",
    "#     end_line:   Optional[int] = None,\n",
    "#     chunk_size: int = 50_000,\n",
    "#     # filter policy (FIXED per run)\n",
    "#     thresholds: Dict[str, Any] = dict(\n",
    "#         min_cov_ok=0.50,\n",
    "#         max_cov_gap=0.35,\n",
    "#         max_int_ratio=0.33,\n",
    "#         max_max_int=9,\n",
    "#         max_sent_diff=1,\n",
    "#         min_sim_ok=0.30,\n",
    "#         spill_tolerance=0.60\n",
    "#     ),\n",
    "#     use_window: bool = True,\n",
    "#     sim_fn=None,\n",
    "#     # how many examples to keep in memory\n",
    "#     max_store_flagged: int = 1000,\n",
    "#     max_store_passed: int  = 1000,\n",
    "#     seed: int = 13,\n",
    "# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Iterate in chunks, compute alignment_quality_features + alignment_quality_flag\n",
    "#     with FIXED thresholds. Print per-chunk stats.\n",
    "#     Returns (flagged_df, passed_df, summary_df). No highlights, no DB writes.\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         # build range\n",
    "#         where, args = [], []\n",
    "#         if start_line is not None:\n",
    "#             where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "#         if end_line is not None:\n",
    "#             where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "#         WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "#         mn, mx = con.execute(\n",
    "#             f\"SELECT MIN({order_col}), MAX({order_col}) FROM {table} {WHERE}\", args\n",
    "#         ).fetchone()\n",
    "#         if mn is None or mx is None:\n",
    "#             print(\"No rows to audit.\")\n",
    "#             return (pd.DataFrame(), pd.DataFrame(), pd.DataFrame())\n",
    "\n",
    "#         total_rows = 0\n",
    "#         total_flagged = 0\n",
    "#         summaries = []\n",
    "\n",
    "#         # sample reservoirs\n",
    "#         flagged_res, passed_res = [], []\n",
    "#         seen_flagged = seen_passed = 0\n",
    "\n",
    "#         cur = int(mn)\n",
    "#         while cur <= int(mx):\n",
    "#             hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "#             df = con.execute(f\"\"\"\n",
    "#                 SELECT {order_col} AS line_no, {id_col} AS pair_id,\n",
    "#                        {br_col} AS br, {pt_col} AS pt\n",
    "#                 FROM {table}\n",
    "#                 WHERE {order_col} BETWEEN ? AND ?\n",
    "#                 ORDER BY {order_col}\n",
    "#             \"\"\", [cur, hi]).df()\n",
    "\n",
    "#             if df.empty:\n",
    "#                 cur = hi + 1\n",
    "#                 continue\n",
    "\n",
    "#             chunk_rows = len(df)\n",
    "#             chunk_flagged = 0\n",
    "\n",
    "#             for i in range(chunk_rows):\n",
    "#                 br_prev = df.br.iloc[i-1] if i > 0 else \"\"\n",
    "#                 br_here = df.br.iloc[i]\n",
    "#                 br_next = df.br.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "#                 pt_prev = df.pt.iloc[i-1] if i > 0 else \"\"\n",
    "#                 pt_here = df.pt.iloc[i]\n",
    "#                 pt_next = df.pt.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "#                 feats = alignment_quality_features(\n",
    "#                     br_prev, br_here, br_next,\n",
    "#                     pt_prev, pt_here, pt_next,\n",
    "#                     use_window=use_window, sim_fn=sim_fn or sim\n",
    "#                 )\n",
    "\n",
    "#                 # alignment_quality_flag may return (activate, reason) OR (activate, reason, flags)\n",
    "#                 res = alignment_quality_flag(feats, **thresholds)\n",
    "#                 if isinstance(res, tuple) and len(res) == 3:\n",
    "#                     activate, reason_str, flags = res\n",
    "#                 else:\n",
    "#                     activate, reason_str = res\n",
    "#                     flags = {}\n",
    "\n",
    "#                 row_info = {\n",
    "#                     \"line_no\": int(df.line_no.iloc[i]),\n",
    "#                     \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#                     \"activate_filter\": bool(activate),\n",
    "#                     \"reason\": reason_str,\n",
    "#                     \"base_sim\": feats[\"base_sim\"],\n",
    "#                     \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "#                     \"cov_gap\": feats[\"cov_gap\"],\n",
    "#                     \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "#                     \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "#                     \"br_max_int\": feats[\"br_max_int\"], \"pt_max_int\": feats[\"pt_max_int\"],\n",
    "#                     \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "#                     # include sentence mismatch if available\n",
    "#                     \"sent_diff\": feats.get(\"sent_diff\", None),\n",
    "#                 }\n",
    "#                 # keep reason flags if your policy returns them\n",
    "#                 row_info.update({k: v for k, v in flags.items()})\n",
    "\n",
    "#                 if activate:\n",
    "#                     chunk_flagged += 1\n",
    "#                     seen_flagged = _reservoir_add(flagged_res, row_info, max_store_flagged, seen_flagged)\n",
    "#                 else:\n",
    "#                     seen_passed = _reservoir_add(passed_res, row_info, max_store_passed, seen_passed)\n",
    "\n",
    "#             total_rows    += chunk_rows\n",
    "#             total_flagged += chunk_flagged\n",
    "#             frac = (chunk_flagged / chunk_rows) if chunk_rows else 0.0\n",
    "#             print(f\"[{cur}..{hi}] rows={chunk_rows} flagged={chunk_flagged} ({frac:.1%})\")\n",
    "\n",
    "#             summaries.append({\n",
    "#                 \"chunk_start\": cur, \"chunk_end\": hi,\n",
    "#                 \"rows\": chunk_rows, \"flagged\": chunk_flagged, \"ratio\": frac\n",
    "#             })\n",
    "\n",
    "#             cur = hi + 1\n",
    "\n",
    "#         overall = (total_flagged / total_rows) if total_rows else 0.0\n",
    "#         print(f\"\\nTOTAL rows={total_rows} flagged={total_flagged} ({overall:.1%})\")\n",
    "\n",
    "#         flagged_df = pd.DataFrame(flagged_res)\n",
    "#         passed_df  = pd.DataFrame(passed_res)\n",
    "#         summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "#         return flagged_df, passed_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27286c3e",
   "metadata": {},
   "source": [
    "**//SIMPLE FILTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24584d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_similarity_only_flag(\n",
    "    feats: dict,\n",
    "    *,\n",
    "    # pass a constant to disable adaptation entirely\n",
    "    min_sim_ok: float | None = None,\n",
    "\n",
    "    # length adapter config\n",
    "    length_mode: str = \"total\",    # \"total\" | \"content\" | \"char\"\n",
    "    base_min_sim: float = 0.30,    # short sentences\n",
    "    long_min_sim: float = 0.70,    # long sentences (raise as you like)\n",
    "    short_len: int = 8,            # <= this → base_min_sim\n",
    "    long_len: int = 28,            # >= this → long_min_sim\n",
    "    **kwargs,                       # ignore extras\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Similarity-only flag with a length adapter.\n",
    "    Threshold grows from base_min_sim to long_min_sim as length increases.\n",
    "    length_mode:\n",
    "      - \"total\": use total token count (recommended)\n",
    "      - \"content\": use content-token count (previous behavior)\n",
    "      - \"char\": use character count (scaled by 5 chars ≈ 1 token)\n",
    "    \"\"\"\n",
    "    sim = float(feats[\"base_sim\"])\n",
    "\n",
    "    if min_sim_ok is not None:\n",
    "        thr = float(min_sim_ok)\n",
    "    else:\n",
    "        if length_mode == \"total\":\n",
    "            L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "        elif length_mode == \"char\":\n",
    "            # rough token equivalent from chars to reuse short_len/long_len in \"token units\"\n",
    "            L = int(max(int(feats.get(\"br_chars\", 0)), int(feats.get(\"pt_chars\", 0))) / 5)\n",
    "        else:  # \"content\"\n",
    "            L = max(int(feats.get(\"br_content_total\", 0)), int(feats.get(\"pt_content_total\", 0)))\n",
    "\n",
    "        if L <= short_len:\n",
    "            thr = base_min_sim\n",
    "        elif L >= long_len:\n",
    "            thr = long_min_sim\n",
    "        else:\n",
    "            t = (L - short_len) / max(1, (long_len - short_len))\n",
    "            thr = base_min_sim + t * (long_min_sim - base_min_sim)\n",
    "\n",
    "    activate = sim < thr\n",
    "    return activate, (f\"low_similarity@simple(thr={thr:.2f},L={L})\" if activate else \"ok@simple\")\n",
    "\n",
    "\n",
    "\n",
    "def preview_alignment_quality_window(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    sim_fn=None,\n",
    "    flag_fn=None,                     # NEW: choose which flag function to use\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Numbers-only preview (no highlights). No DB writes.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {}\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "        )\n",
    "\n",
    "        # Use the provided flag function; default to the multi-feature flag\n",
    "        use_flag = flag_fn or alignment_quality_flag\n",
    "        activate, reason = use_flag(feats, **(thresholds or {}))\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            **feats,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def preview_alignment_quality_window_with_highlights_simple(\n",
    "    start_line: int,\n",
    "    window: int = 40,\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "    show_when: str = \"flagged\",   # \"flagged\" | \"all\"\n",
    "    sim_fn=None,\n",
    "    flag_fn=None,                  # NEW: choose which flag function to use\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preview with [[INTERIOR]] and <EDGE> highlights using pure SimAlign links.\"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = dict(\n",
    "            min_cov_ok=0.50,\n",
    "            max_cov_gap=0.35,\n",
    "            max_int_ratio=0.33,\n",
    "            max_max_int=9,\n",
    "            max_sent_diff=1,\n",
    "            min_sim_ok=0.30,\n",
    "            spill_tolerance=0.60\n",
    "        )\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "        br_here = df.sent_pt_br.iloc[i]\n",
    "        br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "        pt_here = df.sent_pt_pt.iloc[i]\n",
    "        pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "        feats = alignment_quality_features(\n",
    "            br_prev, br_here, br_next,\n",
    "            pt_prev, pt_here, pt_next,\n",
    "            use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "        )\n",
    "\n",
    "        # Use the provided flag function; default to the multi-feature flag\n",
    "        use_flag = flag_fn or alignment_similarity_only_flag\n",
    "        activate, reason = use_flag(feats, **(thresholds or {}))\n",
    "        if show_when == \"flagged\" and not activate:\n",
    "            continue\n",
    "\n",
    "        br_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            br_here, pt_prev, pt_here, pt_next, use_window=use_window\n",
    "        )\n",
    "        pt_hi, _, _ = _alignment_uncovered_highlights(\n",
    "            pt_here, br_prev, br_here, br_next, use_window=use_window\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"line_no\": int(df.line_no.iloc[i]),\n",
    "            \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "            \"activate_filter\": bool(activate),\n",
    "            \"reason\": reason,\n",
    "            \"base_sim\": feats[\"base_sim\"],\n",
    "            \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "            \"cov_gap\": feats[\"cov_gap\"],\n",
    "            \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "            \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "            \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "            \"br_highlight\": br_hi,   # [[INTERIOR]] and <EDGE>\n",
    "            \"pt_highlight\": pt_hi,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54b5998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>activate_filter</th>\n",
       "      <th>reason</th>\n",
       "      <th>base_sim</th>\n",
       "      <th>br_cov</th>\n",
       "      <th>pt_cov</th>\n",
       "      <th>cov_gap</th>\n",
       "      <th>br_int_content_ratio</th>\n",
       "      <th>pt_int_content_ratio</th>\n",
       "      <th>spillish</th>\n",
       "      <th>br_highlight</th>\n",
       "      <th>pt_highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8878855</td>\n",
       "      <td>8878856</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mesmo um cervo pode ser uma testemunha.</td>\n",
       "      <td>Até um veado pode ser uma testemunha.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8878856</td>\n",
       "      <td>8878857</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Talvez ele viu algo e pagou o preço.</td>\n",
       "      <td>Talvez tenha visto alguma coisa e pagou o preço.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8878857</td>\n",
       "      <td>8878858</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[&lt;Conversas de rádio&gt; da polícia, indistinto] Orville.</td>\n",
       "      <td>Orville.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8878858</td>\n",
       "      <td>8878859</td>\n",
       "      <td>True</td>\n",
       "      <td>low_similarity@simple(thr=0.51,L=6)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>[Conversas [[de rádio, indistinto]]] 10-4.</td>\n",
       "      <td>10-4.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8878859</td>\n",
       "      <td>8878860</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Seu pai [[era manchou]] a [[verificação em]] um motel com um psicopata vestindo uma apertada t -shirt e carregando uma bolsa.</td>\n",
       "      <td>&lt;O&gt; teu pai foi visto a [[entrar num]] motel com um psicopata [[usando uma]] t-shirt apertada e carregando uma bolsa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8878860</td>\n",
       "      <td>8878861</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Então está bem do meu pai?</td>\n",
       "      <td>Então o meu pai está bem?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8878861</td>\n",
       "      <td>8878862</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ele está vivo? Mas meu nariz me diz policial que há uma feiúra espalhando por todo nesta cidade.</td>\n",
       "      <td>Ele está vivo? Sim. Mas o meu nariz policial diz-me que há uma feiúra a espalhar-se por toda esta cidade.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8878863</td>\n",
       "      <td>8878864</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ninguém está a salvo.</td>\n",
       "      <td>Ninguém está seguro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8878867</td>\n",
       "      <td>8878868</td>\n",
       "      <td>True</td>\n",
       "      <td>low_similarity@simple(thr=0.51,L=4)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Sim, você deve ser.</td>\n",
       "      <td>&lt;Sim&gt;, deves estar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8878868</td>\n",
       "      <td>8878869</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[Telefone toca] &lt;Olá&gt;?</td>\n",
       "      <td>Estou?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8878869</td>\n",
       "      <td>8878870</td>\n",
       "      <td>True</td>\n",
       "      <td>low_similarity@simple(thr=0.51,L=1)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Salomão!</td>\n",
       "      <td>Solomon!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8878870</td>\n",
       "      <td>8878871</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>Strudwick! [[[caminhão se]] aproximando]</td>\n",
       "      <td>Strudwick! Eu vou-vos matar!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8878871</td>\n",
       "      <td>8878872</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>Eu vou matar [[você! [caminhão buzina de]] chifre] [[[Caminhão buzina]] de chifre] Eu acho que é a pessoa isso vai nos matar.</td>\n",
       "      <td>Eu acho que é a pessoa que nos vai matar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8878873</td>\n",
       "      <td>8878874</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Dick, minha escova de dentes está molhado.</td>\n",
       "      <td>Dick, a minha escova dos dentes está molhada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8878874</td>\n",
       "      <td>8878875</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;Você&gt; escovar os dentes com ele?</td>\n",
       "      <td>Lavaste os teus dentes com ela?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8878875</td>\n",
       "      <td>8878876</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;Não, eu&gt; não... escovar meu dentes... com a sua escova de dentes.</td>\n",
       "      <td>&lt;No&gt;, não lavei...os meus dentes... com a tua escova dos dentes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8878876</td>\n",
       "      <td>8878877</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Bem, vamos tentar e dormir um pouco.</td>\n",
       "      <td>Bem, vamos tentar dormir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8878877</td>\n",
       "      <td>8878878</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Eu nunca mais quero pensar sobre este novo dia.</td>\n",
       "      <td>Eu nunca quero pensar sobre este dia outra &lt;vez&gt;.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8878878</td>\n",
       "      <td>8878879</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Qual parte não sabe quero pensar em mais?</td>\n",
       "      <td>Qual [[é a]] parte que tu não queres pensar mais?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8878879</td>\n",
       "      <td>8878880</td>\n",
       "      <td>True</td>\n",
       "      <td>low_similarity@simple(thr=0.51,L=5)</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nossa abandono de &lt;Maria&gt;?</td>\n",
       "      <td>&lt;O nosso&gt; abandono da Mary?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8878880</td>\n",
       "      <td>8878881</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nossa colisão com aquele veado?</td>\n",
       "      <td>A nossa colisão com aquele veado?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8878881</td>\n",
       "      <td>8878882</td>\n",
       "      <td>True</td>\n",
       "      <td>low_similarity@simple(thr=0.57,L=13)</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>Ou sua epifania vem que eu sou um &lt;Spooner noturno&gt;?</td>\n",
       "      <td>Ou [[a chegada da]] tua epifânia de que eu sou uma colher nocturna?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8878882</td>\n",
       "      <td>8878883</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Porque é tudo demais para mim.</td>\n",
       "      <td>Porque é demais para mim.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8878884</td>\n",
       "      <td>8878885</td>\n",
       "      <td>False</td>\n",
       "      <td>ok@simple</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Dick, pare de pensar sobre &lt;si mesmo&gt;.</td>\n",
       "      <td>Dick, pára de pensar sobre ti.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  activate_filter                                reason  \\\n",
       "0   8878855  8878856            False                             ok@simple   \n",
       "1   8878856  8878857            False                             ok@simple   \n",
       "2   8878857  8878858            False                             ok@simple   \n",
       "3   8878858  8878859             True   low_similarity@simple(thr=0.51,L=6)   \n",
       "4   8878859  8878860            False                             ok@simple   \n",
       "5   8878860  8878861            False                             ok@simple   \n",
       "6   8878861  8878862            False                             ok@simple   \n",
       "7   8878863  8878864            False                             ok@simple   \n",
       "8   8878867  8878868             True   low_similarity@simple(thr=0.51,L=4)   \n",
       "9   8878868  8878869            False                             ok@simple   \n",
       "10  8878869  8878870             True   low_similarity@simple(thr=0.51,L=1)   \n",
       "11  8878870  8878871            False                             ok@simple   \n",
       "12  8878871  8878872            False                             ok@simple   \n",
       "13  8878873  8878874            False                             ok@simple   \n",
       "14  8878874  8878875            False                             ok@simple   \n",
       "15  8878875  8878876            False                             ok@simple   \n",
       "16  8878876  8878877            False                             ok@simple   \n",
       "17  8878877  8878878            False                             ok@simple   \n",
       "18  8878878  8878879            False                             ok@simple   \n",
       "19  8878879  8878880             True   low_similarity@simple(thr=0.51,L=5)   \n",
       "20  8878880  8878881            False                             ok@simple   \n",
       "21  8878881  8878882             True  low_similarity@simple(thr=0.57,L=13)   \n",
       "22  8878882  8878883            False                             ok@simple   \n",
       "23  8878884  8878885            False                             ok@simple   \n",
       "\n",
       "    base_sim    br_cov    pt_cov   cov_gap  br_int_content_ratio  \\\n",
       "0   1.000000  1.000000  1.000000  0.000000              0.000000   \n",
       "1   0.583333  1.000000  1.000000  0.000000              0.000000   \n",
       "2   0.571429  0.571429  1.000000  0.428571              0.000000   \n",
       "3   0.500000  0.500000  1.000000  0.500000              0.500000   \n",
       "4   0.666667  0.809524  0.750000  0.059524              0.272727   \n",
       "5   0.857143  1.000000  1.000000  0.000000              0.000000   \n",
       "6   0.840000  1.000000  1.000000  0.000000              0.000000   \n",
       "7   0.800000  1.000000  1.000000  0.000000              0.000000   \n",
       "8   0.500000  1.000000  0.666667  0.333333              0.000000   \n",
       "9   0.750000  0.666667  1.000000  0.333333              0.000000   \n",
       "10  0.500000  1.000000  1.000000  0.000000              0.000000   \n",
       "11  0.571429  0.500000  1.000000  0.500000              0.333333   \n",
       "12  0.750000  0.727273  1.000000  0.272727              0.384615   \n",
       "13  0.700000  1.000000  1.000000  0.000000              0.000000   \n",
       "14  0.666667  0.833333  1.000000  0.166667              0.000000   \n",
       "15  0.750000  0.833333  0.916667  0.083333              0.000000   \n",
       "16  0.857143  1.000000  1.000000  0.000000              0.000000   \n",
       "17  0.909091  1.000000  0.888889  0.111111              0.000000   \n",
       "18  0.615385  1.000000  0.800000  0.200000              0.000000   \n",
       "19  0.375000  0.750000  0.600000  0.150000              0.000000   \n",
       "20  0.833333  1.000000  1.000000  0.000000              0.000000   \n",
       "21  0.526316  0.800000  0.769231  0.030769              0.000000   \n",
       "22  0.833333  1.000000  1.000000  0.000000              0.000000   \n",
       "23  0.666667  0.714286  1.000000  0.285714              0.000000   \n",
       "\n",
       "    pt_int_content_ratio  spillish  \\\n",
       "0               0.000000  0.000000   \n",
       "1               0.000000  0.000000   \n",
       "2               0.000000  0.000000   \n",
       "3               0.000000  0.428571   \n",
       "4               0.230769  0.000000   \n",
       "5               0.000000  0.000000   \n",
       "6               0.000000  0.000000   \n",
       "7               0.000000  0.000000   \n",
       "8               0.000000  0.000000   \n",
       "9               0.000000  0.000000   \n",
       "10              0.000000  0.000000   \n",
       "11              0.000000  0.058824   \n",
       "12              0.000000  0.142857   \n",
       "13              0.000000  0.000000   \n",
       "14              0.000000  0.000000   \n",
       "15              0.000000  0.000000   \n",
       "16              0.000000  0.000000   \n",
       "17              0.000000  0.000000   \n",
       "18              0.000000  0.000000   \n",
       "19              0.000000  0.000000   \n",
       "20              0.000000  0.000000   \n",
       "21              0.166667  0.125000   \n",
       "22              0.000000  0.000000   \n",
       "23              0.000000  0.000000   \n",
       "\n",
       "                                                                                                                     br_highlight  \\\n",
       "0                                                                                         Mesmo um cervo pode ser uma testemunha.   \n",
       "1                                                                                            Talvez ele viu algo e pagou o preço.   \n",
       "2                                                                          [<Conversas de rádio> da polícia, indistinto] Orville.   \n",
       "3                                                                                      [Conversas [[de rádio, indistinto]]] 10-4.   \n",
       "4   Seu pai [[era manchou]] a [[verificação em]] um motel com um psicopata vestindo uma apertada t -shirt e carregando uma bolsa.   \n",
       "5                                                                                                      Então está bem do meu pai?   \n",
       "6                                Ele está vivo? Mas meu nariz me diz policial que há uma feiúra espalhando por todo nesta cidade.   \n",
       "7                                                                                                           Ninguém está a salvo.   \n",
       "8                                                                                                             Sim, você deve ser.   \n",
       "9                                                                                                          [Telefone toca] <Olá>?   \n",
       "10                                                                                                                       Salomão!   \n",
       "11                                                                                       Strudwick! [[[caminhão se]] aproximando]   \n",
       "12  Eu vou matar [[você! [caminhão buzina de]] chifre] [[[Caminhão buzina]] de chifre] Eu acho que é a pessoa isso vai nos matar.   \n",
       "13                                                                                     Dick, minha escova de dentes está molhado.   \n",
       "14                                                                                              <Você> escovar os dentes com ele?   \n",
       "15                                                             <Não, eu> não... escovar meu dentes... com a sua escova de dentes.   \n",
       "16                                                                                           Bem, vamos tentar e dormir um pouco.   \n",
       "17                                                                                Eu nunca mais quero pensar sobre este novo dia.   \n",
       "18                                                                                      Qual parte não sabe quero pensar em mais?   \n",
       "19                                                                                                     Nossa abandono de <Maria>?   \n",
       "20                                                                                                Nossa colisão com aquele veado?   \n",
       "21                                                                           Ou sua epifania vem que eu sou um <Spooner noturno>?   \n",
       "22                                                                                                 Porque é tudo demais para mim.   \n",
       "23                                                                                         Dick, pare de pensar sobre <si mesmo>.   \n",
       "\n",
       "                                                                                                             pt_highlight  \n",
       "0                                                                                   Até um veado pode ser uma testemunha.  \n",
       "1                                                                        Talvez tenha visto alguma coisa e pagou o preço.  \n",
       "2                                                                                                                Orville.  \n",
       "3                                                                                                                   10-4.  \n",
       "4   <O> teu pai foi visto a [[entrar num]] motel com um psicopata [[usando uma]] t-shirt apertada e carregando uma bolsa.  \n",
       "5                                                                                               Então o meu pai está bem?  \n",
       "6               Ele está vivo? Sim. Mas o meu nariz policial diz-me que há uma feiúra a espalhar-se por toda esta cidade.  \n",
       "7                                                                                                    Ninguém está seguro.  \n",
       "8                                                                                                     <Sim>, deves estar.  \n",
       "9                                                                                                                  Estou?  \n",
       "10                                                                                                               Solomon!  \n",
       "11                                                                                           Strudwick! Eu vou-vos matar!  \n",
       "12                                                                              Eu acho que é a pessoa que nos vai matar.  \n",
       "13                                                                          Dick, a minha escova dos dentes está molhada.  \n",
       "14                                                                                        Lavaste os teus dentes com ela?  \n",
       "15                                                       <No>, não lavei...os meus dentes... com a tua escova dos dentes.  \n",
       "16                                                                                              Bem, vamos tentar dormir.  \n",
       "17                                                                      Eu nunca quero pensar sobre este dia outra <vez>.  \n",
       "18                                                                      Qual [[é a]] parte que tu não queres pensar mais?  \n",
       "19                                                                                            <O nosso> abandono da Mary?  \n",
       "20                                                                                      A nossa colisão com aquele veado?  \n",
       "21                                                    Ou [[a chegada da]] tua epifânia de que eu sou uma colher nocturna?  \n",
       "22                                                                                              Porque é demais para mim.  \n",
       "23                                                                                         Dick, pára de pensar sobre ti.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq = preview_alignment_quality_window_with_highlights_simple(\n",
    "    start_line=8878855, window=30,\n",
    "    use_window=True,           # align against prev+here+next window\n",
    "    show_when=\"all\",\n",
    "    flag_fn=alignment_similarity_only_flag,\n",
    "    thresholds=dict(\n",
    "        length_mode=\"total\", \n",
    "        base_min_sim=0.51,\n",
    "        long_min_sim=0.75,\n",
    "        short_len=8,\n",
    "        long_len=28,\n",
    "    )\n",
    "    )\n",
    "\n",
    "hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821ba05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae355637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- line_no=18143 ---\n",
      "L tokens: ['Tal', 'operação', 'é', 'muito', 'cara']\n",
      "R tokens: ['Uma', 'operação', 'assim', 'é', 'muito', 'cara']\n",
      "L0 codepoints: ['U+0054', 'U+0061', 'U+006C']\n",
      "R0 codepoints: ['U+0055', 'U+006D', 'U+0061']\n",
      "L0 NFC==R0 NFC? False\n",
      "pairs: [(1, 1), (2, 3), (3, 4), (4, 5)]\n",
      "uncovered L indices: [0]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "def _cp(s):  # code points for sanity\n",
    "    return [f\"U+{ord(c):04X}\" for c in s]\n",
    "\n",
    "def debug_alignment(br, pt, method=\"union\"):\n",
    "    L = ali_tokenize(br or \"\")\n",
    "    R = ali_tokenize(pt or \"\")\n",
    "    print(\"L tokens:\", L)\n",
    "    print(\"R tokens:\", R)\n",
    "    print(\"L0 codepoints:\", _cp(L[0]) if L else [])\n",
    "    print(\"R0 codepoints:\", _cp(R[0]) if R else [])\n",
    "    print(\"L0 NFC==R0 NFC?\", ud.normalize(\"NFC\", L[0] if L else \"\") == ud.normalize(\"NFC\", R[0] if R else \"\"))\n",
    "\n",
    "    pairs = _raw_pairs(L, R, method=method)\n",
    "    print(\"pairs:\", pairs)\n",
    "    covered_L = {i for i,_ in pairs}\n",
    "    print(\"uncovered L indices:\", [i for i in range(len(L)) if i not in covered_L])\n",
    "\n",
    "import duckdb\n",
    "\n",
    "def run_debug_for_line(line_no: int, method=\"union\", db_path=DB):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        row = con.execute(\n",
    "            \"SELECT sent_pt_br, sent_pt_pt FROM opus_moses WHERE line_no = ?\",\n",
    "            [int(line_no)]\n",
    "        ).fetchone()\n",
    "    if not row:\n",
    "        print(f\"line_no {line_no} not found\"); return\n",
    "    br, pt = row\n",
    "    print(f\"\\n--- line_no={line_no} ---\")\n",
    "    debug_alignment(br, pt, method=method)\n",
    "\n",
    "\n",
    "run_debug_for_line(18143, method=\"inter\")   # compare methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98cb4a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msimalign\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceAligner\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     _ALIGNER = \u001b[43mSentenceAligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxlmr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatching_methods\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSimAlign required. Install:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  pip install simalign torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImport error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    247\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/simalign/simalign.py:90\u001b[39m, in \u001b[36mSentenceAligner.__init__\u001b[39m\u001b[34m(self, model, token_type, distortion, matching_methods, device, layer)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.matching_methods = [all_matching_methods[m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matching_methods]\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.device = torch.device(device)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28mself\u001b[39m.embed_loader = \u001b[43mEmbeddingLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/simalign/simalign.py:45\u001b[39m, in \u001b[36mEmbeddingLoader.__init__\u001b[39m\u001b[34m(self, model, device, layer)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m TR_Models:\n\u001b[32m     44\u001b[39m \tmodel_class, tokenizer_class = TR_Models[model]\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \t\u001b[38;5;28mself\u001b[39m.emb_model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \t\u001b[38;5;28mself\u001b[39m.emb_model.eval()\n\u001b[32m     47\u001b[39m \t\u001b[38;5;28mself\u001b[39m.emb_model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_utils.py:707\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_meta_state_dict:\n\u001b[32m    705\u001b[39m     \u001b[38;5;66;03m# This is the name of the parameter as it appears on disk file\u001b[39;00m\n\u001b[32m    706\u001b[39m     serialized_param_name = reverse_renaming_mapping[param_name]\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     param = \u001b[43mfile_pointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserialized_param_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     param = empty_param.to(tensor_device)  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import re, gc, duckdb, pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------------------------\n",
    "# Small utilities / schema\n",
    "# ---------------------------\n",
    "def _as_int_or_none(x):\n",
    "    if x is None: return None\n",
    "    try:\n",
    "        if pd.isna(x): return None\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(str(x).strip())\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _ensure_opus_ops_tables(con: duckdb.DuckDBPyConnection):\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_update(\n",
    "            line_no BIGINT PRIMARY KEY,\n",
    "            sent_pt_br TEXT,\n",
    "            sent_pt_pt TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_delete(\n",
    "            line_no BIGINT PRIMARY KEY\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_progress(\n",
    "            done_through BIGINT\n",
    "        )\n",
    "    \"\"\")\n",
    "    if con.execute(\"SELECT COUNT(*) FROM opus_ops_progress\").fetchone()[0] == 0:\n",
    "        con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "def _ensure_simple_filter_table(con: duckdb.DuckDBPyConnection):\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_filter_simple(\n",
    "            line_no BIGINT PRIMARY KEY,\n",
    "            pair_id BIGINT,\n",
    "            reason  TEXT,\n",
    "            base_sim DOUBLE,\n",
    "            thr     DOUBLE,\n",
    "            L       INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "# ---------------------------\n",
    "# A) PREPROCESSING\n",
    "# ---------------------------\n",
    "def plan_ops_over_corpus(block_size=50_000, reset=False, *, db_path=None):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        _ensure_opus_ops_tables(con)\n",
    "        lo, hi = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        if reset:\n",
    "            con.execute(\"DELETE FROM opus_ops_update\")\n",
    "            con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "            con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "            con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "        done = int(con.execute(\"SELECT done_through FROM opus_ops_progress\").fetchone()[0])\n",
    "        cur  = max(lo, done + 1)\n",
    "\n",
    "        carry = None\n",
    "        last_br, last_pt = None, None\n",
    "\n",
    "        while cur <= hi:\n",
    "            win = min(block_size, hi - cur + 1)\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [cur, cur+win-1]).df()\n",
    "\n",
    "            rows = df.to_dict(\"records\")\n",
    "            if carry is not None:\n",
    "                rows = [carry] + rows\n",
    "                carry = None\n",
    "\n",
    "            updates, deletes = [], []\n",
    "            i, n = 0, len(rows)\n",
    "            while i < n:\n",
    "                base = rows[i]; i += 1\n",
    "                br = _normalize_line_text(base.get(\"sent_pt_br\"))\n",
    "                pt = _normalize_line_text(base.get(\"sent_pt_pt\"))\n",
    "                group_lines = [int(base[\"line_no\"])]\n",
    "\n",
    "                while i < n:\n",
    "                    nxt = rows[i]\n",
    "                    br2 = _normalize_line_text(nxt.get(\"sent_pt_br\"))\n",
    "                    pt2 = _normalize_line_text(nxt.get(\"sent_pt_pt\"))\n",
    "                    if _should_merge_pair(br, br2, pt, pt2):\n",
    "                        br = _join_text(br, br2)\n",
    "                        pt = _join_text(pt, pt2)\n",
    "                        group_lines.append(int(nxt[\"line_no\"]))\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if i >= n:\n",
    "                    carry = {\n",
    "                        \"line_no\": group_lines[0],\n",
    "                        \"pair_id\": _as_int_or_none(base.get(\"pair_id\")),\n",
    "                        \"sent_pt_br\": br,\n",
    "                        \"sent_pt_pt\": pt\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "                if (br.strip() == \"\") or (pt.strip() == \"\"):\n",
    "                    for ln in group_lines:\n",
    "                        deletes.append({\"line_no\": int(ln)})\n",
    "                    last_br, last_pt = None, None\n",
    "                    continue\n",
    "\n",
    "                if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                    br = _capitalize_first_alpha(br)\n",
    "                if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                    pt = _capitalize_first_alpha(pt)\n",
    "\n",
    "                head = group_lines[0]\n",
    "                if br != base.get(\"sent_pt_br\") or pt != base.get(\"sent_pt_pt\") or len(group_lines) > 1:\n",
    "                    updates.append({\"line_no\": int(head), \"sent_pt_br\": br, \"sent_pt_pt\": pt})\n",
    "                for ln in group_lines[1:]:\n",
    "                    deletes.append({\"line_no\": int(ln)})\n",
    "\n",
    "                last_br, last_pt = br, pt\n",
    "\n",
    "            if updates:\n",
    "                con.register(\"upd\", pd.DataFrame(updates))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                    SELECT line_no, sent_pt_br, sent_pt_pt FROM upd\n",
    "                    ON CONFLICT(line_no) DO UPDATE SET\n",
    "                        sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                        sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "                \"\"\")\n",
    "                con.unregister(\"upd\")\n",
    "            if deletes:\n",
    "                con.register(\"del\", pd.DataFrame(deletes))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_delete (line_no)\n",
    "                    SELECT DISTINCT line_no FROM del\n",
    "                    ON CONFLICT(line_no) DO NOTHING\n",
    "                \"\"\")\n",
    "                con.unregister(\"del\")\n",
    "\n",
    "            del df, rows, updates, deletes\n",
    "            gc.collect()\n",
    "\n",
    "            cur += win\n",
    "            con.execute(\"UPDATE opus_ops_progress SET done_through = ?\", [cur - 1])\n",
    "\n",
    "        if carry is not None:\n",
    "            if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                carry[\"sent_pt_br\"] = _capitalize_first_alpha(carry[\"sent_pt_br\"])\n",
    "            if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                carry[\"sent_pt_pt\"] = _capitalize_first_alpha(carry[\"sent_pt_pt\"])\n",
    "            con.register(\"tail_upd\", pd.DataFrame([{\n",
    "                \"line_no\": int(carry[\"line_no\"]),\n",
    "                \"sent_pt_br\": carry[\"sent_pt_br\"],\n",
    "                \"sent_pt_pt\": carry[\"sent_pt_pt\"],\n",
    "            }]))\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                SELECT line_no, sent_pt_br, sent_pt_pt FROM tail_upd\n",
    "                ON CONFLICT(line_no) DO UPDATE SET\n",
    "                    sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                    sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "            \"\"\")\n",
    "            con.unregister(\"tail_upd\")\n",
    "\n",
    "def apply_ops_ctas_swap(*, db_path=None, force_checkpoint=True):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        try: con.execute(\"ROLLBACK\")\n",
    "        except: pass\n",
    "\n",
    "        con.execute(\"\"\"\n",
    "            DELETE FROM opus_ops_delete\n",
    "            WHERE line_no IN (SELECT line_no FROM opus_ops_update)\n",
    "        \"\"\")\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE opus_moses_new AS\n",
    "                SELECT\n",
    "                    o.line_no,\n",
    "                    o.pair_id,\n",
    "                    COALESCE(u.sent_pt_br, o.sent_pt_br) AS sent_pt_br,\n",
    "                    COALESCE(u.sent_pt_pt, o.sent_pt_pt) AS sent_pt_pt\n",
    "                FROM opus_moses o\n",
    "                LEFT JOIN opus_ops_update u USING (line_no)\n",
    "                WHERE o.line_no NOT IN (SELECT line_no FROM opus_ops_delete)\n",
    "                ORDER BY o.line_no\n",
    "            \"\"\")\n",
    "            con.execute(\"DROP TABLE opus_moses\")\n",
    "            con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "            con.execute(\"COMMIT\")\n",
    "        except:\n",
    "            con.execute(\"ROLLBACK\"); raise\n",
    "\n",
    "        if force_checkpoint:\n",
    "            con.execute(\"FORCE CHECKPOINT\")\n",
    "\n",
    "# ---------------------------\n",
    "# B) SIMPLE ALIGNMENT FILTER\n",
    "# ---------------------------\n",
    "ALIGN_METHOD = \"inter\"  # alternatives: \"inter\", \"mwmf\", \"itermax\", \"union\"\n",
    "\n",
    "WORD = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+(?:[-'][A-Za-zÀ-ÖØ-öø-ÿ]+)*|\\d+\", re.UNICODE)\n",
    "PT_STOPWORDS = {\n",
    "    \"a\",\"à\",\"ao\",\"aos\",\"as\",\"o\",\"os\",\"um\",\"uma\",\"de\",\"da\",\"das\",\"do\",\"dos\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\n",
    "    \"e\",\"ou\",\"que\",\"se\",\"por\",\"para\",\"com\",\"como\",\"mais\",\"mas\",\"não\",\"sim\",\"já\",\"sua\",\"seu\",\"suas\",\"seus\",\n",
    "    \"eu\",\"tu\",\"ele\",\"ela\",\"nós\",\"vós\",\"eles\",\"elas\",\"me\",\"te\",\"lhe\",\"nos\",\"vos\",\"lhes\",\"isso\",\"isto\",\"aquilo\"\n",
    "}\n",
    "\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return WORD.findall(s or \"\")\n",
    "\n",
    "def ali_tokenize(s: str) -> List[str]:\n",
    "    return tokenize(s)\n",
    "\n",
    "def is_content_token(tok: str) -> bool:\n",
    "    t = (tok or \"\").lower()\n",
    "    return (t not in PT_STOPWORDS) and (len(t) > 1)\n",
    "\n",
    "try:\n",
    "    from simalign import SentenceAligner\n",
    "    _ALIGNER = SentenceAligner(model=\"xlmr\", token_type=\"word\", matching_methods=\"a\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"SimAlign required. Install:\\n\"\n",
    "        \"  pip install simalign torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "def _raw_pairs(l_tokens, r_tokens, method: str = ALIGN_METHOD):\n",
    "    \"\"\"\n",
    "    Ask SimAlign for alignments, but never crash.\n",
    "    - Catches SimAlign internal errors (IndexError, ValueError, etc.)\n",
    "    - Handles missing keys ('itermax'/'inter'/'mwmf') and falls back.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = _ALIGNER.get_word_aligns(l_tokens, r_tokens)  # dict-like\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(out, dict):\n",
    "        return []\n",
    "\n",
    "    # normalize keys robustly\n",
    "    keys = {k.lower(): k for k in out.keys()}\n",
    "    def get(name: str):\n",
    "        return out.get(keys.get(name.lower()), [])\n",
    "\n",
    "    if method.lower() == \"union\":\n",
    "        return list({*get(\"inter\"), *get(\"itermax\"), *get(\"mwmf\")})\n",
    "\n",
    "    pairs = get(method)\n",
    "    if pairs:\n",
    "        return pairs\n",
    "\n",
    "    # graceful fallbacks\n",
    "    for alt in (\"itermax\", \"inter\", \"mwmf\"):\n",
    "        pairs = get(alt)\n",
    "        if pairs:\n",
    "            return pairs\n",
    "    return []\n",
    "\n",
    "\n",
    "def _form_norm(t: str) -> str:\n",
    "    import unicodedata as ud\n",
    "    return ud.normalize(\"NFC\", t or \"\").casefold()\n",
    "\n",
    "def _type_sim_from_pairs(L_tokens, R_tokens, pairs):\n",
    "    Ln = [_form_norm(t) for t in L_tokens]\n",
    "    Rn = [_form_norm(t) for t in R_tokens]\n",
    "    union = set(Ln) | set(Rn)\n",
    "    if not union: return 1.0\n",
    "    aligned = set()\n",
    "    for i, j in pairs:\n",
    "        if 0 <= i < len(Ln): aligned.add(Ln[i])\n",
    "        if 0 <= j < len(Rn): aligned.add(Rn[j])\n",
    "    return len(aligned) / len(union)\n",
    "\n",
    "def _smooth_small_gaps(covered: set, n_tokens: int, max_gap: int = 1):\n",
    "    C = set(covered); i = 0\n",
    "    while i < n_tokens:\n",
    "        if i not in C:\n",
    "            j = i\n",
    "            while j < n_tokens and j not in C: j += 1\n",
    "            gap = j - i\n",
    "            if 0 < gap <= max_gap and i > 0 and j < n_tokens:\n",
    "                for k in range(i, j): C.add(k)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return C\n",
    "\n",
    "def _interior_uncovered(tokens: List[str], covered: set) -> List[Tuple[int,int]]:\n",
    "    runs, cur = [], []\n",
    "    for i in range(len(tokens)):\n",
    "        if i not in covered: cur.append(i)\n",
    "        elif cur: runs.append((cur[0], cur[-1])); cur=[]\n",
    "    if cur: runs.append((cur[0], cur[-1]))\n",
    "    n = len(tokens)\n",
    "    return [(i0,i1) for (i0,i1) in runs if i0>0 and i1<n-1]\n",
    "\n",
    "def _content_count(tokens: List[str]) -> int:\n",
    "    return sum(1 for t in tokens if is_content_token(t))\n",
    "\n",
    "def _type_jaccard(L_tokens, R_tokens) -> float:\n",
    "    Ln = {_form_norm(t) for t in L_tokens}\n",
    "    Rn = {_form_norm(t) for t in R_tokens}\n",
    "    U = Ln | Rn\n",
    "    return (len(Ln & Rn) / len(U)) if U else 1.0\n",
    "\n",
    "def new_sim(br: str, pt: str, method: str = ALIGN_METHOD) -> float:\n",
    "    L = ali_tokenize(br or \"\"); R = ali_tokenize(pt or \"\")\n",
    "    if not L and not R: return 1.0\n",
    "    if not L or not R:  return 0.0\n",
    "    try:\n",
    "        pairs = _raw_pairs(L, R, method=method)\n",
    "    except Exception:\n",
    "        pairs = []\n",
    "    if not pairs:\n",
    "        return _type_jaccard(L, R)\n",
    "    return _type_sim_from_pairs(L, R, pairs)\n",
    "\n",
    "\n",
    "def alignment_quality_features(\n",
    "    br_prev: str, br_here: str, br_next: str,\n",
    "    pt_prev: str, pt_here: str, pt_next: str,\n",
    "    *, use_window: bool = True, sim_fn=None\n",
    ") -> dict:\n",
    "    if sim_fn is None:\n",
    "        sim_fn = new_sim\n",
    "\n",
    "    br_toks = ali_tokenize(br_here); pt_toks = ali_tokenize(pt_here)\n",
    "    if use_window:\n",
    "        pt_win = ali_tokenize(\" \".join(x for x in [pt_prev, pt_here, pt_next] if x))\n",
    "        br_pairs = _raw_pairs(br_toks, pt_win, method=ALIGN_METHOD)\n",
    "        br_cov = {i for i,_ in br_pairs}\n",
    "        br_win = ali_tokenize(\" \".join(x for x in [br_prev, br_here, br_next] if x))\n",
    "        pt_pairs = _raw_pairs(pt_toks, br_win, method=ALIGN_METHOD)\n",
    "        pt_cov = {i for i,_ in pt_pairs}\n",
    "    else:\n",
    "        pairs  = _raw_pairs(br_toks, pt_toks, method=ALIGN_METHOD)\n",
    "        br_cov = {i for i,_ in pairs}\n",
    "        pt_cov = {j for _,j in pairs}\n",
    "\n",
    "    br_cov = _smooth_small_gaps(br_cov, len(br_toks), 1)\n",
    "    pt_cov = _smooth_small_gaps(pt_cov, len(pt_toks), 1)\n",
    "\n",
    "    br_len = len(br_toks); pt_len = len(pt_toks)\n",
    "    br_cov_ratio = len(br_cov) / max(1, br_len)\n",
    "    pt_cov_ratio = len(pt_cov) / max(1, pt_len)\n",
    "    cov_min = min(br_cov_ratio, pt_cov_ratio)\n",
    "    cov_gap = abs(br_cov_ratio - pt_cov_ratio)\n",
    "\n",
    "    br_int_spans = _interior_uncovered(br_toks, br_cov)\n",
    "    pt_int_spans = _interior_uncovered(pt_toks, pt_cov)\n",
    "\n",
    "    def _content_in_runs(tokens, runs):\n",
    "        return sum(1 for i0,i1 in runs for t in tokens[i0:i1+1] if is_content_token(t))\n",
    "\n",
    "    br_ct = _content_count(br_toks)\n",
    "    pt_ct = _content_count(pt_toks)\n",
    "    br_int_ratio = _content_in_runs(br_toks, br_int_spans) / max(1, br_ct)\n",
    "    pt_int_ratio = _content_in_runs(pt_toks, pt_int_spans) / max(1, pt_ct)\n",
    "\n",
    "    base_sim = float(new_sim(br_here, pt_here, method=ALIGN_METHOD))\n",
    "\n",
    "    return {\n",
    "        \"br_cov\": br_cov_ratio, \"pt_cov\": pt_cov_ratio,\n",
    "        \"cov_min\": cov_min, \"cov_gap\": cov_gap,\n",
    "        \"br_int_content_ratio\": br_int_ratio, \"pt_int_content_ratio\": pt_int_ratio,\n",
    "        \"br_content_total\": br_ct, \"pt_content_total\": pt_ct,\n",
    "        \"br_len\": br_len, \"pt_len\": pt_len,\n",
    "        \"base_sim\": base_sim\n",
    "    }\n",
    "\n",
    "def alignment_similarity_only_flag(\n",
    "    feats: dict,\n",
    "    *,\n",
    "    min_sim_ok: float | None = None,\n",
    "    length_mode: str = \"total\",\n",
    "    base_min_sim: float = 0.30,\n",
    "    long_min_sim: float = 0.70,\n",
    "    short_len: int = 8,\n",
    "    long_len: int = 28,\n",
    "    **kwargs,\n",
    ") -> Tuple[bool,str,float,int]:\n",
    "    sim = float(feats[\"base_sim\"])\n",
    "\n",
    "    if min_sim_ok is not None:\n",
    "        thr = float(min_sim_ok)\n",
    "        L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "    else:\n",
    "        if length_mode == \"total\":\n",
    "            L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "        elif length_mode == \"char\":\n",
    "            L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "        else:  # \"content\"\n",
    "            L = max(int(feats.get(\"br_content_total\", 0)), int(feats.get(\"pt_content_total\", 0)))\n",
    "\n",
    "        if L <= short_len:\n",
    "            thr = base_min_sim\n",
    "        elif L >= long_len:\n",
    "            thr = long_min_sim\n",
    "        else:\n",
    "            t = (L - short_len) / max(1, (long_len - short_len))\n",
    "            thr = base_min_sim + t * (long_min_sim - base_min_sim)\n",
    "\n",
    "    activate = sim < thr\n",
    "    reason = (f\"low_similarity@simple(thr={thr:.2f},L={L})\" if activate else \"ok@simple\")\n",
    "    return activate, reason, thr, L\n",
    "\n",
    "# ---------------------------\n",
    "# C) Simple filter across the corpus\n",
    "# ---------------------------\n",
    "def build_simple_filter_flags_chunked(\n",
    "    *,\n",
    "    db_path,\n",
    "    chunk_size: int = 50_000,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "):\n",
    "    if thresholds is None:\n",
    "        thresholds = dict(min_sim_ok=None, length_mode=\"total\",\n",
    "                          base_min_sim=0.30, long_min_sim=0.70,\n",
    "                          short_len=8, long_len=28)\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        _ensure_simple_filter_table(con)\n",
    "        mn, mx = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"[simple-filter] opus_moses empty; nothing to do.\")\n",
    "            return\n",
    "        mn, mx = int(mn), int(mx)\n",
    "\n",
    "        cur = mn\n",
    "        total_flagged = 0\n",
    "        while cur <= mx:\n",
    "            hi = min(cur + chunk_size - 1, mx)\n",
    "\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [max(mn, cur-1), min(mx, hi+1)]).df()\n",
    "\n",
    "            rows = []\n",
    "            n = len(df)\n",
    "            for idx in range(n):\n",
    "                line_no = int(df.line_no.iloc[idx])\n",
    "                if line_no < cur or line_no > hi:\n",
    "                    continue\n",
    "                br_prev = df.sent_pt_br.iloc[idx-1] if idx > 0 else \"\"\n",
    "                br_here = df.sent_pt_br.iloc[idx]\n",
    "                br_next = df.sent_pt_br.iloc[idx+1] if idx+1 < n else \"\"\n",
    "                pt_prev = df.sent_pt_pt.iloc[idx-1] if idx > 0 else \"\"\n",
    "                pt_here = df.sent_pt_pt.iloc[idx]\n",
    "                pt_next = df.sent_pt_pt.iloc[idx+1] if idx+1 < n else \"\"\n",
    "\n",
    "                feats = alignment_quality_features(\n",
    "                    br_prev, br_here, br_next,\n",
    "                    pt_prev, pt_here, pt_next,\n",
    "                    use_window=use_window, sim_fn=new_sim\n",
    "                )\n",
    "                activate, reason, thr, L = alignment_similarity_only_flag(feats, **thresholds)\n",
    "                if activate:\n",
    "                    rows.append({\n",
    "                        \"line_no\": line_no,\n",
    "                        \"pair_id\": _as_int_or_none(df.pair_id.iloc[idx]),\n",
    "                        \"reason\": reason,\n",
    "                        \"base_sim\": float(feats[\"base_sim\"]),\n",
    "                        \"thr\": float(thr),\n",
    "                        \"L\": int(L),\n",
    "                    })\n",
    "\n",
    "            if rows:\n",
    "                con.register(\"flags\", pd.DataFrame(rows))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_filter_simple (line_no, pair_id, reason, base_sim, thr, L)\n",
    "                    SELECT line_no, pair_id, reason, base_sim, thr, L FROM flags\n",
    "                    ON CONFLICT(line_no) DO UPDATE SET\n",
    "                        pair_id = EXCLUDED.pair_id,\n",
    "                        reason  = EXCLUDED.reason,\n",
    "                        base_sim= EXCLUDED.base_sim,\n",
    "                        thr     = EXCLUDED.thr,\n",
    "                        L       = EXCLUDED.L\n",
    "                \"\"\")\n",
    "                con.unregister(\"flags\")\n",
    "                total_flagged += len(rows)\n",
    "\n",
    "            print(f\"[simple-filter] [{cur}..{hi}] flagged={len(rows)} (cum={total_flagged})\")\n",
    "            cur = hi + 1\n",
    "\n",
    "        print(f\"[simple-filter] DONE. total flagged lines: {total_flagged}\")\n",
    "\n",
    "def apply_simple_filter_ctas_swap(*, db_path, drop_flags=False):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        con.execute(\"BEGIN\")\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE opus_moses_new AS\n",
    "                SELECT o.*\n",
    "                FROM opus_moses o\n",
    "                LEFT JOIN opus_filter_simple f USING (line_no)\n",
    "                WHERE f.line_no IS NULL\n",
    "                ORDER BY o.line_no\n",
    "            \"\"\")\n",
    "            con.execute(\"DROP TABLE opus_moses\")\n",
    "            con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "            con.execute(\"COMMIT\")\n",
    "        except:\n",
    "            con.execute(\"ROLLBACK\"); raise\n",
    "\n",
    "        con.execute(\"FORCE CHECKPOINT\")\n",
    "        kept = con.execute(\"SELECT COUNT(*) FROM opus_moses\").fetchone()[0]\n",
    "        removed = con.execute(\"SELECT COUNT(*) FROM opus_filter_simple\").fetchone()[0]\n",
    "        print(f\"[simple-filter] applied. kept={kept:,} removed={removed:,}\")\n",
    "        if drop_flags:\n",
    "            con.execute(\"DROP TABLE opus_filter_simple\")\n",
    "            con.execute(\"FORCE CHECKPOINT\")\n",
    "\n",
    "# ---------------------------\n",
    "# D) One-call convenience runner (with simple reset)\n",
    "# ---------------------------\n",
    "def run_opus_pipeline_simple(\n",
    "    *,\n",
    "    db_path,\n",
    "    block_size: int = 50_000,\n",
    "    neighbor_overlap: int = 3,\n",
    "    neighbor_margin: float = 0.04,\n",
    "    neighbor_max_clause_chars: int = 60,\n",
    "    neighbor_max_iters: int = 5,\n",
    "    dedup_chunk_size: int = 50_000,\n",
    "    delete_on_trigger: bool = True,\n",
    "    filter_chunk_size: int = 50_000,\n",
    "    filter_use_window: bool = True,\n",
    "    filter_thresholds: dict | None = None,\n",
    "    apply_filter: bool = True,\n",
    "    reset: bool = False,          # <<< simple switch\n",
    "):\n",
    "    # reset planning progress + clear flags if asked\n",
    "    if reset:\n",
    "        with duckdb.connect(str(db_path)) as con:\n",
    "            _ensure_opus_ops_tables(con)\n",
    "            _ensure_simple_filter_table(con)\n",
    "            con.execute(\"DELETE FROM opus_ops_update\")\n",
    "            con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "            con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "            con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "            con.execute(\"DELETE FROM opus_filter_simple\")\n",
    "        print(\"[pipeline] reset: cleared ops progress and flags.\")\n",
    "\n",
    "    # 1) plan + swap (merge lines, clean)\n",
    "    plan_ops_over_corpus(block_size=block_size, reset=False, db_path=db_path)\n",
    "    apply_ops_ctas_swap(db_path=db_path)\n",
    "\n",
    "    # 2) neighbor moves (assumes your function exists)\n",
    "    apply_neighbor_moves_corpus_inplace(\n",
    "        block_size=block_size,\n",
    "        overlap=neighbor_overlap,\n",
    "        margin=neighbor_margin,\n",
    "        max_clause_chars=neighbor_max_clause_chars,\n",
    "        max_iters=neighbor_max_iters,\n",
    "    )\n",
    "\n",
    "    # 3) repeated-prefix cleaner (assumes your function exists)\n",
    "    run_repeated_prefix_cleaner_chunked(\n",
    "        db_path=db_path,\n",
    "        prev_window=6,\n",
    "        min_prefix_tokens=6,\n",
    "        coverage_thresh=0.92,\n",
    "        require_both=False,\n",
    "        collapse_adjacent_dups=True,\n",
    "        delete_on_trigger=delete_on_trigger,\n",
    "        delete_if_empty_only=False,\n",
    "        chunk_size=dedup_chunk_size,\n",
    "        start_line=None, end_line=None,\n",
    "        apply_changes=True,\n",
    "        print_updates=False,\n",
    "        trace_lines=None\n",
    "    )\n",
    "\n",
    "    # 4) simple alignment filter → write flags\n",
    "    build_simple_filter_flags_chunked(\n",
    "        db_path=db_path,\n",
    "        chunk_size=filter_chunk_size,\n",
    "        use_window=filter_use_window,\n",
    "        thresholds=filter_thresholds\n",
    "    )\n",
    "\n",
    "    # 5) apply filter (CTAS swap) if requested\n",
    "    if apply_filter:\n",
    "        apply_simple_filter_ctas_swap(db_path=db_path, drop_flags=False)\n",
    "\n",
    "    print(\"[pipeline] complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "\n",
    "run_opus_pipeline_simple(\n",
    "    db_path=DB_PATH,\n",
    "    block_size=50_000,\n",
    "    filter_use_window=False,            # prev/here/next context for SimAlign\n",
    "    filter_thresholds=dict(            # tune these if you like\n",
    "        min_sim_ok=None,               # None → length-adaptive threshold\n",
    "        length_mode=\"total\",\n",
    "        base_min_sim=0.30,\n",
    "        long_min_sim=0.70,\n",
    "        short_len=8,\n",
    "        long_len=28,\n",
    "    ),\n",
    "    apply_filter=True                  # set False to only write flags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6410e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
