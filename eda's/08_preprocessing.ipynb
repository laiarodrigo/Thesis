{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84263414",
   "metadata": {},
   "source": [
    "**//IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facf6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laiarodrigo/repos/Thesis/thesis/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd, pathlib, itertools, textwrap, re, gc\n",
    "import numpy as np\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from rapidfuzz import fuzz, distance\n",
    "from simalign import SentenceAligner\n",
    "from typing import Optional, Dict, Any, Tuple, List, Iterable, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951770de",
   "metadata": {},
   "source": [
    "**//CONFIGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4085ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = '../data/duckdb/subs.duckdb'\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>|\\{[^}]+\\}')\n",
    "NL_RE  = re.compile(r'\\s*\\n\\s*')\n",
    "SENT_SPLIT_RE = re.compile(r'(?<=[\\.\\?\\!…])\\s+')\n",
    "\n",
    "ABBREVS = {\"dr\",\"dra\",\"sr\",\"sra\",\"srta\",\"prof\",\"profa\",\"etc\",\"av\",\"nº\",\"n.º\",\"vs\",\"p.ex\"}\n",
    "ABBR_RX = re.compile(r'\\b(' + '|'.join(re.escape(x) for x in ABBREVS) + r')\\.', re.IGNORECASE)\n",
    "\n",
    "# PREPROCESSING\n",
    "QUOTE_PAIRS = [\n",
    "    ('\"', '\"'), (\"'\", \"'\"),\n",
    "    ('“', '”'), ('‘', '’'),\n",
    "    ('«', '»'), ('„', '“'), ('‹', '›')\n",
    "]\n",
    "QUOTE_CHARS = set(ch for L,R in QUOTE_PAIRS for ch in (L,R))\n",
    "\n",
    "HARD_STOPS = \".?!…\"\n",
    "SOFT_TAILS = \",;:—–-\"\n",
    "DASHES     = \"-–—\"\n",
    "\n",
    "LETTER      = r\"[^\\W\\d_]\"                                   # any letter, no digits/underscore\n",
    "NAME_TOKEN  = rf\"{LETTER}(?:{LETTER}|[.'\\-])*\"              # e.g., Steve, O'Neill, João-Pedro\n",
    "NAME_PHRASE = rf\"{NAME_TOKEN}(?:\\s+{NAME_TOKEN}){{0,2}}\"    # up to 3-word names\n",
    "\n",
    "WS = r\"(?:\\s|\\u00A0|\\u202F)*\"                               # normal/narrow/nbsp\n",
    "SPEAKER_LABEL_DROP = re.compile(\n",
    "    rf\"(^|[^\\w]){NAME_PHRASE}{WS}[:\\uFF1A]{WS}\",            # keep boundary, drop label\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "WORD = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
    "# Lightweight PT stopword set for \"content-token\" accounting\n",
    "PT_PREPS = {\"de\",\"do\",\"da\",\"dos\",\"das\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\"com\",\"para\",\"por\",\"a\",\"ao\",\"à\",\"às\",\"aos\"}\n",
    "PT_DETS  = {\"o\",\"a\",\"os\",\"as\",\"um\",\"uma\",\"uns\",\"umas\",\"este\",\"esta\",\"estes\",\"estas\",\"esse\",\"essa\",\"esses\",\"essas\",\"aquele\",\"aquela\",\"aqueles\",\"aquelas\"}\n",
    "PT_CLITICS={\"me\",\"te\",\"se\",\"lhe\",\"nos\",\"vos\",\"lhes\"}\n",
    "PT_CONJ  = {\"e\",\"ou\",\"mas\",\"nem\",\"que\",\"porque\",\"pois\",\"porém\",\"porem\"}\n",
    "PT_NEG   = {\"não\",\"nao\"}\n",
    "PT_STOPWORDS = (PT_PREPS | PT_DETS | PT_CLITICS | PT_CONJ | PT_NEG)\n",
    "\n",
    "# sentence splitter (no look-behind) for light stats only\n",
    "_SENT_RE = re.compile(r'.*?[.!?…]+(?:[\"”»\\'\\)\\]\\}]+)?(?=\\s|$)|.+?(?=\\s|$)', re.UNICODE)\n",
    "\n",
    "ALIGN_METHOD = \"inter\"  # alternatives: \"inter\" (↑recall), \"mwmf\", \"itermax\", \"union\"\n",
    "\n",
    "# === alignment tokenizer (use ONLY for SimAlign) ===\n",
    "WS_TOKEN = re.compile(r\"\\S+\", re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623fca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- cleaning + similarity ----------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return TAG_RE.sub('', NL_RE.sub(' ', s)).strip()\n",
    "\n",
    "def sim(a: str, b: str) -> float:\n",
    "    a = clean_text(a); b = clean_text(b)\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    # edit-distance core signals (all penalize insertions/deletions)\n",
    "    s_edit  = fuzz.ratio(a, b) / 100.0\n",
    "    s_sort  = fuzz.token_sort_ratio(a, b) / 100.0         # order-insensitive but still length-aware\n",
    "    s_lev   = distance.Levenshtein.normalized_similarity(a, b)  # 0..1\n",
    "\n",
    "    # penalize big length mismatches (e.g., a is much longer than b)\n",
    "    lp = min(len(a), len(b)) / max(len(a), len(b))  # 0..1\n",
    "\n",
    "    # blend; weights are tame and easy to tune\n",
    "    base = 0.5*s_edit + 0.2*s_sort + 0.3*s_lev\n",
    "    return base * (0.5 + 0.5*lp)   # shrink score when lengths differ a lot\n",
    "\n",
    "def new_sim():\n",
    "    pass\n",
    "\n",
    "# ---------- clause split (sentences first, comma/dash fallback) ----------\n",
    "def mask_abbrevs(t: str) -> str: return ABBR_RX.sub(lambda m: m.group(1)+\"§\", t or \"\")\n",
    "def unmask_abbrevs(t: str) -> str: return (t or \"\").replace(\"§\",\".\")\n",
    "\n",
    "def sentence_split(t: str):\n",
    "    tt = mask_abbrevs(t or \"\")\n",
    "    parts = [unmask_abbrevs(p).strip() for p in SENT_SPLIT_RE.split(tt.strip()) if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def split_tail_clause(text: str, max_tail_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = ' '.join(parts[:-1]).strip(); tail = parts[-1].strip()\n",
    "        if head and tail: return head, tail\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.rfind(token)\n",
    "        if k != -1 and 1 <= len(t) - (k+len(token)) <= max_tail_chars:\n",
    "            return t[:k].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def split_head_clause(text: str, max_head_chars=60):\n",
    "    parts = sentence_split(text)\n",
    "    if len(parts) >= 2:\n",
    "        head = parts[0].strip(); rest = ' '.join(parts[1:]).strip()\n",
    "        if head and rest: return head, rest\n",
    "    t = (text or \"\").strip()\n",
    "    for token in [\",\", \" - \", \" – \", \" — \"]:\n",
    "        k = t.find(token)\n",
    "        if k != -1 and 1 <= k+1 <= max_head_chars:\n",
    "            return t[:k+len(token)].strip(), t[k+len(token):].strip()\n",
    "    return None, None\n",
    "\n",
    "def ok_piece(seg: str, min_chars=6, min_tokens=2):\n",
    "    toks = [w for w in re.findall(r'\\b\\w+\\b', seg or \"\", flags=re.UNICODE) if any(c.isalpha() for c in w)]\n",
    "    return bool(seg) and len(seg) >= min_chars and len(toks) >= min_tokens\n",
    "\n",
    "def _py_int(x):\n",
    "    # robust cast for numpy/pandas scalars and plain ints\n",
    "    if isinstance(x, (np.generic,)):  # np.int64, np.int32, etc.\n",
    "        return int(x.item())\n",
    "    return int(x)\n",
    "\n",
    "def load_opus_window(start_line: int, window: int = 600) -> pd.DataFrame:\n",
    "    start_line = _py_int(start_line)\n",
    "    window     = _py_int(window)\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        df = con.execute(\"\"\"\n",
    "            SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "            FROM opus_moses\n",
    "            WHERE line_no BETWEEN ? AND ?\n",
    "            ORDER BY line_no\n",
    "        \"\"\", [start_line, start_line + window - 1]).df()\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "\n",
    "# ---------- PASS A: neighbor MOVES (choose tail→next or head←next if it increases sum) ----------\n",
    "def apply_neighbor_moves(df: pd.DataFrame, margin=0.04, max_clause_chars=60):\n",
    "    df2 = df.copy()\n",
    "    log = []\n",
    "    n = len(df2)\n",
    "    for i in range(n-1):\n",
    "        for lang, other in ((\"sent_pt_pt\",\"sent_pt_br\"), (\"sent_pt_br\",\"sent_pt_pt\")):\n",
    "            L_i,  L_ip1  = df2.at[i,lang],     df2.at[i+1,lang]\n",
    "            R_i,  R_ip1  = df2.at[i,other],    df2.at[i+1,other]\n",
    "            keep_sum = sim(L_i,R_i) + sim(L_ip1,R_ip1)\n",
    "\n",
    "            # option 1: move tail of i -> front of i+1\n",
    "            head, tail = split_tail_clause(L_i, max_tail_chars=max_clause_chars)\n",
    "            gain1 = -1e9\n",
    "            if ok_piece(tail) and ok_piece(head, min_chars=4):\n",
    "                move_sum1 = sim(head, R_i) + sim((tail + \" \" + (L_ip1 or \"\")).strip(), R_ip1)\n",
    "                gain1 = move_sum1 - keep_sum\n",
    "\n",
    "            # option 2: move head of i+1 -> end of i\n",
    "            head2, rest2 = split_head_clause(L_ip1, max_head_chars=max_clause_chars)\n",
    "            gain2 = -1e9\n",
    "            if ok_piece(head2) and ok_piece(rest2, min_chars=4):\n",
    "                move_sum2 = sim(((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip(), R_i) + sim(rest2, R_ip1)\n",
    "                gain2 = move_sum2 - keep_sum\n",
    "\n",
    "            # apply the better positive option\n",
    "            if gain1 > margin and gain1 >= gain2:\n",
    "                df2.at[i,lang]     = head\n",
    "                df2.at[i+1,lang]   = (tail + \" \" + (L_ip1 or \"\")).strip()\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"tail_to_next\", \"gain\": float(gain1)})\n",
    "            elif gain2 > margin and gain2 > gain1:\n",
    "                df2.at[i,lang]     = (((L_i or \"\") + (\" \" if L_i else \"\") + head2).strip())\n",
    "                df2.at[i+1,lang]   = rest2\n",
    "                log.append({\"i\": i, \"lang\": lang, \"op\": \"head_from_next\", \"gain\": float(gain2)})\n",
    "            # else: no move\n",
    "    return df2, pd.DataFrame(log)\n",
    "\n",
    "# # ---------- example run on a tiny window ----------\n",
    "# df  = load_opus_window(start_line=13580016, window=10)\n",
    "\n",
    "# # A) move commas/clauses across neighbors when it helps the two-row sum\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "\n",
    "# print(\"Moves:\", len(move_log))\n",
    "# print(move_log.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f006fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _moved_piece(bi, ai, bip1, aip1, op):\n",
    "    \"\"\"Best-effort extract of the moved fragment from before/after strings.\"\"\"\n",
    "    if op == \"tail_to_next\":\n",
    "        # ai = head; piece = suffix removed from bi\n",
    "        if bi.startswith(ai):\n",
    "            return bi[len(ai):].strip()\n",
    "        # fallback: prefix added to next\n",
    "        added = max(0, len(aip1) - len(bip1))\n",
    "        return aip1[:added].strip()\n",
    "    else:  # \"head_from_next\"\n",
    "        # aip1 = rest; piece = prefix removed from bip1\n",
    "        if len(bip1) > len(aip1):\n",
    "            return bip1[:len(bip1) - len(aip1)].strip()\n",
    "        # fallback: suffix added to i\n",
    "        if ai.startswith(bi):\n",
    "            return ai[len(bi):].strip()\n",
    "        return \"\"\n",
    "\n",
    "def preview_moves(df_before, df_after, move_log, k=8):\n",
    "    \"\"\"\n",
    "    Show top-k moves with before/after texts, moved fragment, and score deltas.\n",
    "    Assumes df_before/df_after are the same window (same line_no order).\n",
    "    \"\"\"\n",
    "    if move_log is None or move_log.empty:\n",
    "        print(\"No moves to preview.\")\n",
    "        return\n",
    "\n",
    "    log = move_log.sort_values(\"gain\", ascending=False).head(k)\n",
    "\n",
    "    for _, r in log.iterrows():\n",
    "        i   = int(r[\"i\"])\n",
    "        op  = r[\"op\"]\n",
    "        lang = r[\"lang\"]\n",
    "        other = \"sent_pt_pt\" if lang == \"sent_pt_br\" else \"sent_pt_br\"\n",
    "\n",
    "        # pull rows\n",
    "        bi   = df_before.at[i,   lang]\n",
    "        bip1 = df_before.at[i+1, lang]\n",
    "        ai   = df_after.at[i,    lang]\n",
    "        aip1 = df_after.at[i+1,  lang]\n",
    "\n",
    "        Ri   = df_before.at[i,   other]\n",
    "        Rip1 = df_before.at[i+1, other]  # other side doesn't change during move\n",
    "\n",
    "        piece = _moved_piece(bi, ai, bip1, aip1, op)\n",
    "\n",
    "        keep_sum = sim(bi, Ri) + sim(bip1, Rip1)\n",
    "        new_sum  = sim(ai, Ri) + sim(aip1, Rip1)\n",
    "        d_i   = sim(ai, Ri)   - sim(bi, Ri)\n",
    "        d_ip1 = sim(aip1, Rip1) - sim(bip1, Rip1)\n",
    "\n",
    "        line_i   = int(df_before.at[i,   \"line_no\"])\n",
    "        line_ip1 = int(df_before.at[i+1, \"line_no\"])\n",
    "\n",
    "        print(\"\\n────────────────────────────────────────\")\n",
    "        print(f\"lines {line_i} → {line_ip1} | {lang} | {op} | gain {float(r['gain']):.3f}\")\n",
    "        print(f\"moved piece: [{piece}]\")\n",
    "        print(f\"sum sim: {keep_sum:.3f} → {new_sum:.3f}  (Δi={d_i:+.3f}, Δi+1={d_ip1:+.3f})\")\n",
    "\n",
    "        print(\"\\n— BEFORE —\")\n",
    "        print(f\"i   ({lang}): {bi}\")\n",
    "        print(f\"i+1 ({lang}): {bip1}\")\n",
    "        print(f\"i   ({other}): {Ri}\")\n",
    "        print(f\"i+1 ({other}): {Rip1}\")\n",
    "\n",
    "        print(\"\\n— AFTER —\")\n",
    "        print(f\"i   ({lang}): {ai}\")\n",
    "        print(f\"i+1 ({lang}): {aip1}\")\n",
    "\n",
    "# # Usage example (with what you already computed):\n",
    "# df = load_opus_window(start_line=1750340, window=50)\n",
    "# moved_df, move_log = apply_neighbor_moves(df, margin=0.04, max_clause_chars=60)\n",
    "# preview_moves(df, moved_df, move_log, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937373b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_no</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>sent_pt_br</th>\n",
       "      <th>sent_pt_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6766357</td>\n",
       "      <td>6766358</td>\n",
       "      <td>Muito bem, OK. Ok? Ok.</td>\n",
       "      <td>Está. Bem, tudo bem. Está bem?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6766360</td>\n",
       "      <td>6766361</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Tudo bem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6766361</td>\n",
       "      <td>6766362</td>\n",
       "      <td>Não, não. Rosa, escute.</td>\n",
       "      <td>Adeusinho. Não, não. Rosa, ouça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6766363</td>\n",
       "      <td>6766364</td>\n",
       "      <td>Preciso encontrar a senhora Lieberman. OK.</td>\n",
       "      <td>Preciso de encontrar a Sra. Lieberman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6766365</td>\n",
       "      <td>6766366</td>\n",
       "      <td>Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.</td>\n",
       "      <td>Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6766369</td>\n",
       "      <td>6766370</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Pronto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6766370</td>\n",
       "      <td>6766371</td>\n",
       "      <td>Gracias.</td>\n",
       "      <td>Adeusinho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6766371</td>\n",
       "      <td>6766372</td>\n",
       "      <td>Adeus.</td>\n",
       "      <td>Gracias.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6766372</td>\n",
       "      <td>6766373</td>\n",
       "      <td>Devolva a bola de gude!</td>\n",
       "      <td>Dá-me esse berlinde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6766373</td>\n",
       "      <td>6766374</td>\n",
       "      <td>É do meu pai! Me dê!</td>\n",
       "      <td>É do meu pai! Dá-mo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6766374</td>\n",
       "      <td>6766375</td>\n",
       "      <td>É a bola de gude do meu pai.</td>\n",
       "      <td>É o berlinde do meu pai.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    line_no  pair_id  \\\n",
       "0   6766357  6766358   \n",
       "1   6766360  6766361   \n",
       "2   6766361  6766362   \n",
       "3   6766363  6766364   \n",
       "4   6766365  6766366   \n",
       "5   6766369  6766370   \n",
       "6   6766370  6766371   \n",
       "7   6766371  6766372   \n",
       "8   6766372  6766373   \n",
       "9   6766373  6766374   \n",
       "10  6766374  6766375   \n",
       "\n",
       "                                                                       sent_pt_br  \\\n",
       "0                                                          Muito bem, OK. Ok? Ok.   \n",
       "1                                                                          Adeus.   \n",
       "2                                                         Não, não. Rosa, escute.   \n",
       "3                                      Preciso encontrar a senhora Lieberman. OK.   \n",
       "4   Se não encontrá-la, posso perder meu emprego. Se não entender, diga \"OK\". OK.   \n",
       "5                                                                          Adeus.   \n",
       "6                                                                        Gracias.   \n",
       "7                                                                          Adeus.   \n",
       "8                                                         Devolva a bola de gude!   \n",
       "9                                                            É do meu pai! Me dê!   \n",
       "10                                                   É a bola de gude do meu pai.   \n",
       "\n",
       "                                                                                 sent_pt_pt  \n",
       "0                                                            Está. Bem, tudo bem. Está bem?  \n",
       "1                                                                                 Tudo bem.  \n",
       "2                                                          Adeusinho. Não, não. Rosa, ouça.  \n",
       "3                                                    Preciso de encontrar a Sra. Lieberman.  \n",
       "4   Está. Se não a encontrar, posso perder o meu emprego. Se não entender, diga \"Está bem\".  \n",
       "5                                                                                   Pronto.  \n",
       "6                                                                                Adeusinho.  \n",
       "7                                                                                  Gracias.  \n",
       "8                                                                      Dá-me esse berlinde.  \n",
       "9                                                                      É do meu pai! Dá-mo!  \n",
       "10                                                                 É o berlinde do meu pai.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preview_window_final_df(\n",
    "    start_line: int,\n",
    "    window: int = 50,\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run apply_neighbor_moves repeatedly (moves only) on a window until no more moves\n",
    "    or max_iters is reached. Return a DF with the same columns as opus_moses\n",
    "    (line_no, pair_id, sent_pt_br, sent_pt_pt) reflecting the FINAL subtitles.\n",
    "    \"\"\"\n",
    "    # load the original window\n",
    "    cur = load_opus_window(start_line=start_line, window=window)\n",
    "\n",
    "    # iterate moves to convergence\n",
    "    for _ in range(max_iters):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "\n",
    "    # return only the opus_moses columns, in order\n",
    "    return cur.loc[:, [\"line_no\", \"pair_id\", \"sent_pt_br\", \"sent_pt_pt\"]].copy()\n",
    "\n",
    "final_df = preview_window_final_df(start_line=6766355, window=20, margin=0.04, max_clause_chars=60, max_iters=5)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569388ce",
   "metadata": {},
   "source": [
    "**//PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa95ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_wrapping_quotes_once(s: str) -> str:\n",
    "    \"\"\"Remove exactly one balanced pair of wrapping quotes if present.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    t = s.strip()\n",
    "    for left, right in QUOTE_PAIRS:\n",
    "        if t.startswith(left) and t.endswith(right):\n",
    "            inner = t[len(left):-len(right)].strip()\n",
    "            # keep only if there's some non-quote content inside\n",
    "            if inner and any(c not in QUOTE_CHARS for c in inner):\n",
    "                return inner\n",
    "    return t\n",
    "\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    \"\"\"Peel multiple layers, e.g., “ 'foo' ” -> foo.\"\"\"\n",
    "    prev, cur = None, s\n",
    "    while cur != prev:\n",
    "        prev = cur\n",
    "        cur = _strip_wrapping_quotes_once(cur)\n",
    "    return cur\n",
    "\n",
    "def _strip_edge_quotes_unbalanced(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Also remove lone leading/trailing quotes if they remain (unbalanced).\n",
    "    Repeats until no edge quote remains or only quotes are left.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    t = s.strip()\n",
    "    lefts  = {L for L,_ in QUOTE_PAIRS}\n",
    "    rights = {R for _,R in QUOTE_PAIRS}\n",
    "\n",
    "    changed = True\n",
    "    while changed and t:\n",
    "        changed = False\n",
    "        if t and t[0] in lefts:\n",
    "            t = t[1:].lstrip(); changed = True\n",
    "        if t and t[-1] in rights:\n",
    "            t = t[:-1].rstrip(); changed = True\n",
    "        # stop if the remainder is only quotes/spaces\n",
    "        if t and all((c in QUOTE_CHARS) or c.isspace() for c in t):\n",
    "            break\n",
    "    return t\n",
    "\n",
    "\n",
    "def _drop_speaker_labels_keep_content(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    # normalize space variants first\n",
    "    s = (s.replace(\"\\u00A0\", \" \")\n",
    "           .replace(\"\\u202F\", \" \")\n",
    "           .replace(\"\\u2007\", \" \")\n",
    "           .replace(\"\\u2009\", \" \"))\n",
    "\n",
    "    # keep the boundary, drop the label\n",
    "    s = SPEAKER_LABEL_DROP.sub(r\"\\1\", s)\n",
    "\n",
    "    # tidy spacing\n",
    "    s = re.sub(r\"\\s+([,.;:!?…)\\]\\}}])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([(\\[\\{{«“\\\"'])\\s+\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _rstrip_quotes(s): return re.sub(r'[\\s\"\\']+$', '', s or \"\")\n",
    "def _last_char(s): \n",
    "    t = _rstrip_quotes(s or \"\").rstrip()\n",
    "    return t[-1:] if t else \"\"\n",
    "def _first_alpha_case(s):\n",
    "    for ch in (s or \"\"):\n",
    "        if ch.isalpha(): return \"upper\" if ch.isupper() else \"lower\"\n",
    "    return None\n",
    "\n",
    "def _starts_with_dash(s): return bool(re.match(r'^\\s*['+re.escape(DASHES)+r']\\s*', s or \"\"))\n",
    "def _strip_leading_dash(s): return re.sub(r'^\\s*['+re.escape(DASHES)+r']\\s*', '', s or \"\")\n",
    "def _remove_dash_after_punct(s): return re.sub(r'([,\\.!\\?])\\s*['+re.escape(DASHES)+r']\\s*', r'\\1 ', s or \"\")\n",
    "\n",
    "def _normalize_spaces(s):\n",
    "    s = (s or \"\").replace(\"\\u00A0\", \" \")\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\s+([,.;:?!…])', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "def _is_all_caps_alpha(s):\n",
    "    letters = [c for c in (s or \"\") if c.isalpha()]\n",
    "    return bool(letters) and all(c.isupper() for c in letters)\n",
    "\n",
    "def _sentence_case_from_lower(s):\n",
    "    t, out, cap = (s or \"\").lower(), [], True\n",
    "    for ch in t:\n",
    "        if cap and ch.isalpha(): out.append(ch.upper()); cap=False\n",
    "        else: out.append(ch)\n",
    "        if ch in HARD_STOPS: cap=True\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _capitalize_first_alpha(s):\n",
    "    if not s: return s\n",
    "    chars=list(s)\n",
    "    for i,ch in enumerate(chars):\n",
    "        if ch.isalpha(): chars[i]=ch.upper(); break\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def _normalize_line_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = _strip_leading_dash(s)\n",
    "    s = _drop_speaker_labels_keep_content(s)\n",
    "    s = _remove_dash_after_punct(s)\n",
    "    s = _normalize_spaces(s)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_edge_quotes_unbalanced(s)\n",
    "    if _is_all_caps_alpha(s):\n",
    "        s = _sentence_case_from_lower(s)\n",
    "    return s\n",
    "\n",
    "def _join_text(a,b):\n",
    "    b2 = _strip_leading_dash(b).lstrip()\n",
    "    if not a: return b2\n",
    "    if not b2: return a\n",
    "    return (a.rstrip() + \" \" + b2).strip()\n",
    "\n",
    "def _has_inner_hard_stop(s):  # two sentences in one row\n",
    "    return bool(re.search(r'[.?!…].+\\S.*[.?!…]', s or \"\"))\n",
    "\n",
    "def _should_merge_pair(br_a, br_b, pt_a, pt_b):\n",
    "    a_end_br = _last_char(br_a); a_end_pt = _last_char(pt_a)\n",
    "    b_head_br = _first_alpha_case(br_b); b_head_pt = _first_alpha_case(pt_b)\n",
    "    hard_br = a_end_br in HARD_STOPS; hard_pt = a_end_pt in HARD_STOPS\n",
    "\n",
    "    cont_br = ((a_end_br in SOFT_TAILS) or (len(br_a) < 40)) and (b_head_br==\"lower\" or _starts_with_dash(br_b))\n",
    "    cont_pt = ((a_end_pt in SOFT_TAILS) or (len(pt_a) < 40)) and (b_head_pt==\"lower\" or _starts_with_dash(pt_b))\n",
    "\n",
    "    underseg = (_has_inner_hard_stop(pt_a) and not _has_inner_hard_stop(br_a)) or \\\n",
    "               (_has_inner_hard_stop(br_a) and not _has_inner_hard_stop(pt_a))\n",
    "\n",
    "    if hard_br and hard_pt and (b_head_br==\"upper\") and (b_head_pt==\"upper\") and not underseg:\n",
    "        return False\n",
    "    return bool(cont_br or cont_pt or underseg)\n",
    "\n",
    "\n",
    "\n",
    "# def plan_ops_over_corpus(block_size=50_000, reset=False):\n",
    "#     with duckdb.connect(str(DB)) as con:\n",
    "#         lo, hi = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "#         lo, hi = int(lo), int(hi)\n",
    "\n",
    "#         # reset this run (only when you want a fresh start)\n",
    "#         if reset:\n",
    "#             con.execute(\"DELETE FROM opus_ops_update\")\n",
    "#             con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "#             con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "#             con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "#         # resume point\n",
    "#         done = int(con.execute(\"SELECT done_through FROM opus_ops_progress\").fetchone()[0])\n",
    "#         cur  = max(lo, done + 1)\n",
    "\n",
    "#         carry = None\n",
    "#         last_br, last_pt = None, None\n",
    "\n",
    "#         while cur <= hi:\n",
    "#             win = min(block_size, hi - cur + 1)\n",
    "#             df = con.execute(\"\"\"\n",
    "#                 SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#                 FROM opus_moses\n",
    "#                 WHERE line_no BETWEEN ? AND ?\n",
    "#                 ORDER BY line_no\n",
    "#             \"\"\", [cur, cur+win-1]).df()\n",
    "\n",
    "#             rows = df.to_dict(\"records\")\n",
    "#             if carry is not None:\n",
    "#                 rows = [carry] + rows\n",
    "#                 carry = None\n",
    "\n",
    "#             updates, deletes = [], []\n",
    "#             i, n = 0, len(rows)\n",
    "#             while i < n:\n",
    "#                 base = rows[i]; i += 1\n",
    "#                 br = _normalize_line_text(base[\"sent_pt_br\"])\n",
    "#                 pt = _normalize_line_text(base[\"sent_pt_pt\"])\n",
    "#                 group_lines = [int(base[\"line_no\"])]\n",
    "\n",
    "#                 while i < n:\n",
    "#                     nxt = rows[i]\n",
    "#                     br2 = _normalize_line_text(nxt[\"sent_pt_br\"])\n",
    "#                     pt2 = _normalize_line_text(nxt[\"sent_pt_pt\"])\n",
    "#                     if _should_merge_pair(br, br2, pt, pt2):\n",
    "#                         br = _join_text(br, br2)\n",
    "#                         pt = _join_text(pt, pt2)\n",
    "#                         group_lines.append(int(nxt[\"line_no\"]))\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         break\n",
    "\n",
    "#                 if i >= n:\n",
    "#                     carry = {\"line_no\": group_lines[0], \"pair_id\": int(base[\"pair_id\"]),\n",
    "#                              \"sent_pt_br\": br, \"sent_pt_pt\": pt}\n",
    "#                     break\n",
    "\n",
    "#                 if (br.strip() == \"\") or (pt.strip() == \"\"):\n",
    "#                     for ln in group_lines:\n",
    "#                         deletes.append({\"line_no\": ln})\n",
    "#                     # don't emit an update for the head; just skip ahead\n",
    "#                     last_br, last_pt = None, None\n",
    "#                     continue\n",
    "\n",
    "#                 if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "#                     br = _capitalize_first_alpha(br)\n",
    "#                 if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "#                     pt = _capitalize_first_alpha(pt)\n",
    "\n",
    "#                 head = group_lines[0]\n",
    "#                 if br != base[\"sent_pt_br\"] or pt != base[\"sent_pt_pt\"] or len(group_lines) > 1:\n",
    "#                     updates.append({\"line_no\": head, \"sent_pt_br\": br, \"sent_pt_pt\": pt})\n",
    "#                 for ln in group_lines[1:]:\n",
    "#                     deletes.append({\"line_no\": ln})\n",
    "\n",
    "#                 last_br, last_pt = br, pt\n",
    "\n",
    "#             if updates:\n",
    "#                 con.register(\"upd\", pd.DataFrame(updates))\n",
    "#                 con.execute(\"\"\"\n",
    "#                     INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "#                     SELECT line_no, sent_pt_br, sent_pt_pt FROM upd\n",
    "#                     ON CONFLICT(line_no) DO UPDATE SET\n",
    "#                         sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "#                         sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "#                 \"\"\")\n",
    "\n",
    "#                 con.unregister(\"upd\")\n",
    "#             if deletes:\n",
    "#                 con.register(\"del\", pd.DataFrame(deletes))\n",
    "#                 con.execute(\"\"\"\n",
    "#                     INSERT INTO opus_ops_delete (line_no)\n",
    "#                     SELECT DISTINCT line_no FROM del\n",
    "#                     ON CONFLICT(line_no) DO NOTHING\n",
    "#                 \"\"\")\n",
    "\n",
    "#                 con.unregister(\"del\")\n",
    "\n",
    "#             del df, rows, updates, deletes\n",
    "#             gc.collect()\n",
    "\n",
    "#             # advance + persist resume point\n",
    "#             cur += win\n",
    "#             con.execute(\"UPDATE opus_ops_progress SET done_through = ?\", [cur - 1])\n",
    "\n",
    "#         # flush final carry (on the same connection)\n",
    "#         if carry is not None:\n",
    "#             if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "#                 carry[\"sent_pt_br\"] = _capitalize_first_alpha(carry[\"sent_pt_br\"])\n",
    "#             if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "#                 carry[\"sent_pt_pt\"] = _capitalize_first_alpha(carry[\"sent_pt_pt\"])\n",
    "#             con.register(\"tail_upd\", pd.DataFrame([{\n",
    "#                 \"line_no\": int(carry[\"line_no\"]),\n",
    "#                 \"sent_pt_br\": carry[\"sent_pt_br\"],\n",
    "#                 \"sent_pt_pt\": carry[\"sent_pt_pt\"],\n",
    "#             }]))\n",
    "#             con.execute(\"\"\"\n",
    "#                 INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "#                 SELECT line_no, sent_pt_br, sent_pt_pt FROM tail_upd\n",
    "#                 ON CONFLICT(line_no) DO UPDATE SET\n",
    "#                     sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "#                     sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "#             \"\"\")\n",
    "#             con.unregister(\"tail_upd\")\n",
    "\n",
    "\n",
    "# # def apply_ops_to_opus_moses():\n",
    "# #     with duckdb.connect(str(DB)) as con:\n",
    "# #         # sanity: no overlap between updates and deletes\n",
    "# #         overlap = con.execute(\"\"\"\n",
    "# #             SELECT COUNT(*) FROM opus_ops_update u\n",
    "# #             INNER JOIN opus_ops_delete d USING (line_no)\n",
    "# #         \"\"\").fetchone()[0]\n",
    "# #         if overlap:\n",
    "# #             raise RuntimeError(f\"{overlap} lines in BOTH update & delete; fix plan_ops first.\")\n",
    "\n",
    "# #         con.execute(\"BEGIN\")\n",
    "\n",
    "# #         # 1) delete merged-away tails FIRST (avoids transient duplicates)\n",
    "# #         con.execute(\"\"\"\n",
    "# #             DELETE FROM opus_moses\n",
    "# #             WHERE line_no IN (SELECT line_no FROM opus_ops_delete)\n",
    "# #         \"\"\")\n",
    "\n",
    "# #         # 2) then update heads with their merged/cleaned text\n",
    "# #         con.execute(\"\"\"\n",
    "# #             UPDATE opus_moses AS o\n",
    "# #             SET sent_pt_br = u.sent_pt_br,\n",
    "# #                 sent_pt_pt = u.sent_pt_pt\n",
    "# #             FROM opus_ops_update AS u\n",
    "# #             WHERE o.line_no = u.line_no\n",
    "# #         \"\"\")\n",
    "\n",
    "# #         con.execute(\"COMMIT\")\n",
    "# #         con.execute(\"CHECKPOINT\")\n",
    "\n",
    "# def apply_ops_ctas_swap(force_checkpoint=True):\n",
    "#     with duckdb.connect(str(DB)) as con:\n",
    "#         # If a previous tx is half-open on this connection, close it\n",
    "#         try:\n",
    "#             con.execute(\"ROLLBACK\")\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         # updates win over deletes\n",
    "#         con.execute(\"\"\"\n",
    "#             DELETE FROM opus_ops_delete\n",
    "#             WHERE line_no IN (SELECT line_no FROM opus_ops_update)\n",
    "#         \"\"\")\n",
    "\n",
    "#         con.execute(\"BEGIN\")\n",
    "#         try:\n",
    "#             con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "#             con.execute(\"\"\"\n",
    "#                 CREATE TABLE opus_moses_new AS\n",
    "#                 SELECT\n",
    "#                     o.line_no,\n",
    "#                     o.pair_id,\n",
    "#                     COALESCE(u.sent_pt_br, o.sent_pt_br) AS sent_pt_br,\n",
    "#                     COALESCE(u.sent_pt_pt, o.sent_pt_pt) AS sent_pt_pt\n",
    "#                 FROM opus_moses o\n",
    "#                 LEFT JOIN opus_ops_update u USING (line_no)\n",
    "#                 WHERE o.line_no NOT IN (SELECT line_no FROM opus_ops_delete)\n",
    "#                 ORDER BY o.line_no\n",
    "#             \"\"\")\n",
    "\n",
    "#             con.execute(\"DROP TABLE opus_moses\")\n",
    "#             con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "#             con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "#             con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "#             con.execute(\"COMMIT\")\n",
    "#         except:\n",
    "#             con.execute(\"ROLLBACK\")\n",
    "#             raise\n",
    "\n",
    "#         if force_checkpoint:\n",
    "#             # waits for other write transactions to finish\n",
    "#             con.execute(\"FORCE CHECKPOINT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan_ops_over_corpus(block_size=50_000, reset=True)\n",
    "# apply_ops_ctas_swap()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7cdf8",
   "metadata": {},
   "source": [
    "**//AFTER PREPROCESSING, RUN THE ALIGNER THROUGH THE WHOLE CORPUS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1da5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, gc\n",
    "\n",
    "def _run_moves_df(df: pd.DataFrame, margin=0.04, max_clause_chars=60, max_iters=5) -> pd.DataFrame:\n",
    "    \"\"\"Repeat apply_neighbor_moves on a DataFrame until no more moves or max_iters.\"\"\"\n",
    "    cur = df.loc[:, [\"line_no\",\"pair_id\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "    for _ in range(int(max_iters)):\n",
    "        nxt, log = apply_neighbor_moves(cur, margin=margin, max_clause_chars=max_clause_chars)\n",
    "        if log is None or log.empty:\n",
    "            break\n",
    "        cur = nxt\n",
    "    return cur\n",
    "\n",
    "def apply_neighbor_moves_corpus_inplace(\n",
    "    block_size: int = 50_000,\n",
    "    overlap: int = 3,                 # rows kept between blocks so moves can cross the seam\n",
    "    margin: float = 0.04,\n",
    "    max_clause_chars: int = 60,\n",
    "    max_iters: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream over opus_moses and apply your neighbor-move heuristic in-place.\n",
    "    - Processes in blocks with 'overlap' rows carried forward.\n",
    "    - Only updates rows that actually changed.\n",
    "    - No row-count changes (this pass only moves clauses).\n",
    "    \"\"\"\n",
    "    with duckdb.connect(str(DB)) as con:\n",
    "        lo, hi = con.execute(\"SELECT min(line_no), max(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        cur_start = lo\n",
    "        carry_df = None  # last 'overlap' rows of the previous processed block (already moved)\n",
    "\n",
    "        while cur_start <= hi:\n",
    "            # choose fetch start/count so we include the carried rows\n",
    "            if carry_df is None:\n",
    "                fetch_start = cur_start\n",
    "                fetch_count = min(block_size, hi - fetch_start + 1)\n",
    "            else:\n",
    "                fetch_start = int(carry_df[\"line_no\"].iloc[0])\n",
    "                fetch_count = min(block_size + overlap, hi - fetch_start + 1)\n",
    "\n",
    "            # load from DB\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [fetch_start, fetch_start + fetch_count - 1]).df()\n",
    "\n",
    "            # overlay carried texts onto the front (so we start from the already-moved boundary)\n",
    "            if carry_df is not None and not carry_df.empty:\n",
    "                df = df.merge(carry_df[[\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]],\n",
    "                              on=\"line_no\", how=\"left\", suffixes=(\"\",\"_car\"))\n",
    "                for col in (\"sent_pt_br\",\"sent_pt_pt\"):\n",
    "                    rep = df[col + \"_car\"]\n",
    "                    df[col] = rep.where(rep.notna(), df[col])\n",
    "                    df.drop(columns=[col + \"_car\"], inplace=True)\n",
    "\n",
    "            # keep a copy for diffing\n",
    "            orig = df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]].copy()\n",
    "\n",
    "            # run your move heuristic on this combined block\n",
    "            moved = _run_moves_df(df, margin=margin, max_clause_chars=max_clause_chars, max_iters=max_iters)\n",
    "\n",
    "            # decide how many rows to flush now (keep the last 'overlap' rows for the next block)\n",
    "            is_last_block = (fetch_start + len(df) - 1) >= hi\n",
    "            flush_n = len(moved) if is_last_block else max(0, len(moved) - overlap)\n",
    "\n",
    "            if flush_n:\n",
    "                out = moved.iloc[:flush_n]\n",
    "                base = orig.iloc[:flush_n]\n",
    "\n",
    "                # diffs → only update changed rows\n",
    "                changed = (out[\"sent_pt_br\"] != base[\"sent_pt_br\"]) | (out[\"sent_pt_pt\"] != base[\"sent_pt_pt\"])\n",
    "                upd = out.loc[changed, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]]\n",
    "\n",
    "                if not upd.empty:\n",
    "                    con.register(\"upd\", upd)\n",
    "                    con.execute(\"\"\"\n",
    "                        UPDATE opus_moses AS o\n",
    "                        SET sent_pt_br = u.sent_pt_br,\n",
    "                            sent_pt_pt = u.sent_pt_pt\n",
    "                        FROM upd AS u\n",
    "                        WHERE o.line_no = u.line_no\n",
    "                    \"\"\")\n",
    "                    con.unregister(\"upd\")\n",
    "\n",
    "                # next fetch should begin right after the last flushed line\n",
    "                cur_start = int(out[\"line_no\"].iloc[-1]) + 1\n",
    "            else:\n",
    "                # nothing flushed (tiny last block)\n",
    "                cur_start = fetch_start + len(df)\n",
    "\n",
    "            # carry the tail (overlap) forward (already moved)\n",
    "            carry_df = moved.iloc[flush_n:].copy()\n",
    "\n",
    "            # tidy memory\n",
    "            del df, orig, moved\n",
    "            gc.collect()\n",
    "\n",
    "        # flush any leftover carried rows (end of file)\n",
    "        if carry_df is not None and not carry_df.empty:\n",
    "            con.register(\"upd_tail\", carry_df.loc[:, [\"line_no\",\"sent_pt_br\",\"sent_pt_pt\"]])\n",
    "            con.execute(\"\"\"\n",
    "                UPDATE opus_moses AS o\n",
    "                SET sent_pt_br = u.sent_pt_br,\n",
    "                    sent_pt_pt = u.sent_pt_pt\n",
    "                FROM upd_tail AS u\n",
    "                WHERE o.line_no = u.line_no\n",
    "            \"\"\")\n",
    "            con.unregister(\"upd_tail\")\n",
    "\n",
    "        # reclaim disk space\n",
    "        con.execute(\"CHECKPOINT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a4cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_neighbor_moves_corpus_inplace(\n",
    "#     block_size=50_000,   # tune for your RAM\n",
    "#     overlap=3,           # 2–3 is plenty for neighbor moves\n",
    "#     margin=0.04,\n",
    "#     max_clause_chars=60,\n",
    "#     max_iters=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc9c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# B) REPEATED-PREFIX CLEANER\n",
    "# ==============================\n",
    "def _split_sents(s: str) -> List[str]:\n",
    "    s = (s or \"\").strip()\n",
    "    return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "def _strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def _norm_for_match(s: str) -> str:\n",
    "    s = _strip_accents(s.lower())\n",
    "    s = re.sub(r\"[^\\w]+\", \" \", s, flags=re.UNICODE)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _tokens(s: str) -> List[str]:\n",
    "    return _norm_for_match(s).split()\n",
    "\n",
    "def _jaccard(a: set, b: set) -> float:\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def _looks_like_sentence_start(s: str) -> bool:\n",
    "    t = (s or \"\").lstrip()\n",
    "    while t and t[0] in \"«“\\\"([{'’”»\": t = t[1:].lstrip()\n",
    "    return (not t) or t[0].isupper()\n",
    "\n",
    "def _adjacent_dedup(sents: List[str], jacc=0.96) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for s in sents:\n",
    "        if out:\n",
    "            a = set(_tokens(out[-1])); b = set(_tokens(s))\n",
    "            if _jaccard(a, b) >= jacc:\n",
    "                continue\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "def dedup_repeated_prefix_block(\n",
    "    br_prev: str, pt_prev: str,\n",
    "    br_here: str, pt_here: str,\n",
    "    *,\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True\n",
    ") -> Tuple[str, str, bool, int]:\n",
    "    \"\"\"\n",
    "    Trim from the START of (br_here, pt_here) the longest sentence-aligned prefix\n",
    "    whose tokens are largely contained in the TAIL of (br_prev, pt_prev).\n",
    "    Returns: (br_trimmed, pt_trimmed, applied, n_sentences_removed)\n",
    "    \"\"\"\n",
    "    def _count_to_remove(prev: str, nxt: str) -> int:\n",
    "        prev_s = _split_sents(prev); nxt_s = _split_sents(nxt)\n",
    "        if not prev_s or not nxt_s: return 0\n",
    "        tail = \" \".join(prev_s[-prev_window:]) if prev_window > 0 else \" \".join(prev_s)\n",
    "        tail_tok = set(_tokens(tail))\n",
    "        best_k = 0\n",
    "        for k in range(1, len(nxt_s) + 1):\n",
    "            pref = \" \".join(nxt_s[:k])\n",
    "            toks = _tokens(pref)\n",
    "            if len(toks) < min_prefix_tokens: continue\n",
    "            cov = len(set(toks) & tail_tok) / max(1, len(set(toks)))\n",
    "            if cov >= coverage_thresh: best_k = k\n",
    "        return best_k\n",
    "\n",
    "    k_br = _count_to_remove(br_prev, br_here)\n",
    "    k_pt = _count_to_remove(pt_prev, pt_here)\n",
    "    k = min(k_br, k_pt) if require_both else max(k_br, k_pt)\n",
    "    if k <= 0: return br_here, pt_here, False, 0\n",
    "\n",
    "    br_s = _split_sents(br_here)[k:]; pt_s = _split_sents(pt_here)[k:]\n",
    "    if collapse_adjacent_dups:\n",
    "        br_s = _adjacent_dedup(br_s); pt_s = _adjacent_dedup(pt_s)\n",
    "\n",
    "    br_out = \" \".join(br_s).strip(); pt_out = \" \".join(pt_s).strip()\n",
    "    if br_out and not _looks_like_sentence_start(br_out): return br_here, pt_here, False, 0\n",
    "    if pt_out and not _looks_like_sentence_start(pt_out): return br_here, pt_here, False, 0\n",
    "    return br_out, pt_out, True, k\n",
    "\n",
    "def run_repeated_prefix_cleaner_chunked(\n",
    "    *,\n",
    "    db_path=DB,\n",
    "    table: str = \"opus_moses\",\n",
    "    order_col: str = \"line_no\",\n",
    "    text_br_col: str = \"sent_pt_br\",\n",
    "    text_pt_col: str = \"sent_pt_pt\",\n",
    "    id_pair_col: str = \"pair_id\",\n",
    "    # knobs (forwarded)\n",
    "    prev_window: int = 6,\n",
    "    min_prefix_tokens: int = 6,\n",
    "    coverage_thresh: float = 0.92,\n",
    "    require_both: bool = False,\n",
    "    collapse_adjacent_dups: bool = True,\n",
    "    # deletion policy\n",
    "    delete_on_trigger: bool = True,\n",
    "    delete_if_empty_only: bool = False,\n",
    "    # execution\n",
    "    chunk_size: int = 50_000,\n",
    "    start_line: Optional[int] = None,\n",
    "    end_line: Optional[int] = None,\n",
    "    apply_changes: bool = False,\n",
    "    print_updates: bool = False,\n",
    "    trace_lines: Optional[Iterable[int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Walk rows; if dedup applies (either language unless require_both=True):\n",
    "      - delete whole row (default), or\n",
    "      - update with trimmed text.\n",
    "    Prints deleted (line_no, pair_id). Processes in chunks.\n",
    "    \"\"\"\n",
    "    assert not (delete_on_trigger and delete_if_empty_only), \\\n",
    "        \"Choose delete_on_trigger=True OR delete_if_empty_only=True (not both).\"\n",
    "\n",
    "    where = []; args = []\n",
    "    if start_line is not None: where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "    if end_line   is not None: where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "    WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        mn, mx = con.execute(\n",
    "            f\"SELECT min({order_col}), max({order_col}) FROM {table} {WHERE}\", args\n",
    "        ).fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"No rows match selection.\"); return\n",
    "\n",
    "        prev_br, prev_pt = \"\", \"\"\n",
    "        cur = int(mn)\n",
    "        while cur <= int(mx):\n",
    "            hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "            df = con.execute(f\"\"\"\n",
    "                SELECT {order_col} AS line_no,\n",
    "                       {id_pair_col} AS pair_id,\n",
    "                       {text_br_col} AS br,\n",
    "                       {text_pt_col} AS pt\n",
    "                FROM {table}\n",
    "                WHERE {order_col} BETWEEN ? AND ?\n",
    "                ORDER BY {order_col}\n",
    "            \"\"\", [cur, hi]).df()\n",
    "\n",
    "            updates = []; deletes = []\n",
    "            deleted_ids_print = []; updated_ids_print = []\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                line_no = int(df.line_no.iloc[i])\n",
    "                pair_id = int(df.pair_id.iloc[i]) if \"pair_id\" in df.columns else None\n",
    "                br_here = df.br.iloc[i] or \"\"; pt_here = df.pt.iloc[i] or \"\"\n",
    "\n",
    "                br_new, pt_new, applied, k = dedup_repeated_prefix_block(\n",
    "                    prev_br, prev_pt, br_here, pt_here,\n",
    "                    prev_window=prev_window,\n",
    "                    min_prefix_tokens=min_prefix_tokens,\n",
    "                    coverage_thresh=coverage_thresh,\n",
    "                    require_both=require_both,\n",
    "                    collapse_adjacent_dups=collapse_adjacent_dups\n",
    "                )\n",
    "\n",
    "                if trace_lines and (line_no in set(trace_lines)):\n",
    "                    print(f\"[trace {line_no}] applied={applied} k={k}\")\n",
    "\n",
    "                will_delete = False\n",
    "                if applied:\n",
    "                    if delete_on_trigger:\n",
    "                        will_delete = True\n",
    "                    elif delete_if_empty_only and (not br_new.strip() and not pt_new.strip()):\n",
    "                        will_delete = True\n",
    "\n",
    "                if will_delete:\n",
    "                    deletes.append((line_no,))\n",
    "                    deleted_ids_print.append((line_no, pair_id))\n",
    "                    # don't advance prev_* on deletion (use last kept row)\n",
    "                else:\n",
    "                    if applied and (br_new != br_here or pt_new != pt_here):\n",
    "                        updates.append((br_new, pt_new, line_no))\n",
    "                        if print_updates: updated_ids_print.append((line_no, pair_id))\n",
    "                        prev_br, prev_pt = br_new, pt_new\n",
    "                    else:\n",
    "                        prev_br, prev_pt = br_here, pt_here\n",
    "\n",
    "            if apply_changes and (updates or deletes):\n",
    "                con.execute(\"BEGIN TRANSACTION\")\n",
    "                if updates:\n",
    "                    con.executemany(\n",
    "                        f\"UPDATE {table} SET {text_br_col} = ?, {text_pt_col} = ? WHERE {order_col} = ?\",\n",
    "                        updates\n",
    "                    )\n",
    "                if deletes:\n",
    "                    con.executemany(\n",
    "                        f\"DELETE FROM {table} WHERE {order_col} = ?\",\n",
    "                        deletes\n",
    "                    )\n",
    "                con.execute(\"COMMIT\")\n",
    "\n",
    "            print(f\"[{cur}..{hi}] updates={len(updates)} deletes={len(deletes)}\")\n",
    "            if deleted_ids_print:\n",
    "                print(\"  Deleted rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(deleted_ids_print), 1000):\n",
    "                    block = deleted_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "            if print_updates and updated_ids_print:\n",
    "                print(\"  Updated rows (line_no, pair_id):\")\n",
    "                for j in range(0, len(updated_ids_print), 1000):\n",
    "                    block = updated_ids_print[j:j+1000]\n",
    "                    print(\"   \", \", \".join(f\"({ln},{pid})\" for ln,pid in block))\n",
    "\n",
    "            cur = hi + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77219d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) repeated-prefix dedup only\n",
    "# run_repeated_prefix_cleaner_chunked(\n",
    "#     db_path=DB,\n",
    "#     prev_window=6,\n",
    "#     min_prefix_tokens=6,\n",
    "#     coverage_thresh=0.92,\n",
    "#     require_both=False,             # either side may trigger\n",
    "#     collapse_adjacent_dups=True,\n",
    "#     delete_on_trigger=True,         # delete whenever dedup applies\n",
    "#     delete_if_empty_only=False,\n",
    "#     chunk_size=50_000,\n",
    "#     start_line=None, end_line=None,\n",
    "#     apply_changes=True,            # DRY RUN\n",
    "#     print_updates=False,\n",
    "#     trace_lines= {18}               # e.g., {18} to debug that row\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe66a3",
   "metadata": {},
   "source": [
    "**//SIMALIGN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4670bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================\n",
    "# # PURE SimAlign LINKS — Filter & Preview (self-contained)\n",
    "# # ==============================\n",
    "# # What this provides:\n",
    "# #  - SimAlign setup (XLM-R, word-level)\n",
    "# #  - Feature extractor using ONLY raw SimAlign word links (optionally with prev/here/next window)\n",
    "# #  - Flag policy (threshold-based; tuneable)\n",
    "# #  - Preview helpers (numbers-only + [[interior]] / <edge> highlights)\n",
    "# #  - No trimming/mutation logic included\n",
    "# # ==============================\n",
    "\n",
    "# # ---------- tokenization / basics ----------\n",
    "# def tokenize(s: str) -> list[str]:\n",
    "#     return WORD.findall(s or \"\")\n",
    "\n",
    "# def ali_tokenize(s: str) -> list[str]:\n",
    "#     \"\"\"Tokens fed to SimAlign (word-level). Keep consistent with ali_char_spans().\"\"\"\n",
    "#     return tokenize(s)\n",
    "\n",
    "# def ali_char_spans(text: str) -> list[tuple[int, int]]:\n",
    "#     \"\"\"Char spans aligned with ali_tokenize().\"\"\"\n",
    "#     return [m.span() for m in WORD.finditer(text or \"\")]\n",
    "\n",
    "# def token_overlap(a: str, b: str) -> float:\n",
    "#     A = set(w.lower() for w in WORD.findall(a or \"\"))\n",
    "#     B = set(w.lower() for w in WORD.findall(b or \"\"))\n",
    "#     return (len(A & B) / len(A | B)) if (A and B) else 0.0\n",
    "\n",
    "\n",
    "# def sim2(a: str, b: str, *, method: str = ALIGN_METHOD,\n",
    "#         smooth_small_gaps: int = 1, content_only: bool = False) -> float:\n",
    "#     \"\"\"\n",
    "#     Similarity = average coverage of aligned tokens on both sides.\n",
    "#     - method: 'itermax' | 'mwmf' | 'inter' (you set ALIGN_METHOD outside)\n",
    "#     - smooth_small_gaps: fill 1-token pinholes if >0\n",
    "#     - content_only: measure coverage over content tokens only (ignores stopwords)\n",
    "#     \"\"\"\n",
    "#     a = (a or \"\").strip()\n",
    "#     b = (b or \"\").strip()\n",
    "#     if not a and not b:\n",
    "#         return 1.0\n",
    "#     if not a or not b:\n",
    "#         return 0.0\n",
    "\n",
    "#     L = ali_tokenize(a)\n",
    "#     R = ali_tokenize(b)\n",
    "#     if not L and not R:\n",
    "#         return 1.0\n",
    "#     if not L or not R:\n",
    "#         return 0.0\n",
    "\n",
    "#     # raw SimAlign links\n",
    "#     out = aligner.get_word_aligns(L, R)\n",
    "#     pairs = out.get(method, out.get(\"inter\", []))  # fall back if needed\n",
    "\n",
    "#     covered_L = {i for i, _ in pairs}\n",
    "#     covered_R = {j for _, j in pairs}\n",
    "\n",
    "#     covL = len(covered_L) / max(1, len(L))\n",
    "#     covR = len(covered_R) / max(1, len(R))\n",
    "\n",
    "#     return 0.5 * (covL + covR)\n",
    "\n",
    "# import unicodedata as ud\n",
    "\n",
    "# def _form_norm(t: str) -> str:\n",
    "#     # Unicode-aware: normalize + casefold\n",
    "#     return ud.normalize(\"NFC\", t or \"\").casefold()\n",
    "\n",
    "\n",
    "# def new_sim(br: str, pt: str, method: str = ALIGN_METHOD) -> float:\n",
    "#     L = ali_tokenize(br or \"\"); R = ali_tokenize(pt or \"\")\n",
    "#     if not L and not R: return 1.0\n",
    "#     if not L or not R:  return 0.0\n",
    "\n",
    "#     Ln = [_form_norm(t) for t in L]\n",
    "#     Rn = [_form_norm(t) for t in R]\n",
    "#     union = set(Ln) | set(Rn)\n",
    "#     if not union: return 1.0\n",
    "\n",
    "#     pairs = _raw_pairs(L, R, method=method)\n",
    "\n",
    "#     aligned_types = set()\n",
    "#     for i, j in pairs:\n",
    "#         if 0 <= i < len(Ln): aligned_types.add(Ln[i])   # <-- use normalized\n",
    "#         if 0 <= j < len(Rn): aligned_types.add(Rn[j])   # <-- use normalized\n",
    "\n",
    "#     return len(aligned_types) / len(union)\n",
    "\n",
    "# def _type_sim_from_pairs(L_tokens, R_tokens, pairs):\n",
    "#     Ln = [_form_norm(t) for t in L_tokens]\n",
    "#     Rn = [_form_norm(t) for t in R_tokens]\n",
    "#     union = set(Ln) | set(Rn)\n",
    "#     if not union: return 1.0\n",
    "#     aligned = set()\n",
    "#     for i, j in pairs:\n",
    "#         if 0 <= i < len(Ln): aligned.add(Ln[i])\n",
    "#         if 0 <= j < len(Rn): aligned.add(Rn[j])\n",
    "#     return len(aligned) / len(union)\n",
    "\n",
    "\n",
    "# def is_content_token(tok: str) -> bool:\n",
    "#     t = (tok or \"\").lower()\n",
    "#     return (t not in PT_STOPWORDS) and (len(t) > 1)\n",
    "\n",
    "\n",
    "# def _split_sents(s: str) -> list[str]:\n",
    "#     s = (s or \"\").strip()\n",
    "#     return [m.group(0).strip() for m in _SENT_RE.finditer(s)]\n",
    "\n",
    "# # ---------- SimAlign setup ----------\n",
    "# aligner = SentenceAligner(model=\"xlmr\", token_type=\"word\", matching_methods=\"a\")\n",
    "\n",
    "# # ---------- raw pairs + utilities ----------\n",
    "# def _raw_pairs(l_tokens, r_tokens, method=ALIGN_METHOD):\n",
    "#     out = aligner.get_word_aligns(l_tokens, r_tokens)  # keys: \"inter\", \"itermax\", \"mwmf\"\n",
    "#     if method == \"itermax\":\n",
    "#         return out[\"itermax\"]\n",
    "#     elif method == \"union\":  # inter ∪ itermax (often a sweet spot)\n",
    "#         return list({*out[\"inter\"], *out[\"itermax\"]})\n",
    "#     else:  # \"inter\" or \"mwmf\"\n",
    "#         return out[method]\n",
    "\n",
    "# def spans_from_uncovered(tokens: list[str], covered_idx: set[int]) -> list[tuple[int,int]]:\n",
    "#     spans, cur = [], []\n",
    "#     for i in range(len(tokens)):\n",
    "#         if i not in covered_idx:\n",
    "#             cur.append(i)\n",
    "#         elif cur:\n",
    "#             spans.append((cur[0], cur[-1])); cur = []\n",
    "#     if cur:\n",
    "#         spans.append((cur[0], cur[-1]))\n",
    "#     return spans\n",
    "\n",
    "# def _smooth_small_gaps(covered: set[int], n_tokens: int, max_gap: int = 1) -> set[int]:\n",
    "#     \"\"\"Fill tiny uncovered holes (≤ max_gap) surrounded by covered tokens (function-word pinholes).\"\"\"\n",
    "#     C = set(covered)\n",
    "#     i = 0\n",
    "#     while i < n_tokens:\n",
    "#         if i not in C:\n",
    "#             j = i\n",
    "#             while j < n_tokens and j not in C:\n",
    "#                 j += 1\n",
    "#             gap = j - i\n",
    "#             if 0 < gap <= max_gap and i > 0 and j < n_tokens:\n",
    "#                 for k in range(i, j):\n",
    "#                     C.add(k)\n",
    "#             i = j\n",
    "#         else:\n",
    "#             i += 1\n",
    "#     return C\n",
    "\n",
    "# def _interior_uncovered(tokens: list[str], covered: set[int]) -> list[tuple[int,int]]:\n",
    "#     \"\"\"Uncovered runs strictly inside (not touching edges).\"\"\"\n",
    "#     runs = spans_from_uncovered(tokens, covered)\n",
    "#     n = len(tokens)\n",
    "#     return [(i0, i1) for (i0, i1) in runs if i0 > 0 and i1 < n - 1]\n",
    "\n",
    "# def _content_count(tokens: list[str]) -> int:\n",
    "#     return sum(1 for t in tokens if is_content_token(t))\n",
    "\n",
    "# # ---------- highlighting ----------\n",
    "# def _split_edge_vs_interior(runs: list[tuple[int,int]], n_tokens: int):\n",
    "#     interior, edges = [], []\n",
    "#     for i0, i1 in runs:\n",
    "#         if i0 > 0 and i1 < n_tokens - 1:\n",
    "#             interior.append((i0, i1))\n",
    "#         else:\n",
    "#             edges.append((i0, i1))\n",
    "#     return interior, edges\n",
    "\n",
    "# def _to_char_spans(token_runs: list[tuple[int,int]], token_char: list[tuple[int,int]]):\n",
    "#     char_runs = []\n",
    "#     for i0, i1 in token_runs:\n",
    "#         if not token_char:\n",
    "#             continue\n",
    "#         i0 = max(0, min(i0, len(token_char) - 1))\n",
    "#         i1 = max(0, min(i1, len(token_char) - 1))\n",
    "#         L = token_char[i0][0]; R = token_char[i1][1]\n",
    "#         char_runs.append((L, R))\n",
    "#     # merge\n",
    "#     char_runs.sort()\n",
    "#     merged = []\n",
    "#     for L, R in char_runs:\n",
    "#         if not merged or L > merged[-1][1]:\n",
    "#             merged.append([L, R])\n",
    "#         else:\n",
    "#             merged[-1][1] = max(merged[-1][1], R)\n",
    "#     return [(L, R) for L, R in merged]\n",
    "\n",
    "# def _apply_highlights(text: str,\n",
    "#                       interior_char: list[tuple[int,int]],\n",
    "#                       edge_char: list[tuple[int,int]],\n",
    "#                       marks=(\"[[\", \"]]\"), edge_marks=(\"<\", \">\")) -> str:\n",
    "#     \"\"\"Insert [[...]] (interior) and <...> (edge) highlights without breaking indices.\"\"\"\n",
    "#     tags = []\n",
    "#     for L, R in interior_char:\n",
    "#         tags.append((L, \"open_i\")); tags.append((R, \"close_i\"))\n",
    "#     for L, R in edge_char:\n",
    "#         tags.append((L, \"open_e\")); tags.append((R, \"close_e\"))\n",
    "#     tags.sort(key=lambda x: (x[0], x[1].startswith(\"close\")))  # close before open at same pos\n",
    "\n",
    "#     out, last = [], 0\n",
    "#     stack = []\n",
    "#     for pos, kind in tags:\n",
    "#         pos = max(0, min(pos, len(text)))\n",
    "#         if pos > last:\n",
    "#             out.append(text[last:pos])\n",
    "#             last = pos\n",
    "#         if kind == \"open_i\":\n",
    "#             out.append(marks[0]); stack.append(\"i\")\n",
    "#         elif kind == \"close_i\":\n",
    "#             if stack and stack[-1] == \"i\":\n",
    "#                 stack.pop()\n",
    "#                 out.append(marks[1])\n",
    "#         elif kind == \"open_e\":\n",
    "#             out.append(edge_marks[0]); stack.append(\"e\")\n",
    "#         elif kind == \"close_e\":\n",
    "#             if stack and stack[-1] == \"e\":\n",
    "#                 stack.pop()\n",
    "#                 out.append(edge_marks[1])\n",
    "#     if last < len(text):\n",
    "#         out.append(text[last:])\n",
    "#     return \"\".join(out)\n",
    "\n",
    "# def _alignment_uncovered_highlights(\n",
    "#     left_text: str, right_prev: str, right_here: str, right_next: str,\n",
    "#     *, use_window: bool\n",
    "# ) -> tuple[str, float, float]:\n",
    "#     \"\"\"\n",
    "#     Highlight uncovered tokens on the left_text using raw SimAlign links.\n",
    "#     Returns (highlighted_text, coverage_ratio, interior_content_ratio).\n",
    "#     \"\"\"\n",
    "#     left_toks = ali_tokenize(left_text)\n",
    "#     if use_window:\n",
    "#         right_win = ali_tokenize(\" \".join(x for x in [right_prev, right_here, right_next] if x))\n",
    "#         pairs = _raw_pairs(left_toks, right_win)\n",
    "#     else:\n",
    "#         pairs = _raw_pairs(left_toks, ali_tokenize(right_here))\n",
    "\n",
    "#     covered = {i for i, _ in pairs}\n",
    "#     covered = _smooth_small_gaps(covered, len(left_toks), max_gap=1)\n",
    "\n",
    "#     n = len(left_toks)\n",
    "#     cov = len(covered) / max(1, n)\n",
    "\n",
    "#     runs_all = spans_from_uncovered(left_toks, covered)\n",
    "#     interior_runs, edge_runs = _split_edge_vs_interior(runs_all, n)\n",
    "\n",
    "#     # interior content ratio\n",
    "#     total_content = sum(1 for t in left_toks if is_content_token(t))\n",
    "#     interior_content = sum(\n",
    "#         1 for i0, i1 in interior_runs for t in left_toks[i0:i1+1] if is_content_token(t)\n",
    "#     )\n",
    "#     interior_content_ratio = interior_content / max(1, total_content)\n",
    "\n",
    "#     token_chars = ali_char_spans(left_text)\n",
    "#     interior_char = _to_char_spans(interior_runs, token_chars)\n",
    "#     edge_char     = _to_char_spans(edge_runs, token_chars)\n",
    "#     hi = _apply_highlights(left_text, interior_char, edge_char)\n",
    "#     return hi, cov, interior_content_ratio\n",
    "\n",
    "# # ---------- feature extractor (pure SimAlign) ----------\n",
    "# def alignment_quality_features(\n",
    "#     br_prev: str, br_here: str, br_next: str,\n",
    "#     pt_prev: str, pt_here: str, pt_next: str,\n",
    "#     *, use_window: bool = True, sim_fn=None\n",
    "# ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Compute alignment metrics using ONLY raw SimAlign word links (+ optional prev/next window).\n",
    "#     \"\"\"\n",
    "#     if sim_fn is None:\n",
    "#         sim_fn = new_sim\n",
    "\n",
    "#     br_toks = ali_tokenize(br_here)\n",
    "#     pt_toks = ali_tokenize(pt_here)\n",
    "#     br_len   = len(br_toks)\n",
    "#     pt_len   = len(pt_toks)\n",
    "\n",
    "#     if len(br_toks) == 10 or len(pt_toks) == 10:\n",
    "#         pass\n",
    "\n",
    "#     if use_window:\n",
    "#         pt_win = ali_tokenize(\" \".join(x for x in [pt_prev, pt_here, pt_next] if x))\n",
    "#         br_pairs = _raw_pairs(br_toks, pt_win)\n",
    "#         br_cov = {i for i, _ in br_pairs}\n",
    "\n",
    "#         br_win = ali_tokenize(\" \".join(x for x in [br_prev, br_here, br_next] if x))\n",
    "#         pt_pairs = _raw_pairs(pt_toks, br_win)\n",
    "#         pt_cov = {i for i, _ in pt_pairs}  # left indices of PT tokens\n",
    "#     else:\n",
    "#         pairs = _raw_pairs(br_toks, pt_toks)\n",
    "#         br_cov = {i for i, _ in pairs}\n",
    "#         pt_cov = {j for _, j in pairs}     # approximate right coverage\n",
    "\n",
    "#     # smooth 1-token pinholes\n",
    "#     br_cov = _smooth_small_gaps(br_cov, len(br_toks), max_gap=1)\n",
    "#     pt_cov = _smooth_small_gaps(pt_cov, len(pt_toks), max_gap=1)\n",
    "\n",
    "#     br_cov_ratio = len(br_cov) / max(1, len(br_toks))\n",
    "#     pt_cov_ratio = len(pt_cov) / max(1, len(pt_toks))\n",
    "#     cov_min = min(br_cov_ratio, pt_cov_ratio)\n",
    "#     cov_gap = abs(br_cov_ratio - pt_cov_ratio)\n",
    "\n",
    "#     # interior uncovered runs + content ratio\n",
    "#     br_int_spans = _interior_uncovered(br_toks, br_cov)\n",
    "#     pt_int_spans = _interior_uncovered(pt_toks, pt_cov)\n",
    "\n",
    "#     def _content_in_runs(tokens, runs):\n",
    "#         return sum(1 for i0, i1 in runs for t in tokens[i0:i1+1] if is_content_token(t))\n",
    "\n",
    "#     br_content_total = _content_count(br_toks)\n",
    "#     pt_content_total = _content_count(pt_toks)\n",
    "#     br_int_content = _content_in_runs(br_toks, br_int_spans)\n",
    "#     pt_int_content = _content_in_runs(pt_toks, pt_int_spans)\n",
    "\n",
    "#     br_int_content_ratio = br_int_content / max(1, br_content_total)\n",
    "#     pt_int_content_ratio = pt_int_content / max(1, pt_content_total)\n",
    "\n",
    "#     br_max_int = max((j - i + 1) for i, j in br_int_spans) if br_int_spans else 0\n",
    "#     pt_max_int = max((j - i + 1) for i, j in pt_int_spans) if pt_int_spans else 0\n",
    "\n",
    "#     # “spillover” vs extra-info (same-language neighbors)\n",
    "#     def _span_text(tokens, sp): i0, i1 = sp; return \" \".join(tokens[i0:i1+1])\n",
    "#     br_int_text = \" \".join(_span_text(br_toks, sp) for sp in br_int_spans)\n",
    "#     pt_int_text = \" \".join(_span_text(pt_toks, sp) for sp in pt_int_spans)\n",
    "#     br_spill = max(token_overlap(br_int_text, br_prev), token_overlap(br_int_text, br_next)) if br_int_text else 0.0\n",
    "#     pt_spill = max(token_overlap(pt_int_text, pt_prev), token_overlap(pt_int_text, pt_next)) if pt_int_text else 0.0\n",
    "\n",
    "#     sent_diff = abs(len(_split_sents(br_here)) - len(_split_sents(pt_here)))\n",
    "#     if use_window:\n",
    "#         # we already computed br_pairs (BR vs PT window)\n",
    "#         base_sim = _type_sim_from_pairs(br_toks, pt_toks, br_pairs)\n",
    "#     else:\n",
    "#         pairs = _raw_pairs(br_toks, pt_toks)\n",
    "#         base_sim = _type_sim_from_pairs(br_toks, pt_toks, pairs)\n",
    "\n",
    "#     return {\n",
    "#         \"br_cov\": br_cov_ratio, \"pt_cov\": pt_cov_ratio,\n",
    "#         \"cov_min\": cov_min, \"cov_gap\": cov_gap,\n",
    "#         \"br_int_content_ratio\": br_int_content_ratio,\n",
    "#         \"pt_int_content_ratio\": pt_int_content_ratio,\n",
    "#         \"br_max_int\": br_max_int, \"pt_max_int\": pt_max_int,\n",
    "#         \"br_spill\": br_spill, \"pt_spill\": pt_spill,\n",
    "#         \"sent_diff\": sent_diff,\n",
    "#         \"base_sim\": float(base_sim),\n",
    "#         \"br_content_total\": br_content_total,\n",
    "#         \"pt_content_total\": pt_content_total,\n",
    "#         \"br_len\": br_len,\n",
    "#         \"pt_len\": pt_len\n",
    "#     }\n",
    "\n",
    "# # ---------- flag policy (tune to taste) ----------\n",
    "# def alignment_quality_flag(\n",
    "#     feats: dict,\n",
    "#     *,\n",
    "#     min_cov_ok: float = 0.50,\n",
    "#     max_cov_gap: float = 0.35,\n",
    "#     max_int_ratio: float = 0.33,\n",
    "#     max_max_int: int = 9,\n",
    "#     max_sent_diff: int = 1,\n",
    "#     min_sim_ok: float = 0.30,\n",
    "#     spill_tolerance: float = 0.60,\n",
    "#     min_row_content: int = 7,\n",
    "#     min_interior_content_for_flag: int = 3,\n",
    "#     hard_low_sim: float = 0.20,   # NEW: hard gate — flag if new_sim is this low\n",
    "# ) -> tuple[bool, str]:\n",
    "#     \"\"\"\n",
    "#     Decide whether to activate the filter using SimAlign features.\n",
    "\n",
    "#     Changes:\n",
    "#     - Adds a hard similarity gate: if base_sim < hard_low_sim, flag immediately.\n",
    "#     - Keeps the very-short logic: tiny pairs pass only when coverage looks sane; else flagged.\n",
    "#     - No 'too_short' shortcut anywhere.\n",
    "#     \"\"\"\n",
    "#     spillish = (feats[\"br_spill\"] >= spill_tolerance) or (feats[\"pt_spill\"] >= spill_tolerance)\n",
    "\n",
    "#     br_ct = feats[\"br_content_total\"]\n",
    "#     pt_ct = feats[\"pt_content_total\"]\n",
    "#     shortish = (br_ct < min_row_content) or (pt_ct < min_row_content)\n",
    "#     very_short = (br_ct <= 3) and (pt_ct <= 3)\n",
    "\n",
    "#     # ---- HARD gate: extremely low token-type similarity means it's off-topic → flag\n",
    "#     if feats[\"base_sim\"] < hard_low_sim:\n",
    "#         return True, \"low_similarity\"\n",
    "\n",
    "#     # ---- very short handling ----\n",
    "#     if very_short:\n",
    "#         # let tiny function-word variants through if coverage looks sane\n",
    "#         if (feats[\"cov_min\"] >= (2/3) and\n",
    "#             feats[\"br_max_int\"] <= 1 and feats[\"pt_max_int\"] <= 1 and\n",
    "#             feats[\"sent_diff\"] <= 0 and not spillish):\n",
    "#             return False, \"ok_short_high_cov\"\n",
    "#         # otherwise be stricter on coverage asymmetry & ignore interior ratio\n",
    "#         min_cov_ok  = 0.40\n",
    "#         max_cov_gap = max_cov_gap + 0.20\n",
    "#         max_int_ratio = 1.00\n",
    "\n",
    "#     # gentle nudge for other short lines (not very short)\n",
    "#     if shortish and not very_short:\n",
    "#         min_cov_ok  = max(min_cov_ok, 0.55)      # was 0.55\n",
    "#         max_cov_gap = min(max_cov_gap, 0.20)     # was +0.10 (relaxed) → now stricter\n",
    "#         max_int_ratio = max(0.0, max_int_ratio - 0.10)\n",
    "#         min_sim_ok  = max(min_sim_ok, 0.60)      # was 0.50\n",
    "\n",
    "\n",
    "#     # quick pass if clearly good under (possibly adjusted) thresholds\n",
    "#     if feats[\"base_sim\"] >= (min_sim_ok + 0.35) and feats[\"cov_min\"] >= (min_cov_ok + 0.10):\n",
    "#         return False, \"ok\"\n",
    "\n",
    "#     reasons = []\n",
    "#     if feats[\"cov_min\"] < min_cov_ok:\n",
    "#         reasons.append(\"low_coverage\")\n",
    "#     if feats[\"cov_gap\"] > max_cov_gap:\n",
    "#         reasons.append(\"coverage_asymmetry\")\n",
    "#     if feats[\"br_int_content_ratio\"] > max_int_ratio or feats[\"pt_int_content_ratio\"] > max_int_ratio:\n",
    "#         reasons.append(\"big_interior_unaligned_content\")\n",
    "#     if feats[\"br_max_int\"] >= max_max_int or feats[\"pt_max_int\"] >= max_max_int:\n",
    "#         reasons.append(\"long_interior_gap\")\n",
    "#     if feats[\"sent_diff\"] > max_sent_diff:\n",
    "#         reasons.append(\"sentence_mismatch\")\n",
    "#     if feats[\"base_sim\"] < min_sim_ok:\n",
    "#         reasons.append(\"low_similarity\")\n",
    "\n",
    "#     # need some actual interior content if we accuse \"content\" reasons\n",
    "#     if {\"big_interior_unaligned_content\",\"long_interior_gap\"} & set(reasons):\n",
    "#         enough_interior = (feats[\"br_int_content_ratio\"]*br_ct >= min_interior_content_for_flag) or \\\n",
    "#                           (feats[\"pt_int_content_ratio\"]*pt_ct >= min_interior_content_for_flag)\n",
    "#         if not enough_interior:\n",
    "#             reasons = [r for r in reasons if r not in {\"big_interior_unaligned_content\",\"long_interior_gap\"}]\n",
    "\n",
    "#     strong = {\"low_coverage\",\"coverage_asymmetry\",\"big_interior_unaligned_content\",\"long_interior_gap\"}\n",
    "#     strong_hits = len([r for r in reasons if r in strong])\n",
    "\n",
    "#     activate = False\n",
    "#     if strong_hits >= 2:\n",
    "#         activate = True\n",
    "#     elif strong_hits >= 1 and not spillish:\n",
    "#         activate = True\n",
    "#     elif len(reasons) >= 3 and not spillish:\n",
    "#         activate = True\n",
    "\n",
    "#     return bool(activate), (\",\".join(reasons) if reasons else \"ok\")\n",
    "\n",
    "# # ---------- previews ----------\n",
    "# def preview_alignment_quality_window(\n",
    "#     start_line: int,\n",
    "#     window: int = 40,\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     use_window: bool = True,\n",
    "#     thresholds: dict | None = None,\n",
    "#     sim_fn=None\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Numbers-only preview (no highlights). No DB writes.\"\"\"\n",
    "#     if thresholds is None:\n",
    "#         thresholds = {}\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         df = con.execute(\"\"\"\n",
    "#             SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#             FROM opus_moses\n",
    "#             WHERE line_no BETWEEN ? AND ?\n",
    "#             ORDER BY line_no\n",
    "#         \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "#     rows = []\n",
    "#     n = len(df)\n",
    "#     for i in range(n):\n",
    "#         br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "#         br_here = df.sent_pt_br.iloc[i]\n",
    "#         br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "#         pt_here = df.sent_pt_pt.iloc[i]\n",
    "#         pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         feats = alignment_quality_features(\n",
    "#             br_prev, br_here, br_next,\n",
    "#             pt_prev, pt_here, pt_next,\n",
    "#             use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "#         )\n",
    "#         activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "\n",
    "#         rows.append({\n",
    "#             \"line_no\": int(df.line_no.iloc[i]),\n",
    "#             \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#             \"activate_filter\": bool(activate),\n",
    "#             \"reason\": reason,\n",
    "#             **feats,\n",
    "#         })\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "# def preview_alignment_quality_window_with_highlights_complex(\n",
    "#     start_line: int,\n",
    "#     window: int = 40,\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     use_window: bool = True,\n",
    "#     thresholds: dict | None = None,\n",
    "#     show_when: str = \"flagged\",   # \"flagged\" | \"all\"\n",
    "#     sim_fn=None\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Preview with [[INTERIOR]] and <EDGE> highlights using pure SimAlign links.\"\"\"\n",
    "#     if thresholds is None:\n",
    "#         thresholds = dict(\n",
    "#             min_cov_ok=0.50,\n",
    "#             max_cov_gap=0.35,\n",
    "#             max_int_ratio=0.33,\n",
    "#             max_max_int=9,\n",
    "#             max_sent_diff=1,\n",
    "#             min_sim_ok=0.30,\n",
    "#             spill_tolerance=0.60\n",
    "#         )\n",
    "\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         df = con.execute(\"\"\"\n",
    "#             SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#             FROM opus_moses\n",
    "#             WHERE line_no BETWEEN ? AND ?\n",
    "#             ORDER BY line_no\n",
    "#         \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "#     rows = []\n",
    "#     n = len(df)\n",
    "#     for i in range(n):\n",
    "#         br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "#         br_here = df.sent_pt_br.iloc[i]\n",
    "#         br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "#         pt_here = df.sent_pt_pt.iloc[i]\n",
    "#         pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         feats = alignment_quality_features(\n",
    "#             br_prev, br_here, br_next,\n",
    "#             pt_prev, pt_here, pt_next,\n",
    "#             use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "#         )\n",
    "#         activate, reason = alignment_quality_flag(feats, **thresholds)\n",
    "#         if show_when == \"flagged\" and not activate:\n",
    "#             continue\n",
    "\n",
    "#         br_hi, _, _ = _alignment_uncovered_highlights(\n",
    "#             br_here, pt_prev, pt_here, pt_next, use_window=use_window\n",
    "#         )\n",
    "#         pt_hi, _, _ = _alignment_uncovered_highlights(\n",
    "#             pt_here, br_prev, br_here, br_next, use_window=use_window\n",
    "#         )\n",
    "\n",
    "#         rows.append({\n",
    "#             \"line_no\": int(df.line_no.iloc[i]),\n",
    "#             \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#             \"activate_filter\": bool(activate),\n",
    "#             \"reason\": reason,\n",
    "#             \"base_sim\": feats[\"base_sim\"],\n",
    "#             \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "#             \"cov_gap\": feats[\"cov_gap\"],\n",
    "#             \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "#             \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "#             \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "#             \"br_highlight\": br_hi,   # [[INTERIOR]] and <EDGE>\n",
    "#             \"pt_highlight\": pt_hi,\n",
    "#         })\n",
    "#     return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c58838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ali_tokenize(s: str) -> list[str]:\n",
    "#     return [m.group(0) for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "# def ali_char_spans(s: str) -> list[tuple[int,int]]:\n",
    "#     return [m.span() for m in WS_TOKEN.finditer(s or \"\")]\n",
    "\n",
    "# def _raw_pairs(left_tokens: list[str], right_tokens: list[str], method: str = ALIGN_METHOD):\n",
    "#     \"\"\"Call SimAlign safely; fall back on truncation and available methods.\"\"\"\n",
    "#     if not left_tokens or not right_tokens:\n",
    "#         return []\n",
    "#     try:\n",
    "#         out = aligner.get_word_aligns(left_tokens, right_tokens)\n",
    "#     except Exception:\n",
    "#         lt, rt = left_tokens[:300], right_tokens[:300]  # rare long-line guard\n",
    "#         try:\n",
    "#             out = aligner.get_word_aligns(lt, rt)\n",
    "#         except Exception:\n",
    "#             return []\n",
    "#     if method in out:\n",
    "#         return out[method]\n",
    "#     for m in (\"itermax\",\"mwmf\",\"inter\"):\n",
    "#         if m in out:\n",
    "#             return out[m]\n",
    "#     return []\n",
    "\n",
    "\n",
    "# # -------- reservoir sampling (keeps a bounded, uniform-ish sample) -------\n",
    "# def _reservoir_add(reservoir: list, item: dict, cap: int, seen_counter: int):\n",
    "#     if cap <= 0:\n",
    "#         return seen_counter + 1\n",
    "#     if len(reservoir) < cap:\n",
    "#         reservoir.append(item)\n",
    "#     else:\n",
    "#         j = random.randint(0, seen_counter)\n",
    "#         if j < cap:\n",
    "#             reservoir[j] = item\n",
    "#     return seen_counter + 1\n",
    "\n",
    "\n",
    "# # -------- main audit: fixed thresholds, no adaptive policy ----------------\n",
    "# def audit_alignment_filter_chunked_fixed(\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     table: str = \"opus_moses\",\n",
    "#     order_col: str = \"line_no\",\n",
    "#     id_col: str = \"pair_id\",\n",
    "#     br_col: str = \"sent_pt_br\",\n",
    "#     pt_col: str = \"sent_pt_pt\",\n",
    "#     start_line: Optional[int] = None,\n",
    "#     end_line:   Optional[int] = None,\n",
    "#     chunk_size: int = 50_000,\n",
    "#     # filter policy (FIXED per run)\n",
    "#     thresholds: Dict[str, Any] = dict(\n",
    "#         min_cov_ok=0.50,\n",
    "#         max_cov_gap=0.35,\n",
    "#         max_int_ratio=0.33,\n",
    "#         max_max_int=9,\n",
    "#         max_sent_diff=1,\n",
    "#         min_sim_ok=0.30,\n",
    "#         spill_tolerance=0.60\n",
    "#     ),\n",
    "#     use_window: bool = True,\n",
    "#     sim_fn=None,\n",
    "#     # how many examples to keep in memory\n",
    "#     max_store_flagged: int = 1000,\n",
    "#     max_store_passed: int  = 1000,\n",
    "#     seed: int = 13,\n",
    "# ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Iterate in chunks, compute alignment_quality_features + alignment_quality_flag\n",
    "#     with FIXED thresholds. Print per-chunk stats.\n",
    "#     Returns (flagged_df, passed_df, summary_df). No highlights, no DB writes.\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         # build range\n",
    "#         where, args = [], []\n",
    "#         if start_line is not None:\n",
    "#             where.append(f\"{order_col} >= ?\"); args.append(int(start_line))\n",
    "#         if end_line is not None:\n",
    "#             where.append(f\"{order_col} <= ?\"); args.append(int(end_line))\n",
    "#         WHERE = (\"WHERE \" + \" AND \".join(where)) if where else \"\"\n",
    "\n",
    "#         mn, mx = con.execute(\n",
    "#             f\"SELECT MIN({order_col}), MAX({order_col}) FROM {table} {WHERE}\", args\n",
    "#         ).fetchone()\n",
    "#         if mn is None or mx is None:\n",
    "#             print(\"No rows to audit.\")\n",
    "#             return (pd.DataFrame(), pd.DataFrame(), pd.DataFrame())\n",
    "\n",
    "#         total_rows = 0\n",
    "#         total_flagged = 0\n",
    "#         summaries = []\n",
    "\n",
    "#         # sample reservoirs\n",
    "#         flagged_res, passed_res = [], []\n",
    "#         seen_flagged = seen_passed = 0\n",
    "\n",
    "#         cur = int(mn)\n",
    "#         while cur <= int(mx):\n",
    "#             hi = min(cur + int(chunk_size) - 1, int(mx))\n",
    "#             df = con.execute(f\"\"\"\n",
    "#                 SELECT {order_col} AS line_no, {id_col} AS pair_id,\n",
    "#                        {br_col} AS br, {pt_col} AS pt\n",
    "#                 FROM {table}\n",
    "#                 WHERE {order_col} BETWEEN ? AND ?\n",
    "#                 ORDER BY {order_col}\n",
    "#             \"\"\", [cur, hi]).df()\n",
    "\n",
    "#             if df.empty:\n",
    "#                 cur = hi + 1\n",
    "#                 continue\n",
    "\n",
    "#             chunk_rows = len(df)\n",
    "#             chunk_flagged = 0\n",
    "\n",
    "#             for i in range(chunk_rows):\n",
    "#                 br_prev = df.br.iloc[i-1] if i > 0 else \"\"\n",
    "#                 br_here = df.br.iloc[i]\n",
    "#                 br_next = df.br.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "#                 pt_prev = df.pt.iloc[i-1] if i > 0 else \"\"\n",
    "#                 pt_here = df.pt.iloc[i]\n",
    "#                 pt_next = df.pt.iloc[i+1] if i+1 < chunk_rows else \"\"\n",
    "\n",
    "#                 feats = alignment_quality_features(\n",
    "#                     br_prev, br_here, br_next,\n",
    "#                     pt_prev, pt_here, pt_next,\n",
    "#                     use_window=use_window, sim_fn=sim_fn or sim\n",
    "#                 )\n",
    "\n",
    "#                 # alignment_quality_flag may return (activate, reason) OR (activate, reason, flags)\n",
    "#                 res = alignment_quality_flag(feats, **thresholds)\n",
    "#                 if isinstance(res, tuple) and len(res) == 3:\n",
    "#                     activate, reason_str, flags = res\n",
    "#                 else:\n",
    "#                     activate, reason_str = res\n",
    "#                     flags = {}\n",
    "\n",
    "#                 row_info = {\n",
    "#                     \"line_no\": int(df.line_no.iloc[i]),\n",
    "#                     \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#                     \"activate_filter\": bool(activate),\n",
    "#                     \"reason\": reason_str,\n",
    "#                     \"base_sim\": feats[\"base_sim\"],\n",
    "#                     \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "#                     \"cov_gap\": feats[\"cov_gap\"],\n",
    "#                     \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "#                     \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "#                     \"br_max_int\": feats[\"br_max_int\"], \"pt_max_int\": feats[\"pt_max_int\"],\n",
    "#                     \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "#                     # include sentence mismatch if available\n",
    "#                     \"sent_diff\": feats.get(\"sent_diff\", None),\n",
    "#                 }\n",
    "#                 # keep reason flags if your policy returns them\n",
    "#                 row_info.update({k: v for k, v in flags.items()})\n",
    "\n",
    "#                 if activate:\n",
    "#                     chunk_flagged += 1\n",
    "#                     seen_flagged = _reservoir_add(flagged_res, row_info, max_store_flagged, seen_flagged)\n",
    "#                 else:\n",
    "#                     seen_passed = _reservoir_add(passed_res, row_info, max_store_passed, seen_passed)\n",
    "\n",
    "#             total_rows    += chunk_rows\n",
    "#             total_flagged += chunk_flagged\n",
    "#             frac = (chunk_flagged / chunk_rows) if chunk_rows else 0.0\n",
    "#             print(f\"[{cur}..{hi}] rows={chunk_rows} flagged={chunk_flagged} ({frac:.1%})\")\n",
    "\n",
    "#             summaries.append({\n",
    "#                 \"chunk_start\": cur, \"chunk_end\": hi,\n",
    "#                 \"rows\": chunk_rows, \"flagged\": chunk_flagged, \"ratio\": frac\n",
    "#             })\n",
    "\n",
    "#             cur = hi + 1\n",
    "\n",
    "#         overall = (total_flagged / total_rows) if total_rows else 0.0\n",
    "#         print(f\"\\nTOTAL rows={total_rows} flagged={total_flagged} ({overall:.1%})\")\n",
    "\n",
    "#         flagged_df = pd.DataFrame(flagged_res)\n",
    "#         passed_df  = pd.DataFrame(passed_res)\n",
    "#         summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "#         return flagged_df, passed_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27286c3e",
   "metadata": {},
   "source": [
    "**//SIMPLE FILTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24584d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def alignment_similarity_only_flag(\n",
    "#     feats: dict,\n",
    "#     *,\n",
    "#     # pass a constant to disable adaptation entirely\n",
    "#     min_sim_ok: float | None = None,\n",
    "\n",
    "#     # length adapter config\n",
    "#     length_mode: str = \"total\",    # \"total\" | \"content\" | \"char\"\n",
    "#     base_min_sim: float = 0.30,    # short sentences\n",
    "#     long_min_sim: float = 0.70,    # long sentences (raise as you like)\n",
    "#     short_len: int = 8,            # <= this → base_min_sim\n",
    "#     long_len: int = 28,            # >= this → long_min_sim\n",
    "#     **kwargs,                       # ignore extras\n",
    "# ) -> tuple[bool, str]:\n",
    "#     \"\"\"\n",
    "#     Similarity-only flag with a length adapter.\n",
    "#     Threshold grows from base_min_sim to long_min_sim as length increases.\n",
    "#     length_mode:\n",
    "#       - \"total\": use total token count (recommended)\n",
    "#       - \"content\": use content-token count (previous behavior)\n",
    "#       - \"char\": use character count (scaled by 5 chars ≈ 1 token)\n",
    "#     \"\"\"\n",
    "#     sim = float(feats[\"base_sim\"])\n",
    "\n",
    "#     if min_sim_ok is not None:\n",
    "#         thr = float(min_sim_ok)\n",
    "#     else:\n",
    "#         if length_mode == \"total\":\n",
    "#             L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "#         elif length_mode == \"char\":\n",
    "#             # rough token equivalent from chars to reuse short_len/long_len in \"token units\"\n",
    "#             L = int(max(int(feats.get(\"br_chars\", 0)), int(feats.get(\"pt_chars\", 0))) / 5)\n",
    "#         else:  # \"content\"\n",
    "#             L = max(int(feats.get(\"br_content_total\", 0)), int(feats.get(\"pt_content_total\", 0)))\n",
    "\n",
    "#         if L <= short_len:\n",
    "#             thr = base_min_sim\n",
    "#         elif L >= long_len:\n",
    "#             thr = long_min_sim\n",
    "#         else:\n",
    "#             t = (L - short_len) / max(1, (long_len - short_len))\n",
    "#             thr = base_min_sim + t * (long_min_sim - base_min_sim)\n",
    "\n",
    "#     activate = sim < thr\n",
    "#     return activate, (f\"low_similarity@simple(thr={thr:.2f},L={L})\" if activate else \"ok@simple\")\n",
    "\n",
    "\n",
    "\n",
    "# def preview_alignment_quality_window(\n",
    "#     start_line: int,\n",
    "#     window: int = 40,\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     use_window: bool = True,\n",
    "#     thresholds: dict | None = None,\n",
    "#     sim_fn=None,\n",
    "#     flag_fn=None,                     # NEW: choose which flag function to use\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Numbers-only preview (no highlights). No DB writes.\"\"\"\n",
    "#     if thresholds is None:\n",
    "#         thresholds = {}\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         df = con.execute(\"\"\"\n",
    "#             SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#             FROM opus_moses\n",
    "#             WHERE line_no BETWEEN ? AND ?\n",
    "#             ORDER BY line_no\n",
    "#         \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "#     rows = []\n",
    "#     n = len(df)\n",
    "#     for i in range(n):\n",
    "#         br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "#         br_here = df.sent_pt_br.iloc[i]\n",
    "#         br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "#         pt_here = df.sent_pt_pt.iloc[i]\n",
    "#         pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         feats = alignment_quality_features(\n",
    "#             br_prev, br_here, br_next,\n",
    "#             pt_prev, pt_here, pt_next,\n",
    "#             use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "#         )\n",
    "\n",
    "#         # Use the provided flag function; default to the multi-feature flag\n",
    "#         use_flag = flag_fn or alignment_quality_flag\n",
    "#         activate, reason = use_flag(feats, **(thresholds or {}))\n",
    "\n",
    "#         rows.append({\n",
    "#             \"line_no\": int(df.line_no.iloc[i]),\n",
    "#             \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#             \"activate_filter\": bool(activate),\n",
    "#             \"reason\": reason,\n",
    "#             **feats,\n",
    "#         })\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# def preview_alignment_quality_window_with_highlights_simple(\n",
    "#     start_line: int,\n",
    "#     window: int = 40,\n",
    "#     *,\n",
    "#     db_path=DB,\n",
    "#     use_window: bool = True,\n",
    "#     thresholds: dict | None = None,\n",
    "#     show_when: str = \"flagged\",   # \"flagged\" | \"all\"\n",
    "#     sim_fn=None,\n",
    "#     flag_fn=None,                  # NEW: choose which flag function to use\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Preview with [[INTERIOR]] and <EDGE> highlights using pure SimAlign links.\"\"\"\n",
    "#     if thresholds is None:\n",
    "#         thresholds = dict(\n",
    "#             min_cov_ok=0.50,\n",
    "#             max_cov_gap=0.35,\n",
    "#             max_int_ratio=0.33,\n",
    "#             max_max_int=9,\n",
    "#             max_sent_diff=1,\n",
    "#             min_sim_ok=0.30,\n",
    "#             spill_tolerance=0.60\n",
    "#         )\n",
    "\n",
    "#     with duckdb.connect(str(db_path)) as con:\n",
    "#         df = con.execute(\"\"\"\n",
    "#             SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "#             FROM opus_moses\n",
    "#             WHERE line_no BETWEEN ? AND ?\n",
    "#             ORDER BY line_no\n",
    "#         \"\"\", [int(start_line), int(start_line + window - 1)]).df()\n",
    "\n",
    "#     rows = []\n",
    "#     n = len(df)\n",
    "#     for i in range(n):\n",
    "#         br_prev = df.sent_pt_br.iloc[i-1] if i > 0 else \"\"\n",
    "#         br_here = df.sent_pt_br.iloc[i]\n",
    "#         br_next = df.sent_pt_br.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         pt_prev = df.sent_pt_pt.iloc[i-1] if i > 0 else \"\"\n",
    "#         pt_here = df.sent_pt_pt.iloc[i]\n",
    "#         pt_next = df.sent_pt_pt.iloc[i+1] if i+1 < n else \"\"\n",
    "\n",
    "#         feats = alignment_quality_features(\n",
    "#             br_prev, br_here, br_next,\n",
    "#             pt_prev, pt_here, pt_next,\n",
    "#             use_window=use_window, sim_fn=sim_fn or new_sim\n",
    "#         )\n",
    "\n",
    "#         # Use the provided flag function; default to the multi-feature flag\n",
    "#         use_flag = flag_fn or alignment_similarity_only_flag\n",
    "#         activate, reason = use_flag(feats, **(thresholds or {}))\n",
    "#         if show_when == \"flagged\" and not activate:\n",
    "#             continue\n",
    "\n",
    "#         br_hi, _, _ = _alignment_uncovered_highlights(\n",
    "#             br_here, pt_prev, pt_here, pt_next, use_window=use_window\n",
    "#         )\n",
    "#         pt_hi, _, _ = _alignment_uncovered_highlights(\n",
    "#             pt_here, br_prev, br_here, br_next, use_window=use_window\n",
    "#         )\n",
    "\n",
    "#         rows.append({\n",
    "#             \"line_no\": int(df.line_no.iloc[i]),\n",
    "#             \"pair_id\": int(df.pair_id.iloc[i]),\n",
    "#             \"activate_filter\": bool(activate),\n",
    "#             \"reason\": reason,\n",
    "#             \"base_sim\": feats[\"base_sim\"],\n",
    "#             \"br_cov\": feats[\"br_cov\"], \"pt_cov\": feats[\"pt_cov\"],\n",
    "#             \"cov_gap\": feats[\"cov_gap\"],\n",
    "#             \"br_int_content_ratio\": feats[\"br_int_content_ratio\"],\n",
    "#             \"pt_int_content_ratio\": feats[\"pt_int_content_ratio\"],\n",
    "#             \"spillish\": max(feats[\"br_spill\"], feats[\"pt_spill\"]),\n",
    "#             \"br_highlight\": br_hi,   # [[INTERIOR]] and <EDGE>\n",
    "#             \"pt_highlight\": pt_hi,\n",
    "#         })\n",
    "#     return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54b5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hq = preview_alignment_quality_window_with_highlights_simple(\n",
    "#     start_line=79799, window=30,\n",
    "#     use_window=False,           # align against prev+here+next window\n",
    "#     show_when=\"all\",\n",
    "#     flag_fn=alignment_similarity_only_flag,\n",
    "#     thresholds=dict(\n",
    "#         length_mode=\"total\", \n",
    "#         base_min_sim=0.35,\n",
    "#         long_min_sim=0.75,\n",
    "#         short_len=8,\n",
    "#         long_len=20,\n",
    "#     )\n",
    "#     )\n",
    "\n",
    "# hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae355637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- line_no=18143 ---\n",
      "L tokens: ['Tal', 'operação', 'é', 'muito', 'cara']\n",
      "R tokens: ['Uma', 'operação', 'assim', 'é', 'muito', 'cara']\n",
      "L0 codepoints: ['U+0054', 'U+0061', 'U+006C']\n",
      "R0 codepoints: ['U+0055', 'U+006D', 'U+0061']\n",
      "L0 NFC==R0 NFC? False\n",
      "pairs: [(1, 1), (2, 3), (3, 4), (4, 5)]\n",
      "uncovered L indices: [0]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "def _cp(s):  # code points for sanity\n",
    "    return [f\"U+{ord(c):04X}\" for c in s]\n",
    "\n",
    "def debug_alignment(br, pt, method=\"union\"):\n",
    "    L = ali_tokenize(br or \"\")\n",
    "    R = ali_tokenize(pt or \"\")\n",
    "    print(\"L tokens:\", L)\n",
    "    print(\"R tokens:\", R)\n",
    "    print(\"L0 codepoints:\", _cp(L[0]) if L else [])\n",
    "    print(\"R0 codepoints:\", _cp(R[0]) if R else [])\n",
    "    print(\"L0 NFC==R0 NFC?\", ud.normalize(\"NFC\", L[0] if L else \"\") == ud.normalize(\"NFC\", R[0] if R else \"\"))\n",
    "\n",
    "    pairs = _raw_pairs(L, R, method=method)\n",
    "    print(\"pairs:\", pairs)\n",
    "    covered_L = {i for i,_ in pairs}\n",
    "    print(\"uncovered L indices:\", [i for i in range(len(L)) if i not in covered_L])\n",
    "\n",
    "import duckdb\n",
    "\n",
    "def run_debug_for_line(line_no: int, method=\"union\", db_path=DB):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        row = con.execute(\n",
    "            \"SELECT sent_pt_br, sent_pt_pt FROM opus_moses WHERE line_no = ?\",\n",
    "            [int(line_no)]\n",
    "        ).fetchone()\n",
    "    if not row:\n",
    "        print(f\"line_no {line_no} not found\"); return\n",
    "    br, pt = row\n",
    "    print(f\"\\n--- line_no={line_no} ---\")\n",
    "    debug_alignment(br, pt, method=method)\n",
    "\n",
    "\n",
    "run_debug_for_line(18143, method=\"inter\")   # compare methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cb4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:52:04,547 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "import re, gc, duckdb, pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------------------------\n",
    "# Small utilities / schema\n",
    "# ---------------------------\n",
    "def _as_int_or_none(x):\n",
    "    if x is None: return None\n",
    "    try:\n",
    "        if pd.isna(x): return None\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(str(x).strip())\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _ensure_opus_ops_tables(con: duckdb.DuckDBPyConnection):\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_update(\n",
    "            line_no BIGINT PRIMARY KEY,\n",
    "            sent_pt_br TEXT,\n",
    "            sent_pt_pt TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_delete(\n",
    "            line_no BIGINT PRIMARY KEY\n",
    "        )\n",
    "    \"\"\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_ops_progress(\n",
    "            done_through BIGINT\n",
    "        )\n",
    "    \"\"\")\n",
    "    if con.execute(\"SELECT COUNT(*) FROM opus_ops_progress\").fetchone()[0] == 0:\n",
    "        con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "def _ensure_simple_filter_table(con: duckdb.DuckDBPyConnection):\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS opus_filter_simple(\n",
    "            line_no BIGINT PRIMARY KEY,\n",
    "            pair_id BIGINT,\n",
    "            reason  TEXT,\n",
    "            base_sim DOUBLE,\n",
    "            thr     DOUBLE,\n",
    "            L       INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "# ---------------------------\n",
    "# A) PREPROCESSING\n",
    "# ---------------------------\n",
    "def plan_ops_over_corpus(block_size=50_000, reset=False, *, db_path=None):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        _ensure_opus_ops_tables(con)\n",
    "        lo, hi = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "        lo, hi = int(lo), int(hi)\n",
    "\n",
    "        if reset:\n",
    "            con.execute(\"DELETE FROM opus_ops_update\")\n",
    "            con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "            con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "            con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "\n",
    "        done = int(con.execute(\"SELECT done_through FROM opus_ops_progress\").fetchone()[0])\n",
    "        cur  = max(lo, done + 1)\n",
    "\n",
    "        carry = None\n",
    "        last_br, last_pt = None, None\n",
    "\n",
    "        while cur <= hi:\n",
    "            win = min(block_size, hi - cur + 1)\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [cur, cur+win-1]).df()\n",
    "\n",
    "            rows = df.to_dict(\"records\")\n",
    "            if carry is not None:\n",
    "                rows = [carry] + rows\n",
    "                carry = None\n",
    "\n",
    "            updates, deletes = [], []\n",
    "            i, n = 0, len(rows)\n",
    "            while i < n:\n",
    "                base = rows[i]; i += 1\n",
    "                br = _normalize_line_text(base.get(\"sent_pt_br\"))\n",
    "                pt = _normalize_line_text(base.get(\"sent_pt_pt\"))\n",
    "                group_lines = [int(base[\"line_no\"])]\n",
    "\n",
    "                while i < n:\n",
    "                    nxt = rows[i]\n",
    "                    br2 = _normalize_line_text(nxt.get(\"sent_pt_br\"))\n",
    "                    pt2 = _normalize_line_text(nxt.get(\"sent_pt_pt\"))\n",
    "                    if _should_merge_pair(br, br2, pt, pt2):\n",
    "                        br = _join_text(br, br2)\n",
    "                        pt = _join_text(pt, pt2)\n",
    "                        group_lines.append(int(nxt[\"line_no\"]))\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if i >= n:\n",
    "                    carry = {\n",
    "                        \"line_no\": group_lines[0],\n",
    "                        \"pair_id\": _as_int_or_none(base.get(\"pair_id\")),\n",
    "                        \"sent_pt_br\": br,\n",
    "                        \"sent_pt_pt\": pt\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "                if (br.strip() == \"\") or (pt.strip() == \"\"):\n",
    "                    for ln in group_lines:\n",
    "                        deletes.append({\"line_no\": int(ln)})\n",
    "                    last_br, last_pt = None, None\n",
    "                    continue\n",
    "\n",
    "                if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                    br = _capitalize_first_alpha(br)\n",
    "                if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                    pt = _capitalize_first_alpha(pt)\n",
    "\n",
    "                head = group_lines[0]\n",
    "                if br != base.get(\"sent_pt_br\") or pt != base.get(\"sent_pt_pt\") or len(group_lines) > 1:\n",
    "                    updates.append({\"line_no\": int(head), \"sent_pt_br\": br, \"sent_pt_pt\": pt})\n",
    "                for ln in group_lines[1:]:\n",
    "                    deletes.append({\"line_no\": int(ln)})\n",
    "\n",
    "                last_br, last_pt = br, pt\n",
    "\n",
    "            if updates:\n",
    "                con.register(\"upd\", pd.DataFrame(updates))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                    SELECT line_no, sent_pt_br, sent_pt_pt FROM upd\n",
    "                    ON CONFLICT(line_no) DO UPDATE SET\n",
    "                        sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                        sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "                \"\"\")\n",
    "                con.unregister(\"upd\")\n",
    "            if deletes:\n",
    "                con.register(\"del\", pd.DataFrame(deletes))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_ops_delete (line_no)\n",
    "                    SELECT DISTINCT line_no FROM del\n",
    "                    ON CONFLICT(line_no) DO NOTHING\n",
    "                \"\"\")\n",
    "                con.unregister(\"del\")\n",
    "\n",
    "            del df, rows, updates, deletes\n",
    "            gc.collect()\n",
    "\n",
    "            cur += win\n",
    "            con.execute(\"UPDATE opus_ops_progress SET done_through = ?\", [cur - 1])\n",
    "\n",
    "        if carry is not None:\n",
    "            if last_br and _last_char(last_br) in HARD_STOPS:\n",
    "                carry[\"sent_pt_br\"] = _capitalize_first_alpha(carry[\"sent_pt_br\"])\n",
    "            if last_pt and _last_char(last_pt) in HARD_STOPS:\n",
    "                carry[\"sent_pt_pt\"] = _capitalize_first_alpha(carry[\"sent_pt_pt\"])\n",
    "            con.register(\"tail_upd\", pd.DataFrame([{\n",
    "                \"line_no\": int(carry[\"line_no\"]),\n",
    "                \"sent_pt_br\": carry[\"sent_pt_br\"],\n",
    "                \"sent_pt_pt\": carry[\"sent_pt_pt\"],\n",
    "            }]))\n",
    "            con.execute(\"\"\"\n",
    "                INSERT INTO opus_ops_update (line_no, sent_pt_br, sent_pt_pt)\n",
    "                SELECT line_no, sent_pt_br, sent_pt_pt FROM tail_upd\n",
    "                ON CONFLICT(line_no) DO UPDATE SET\n",
    "                    sent_pt_br = EXCLUDED.sent_pt_br,\n",
    "                    sent_pt_pt = EXCLUDED.sent_pt_pt\n",
    "            \"\"\")\n",
    "            con.unregister(\"tail_upd\")\n",
    "\n",
    "def apply_ops_ctas_swap(*, db_path=None, force_checkpoint=True):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        try: con.execute(\"ROLLBACK\")\n",
    "        except: pass\n",
    "\n",
    "        con.execute(\"\"\"\n",
    "            DELETE FROM opus_ops_delete\n",
    "            WHERE line_no IN (SELECT line_no FROM opus_ops_update)\n",
    "        \"\"\")\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE opus_moses_new AS\n",
    "                SELECT\n",
    "                    o.line_no,\n",
    "                    o.pair_id,\n",
    "                    COALESCE(u.sent_pt_br, o.sent_pt_br) AS sent_pt_br,\n",
    "                    COALESCE(u.sent_pt_pt, o.sent_pt_pt) AS sent_pt_pt\n",
    "                FROM opus_moses o\n",
    "                LEFT JOIN opus_ops_update u USING (line_no)\n",
    "                WHERE o.line_no NOT IN (SELECT line_no FROM opus_ops_delete)\n",
    "                ORDER BY o.line_no\n",
    "            \"\"\")\n",
    "            con.execute(\"DROP TABLE opus_moses\")\n",
    "            con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "            con.execute(\"COMMIT\")\n",
    "        except:\n",
    "            con.execute(\"ROLLBACK\"); raise\n",
    "\n",
    "        if force_checkpoint:\n",
    "            con.execute(\"FORCE CHECKPOINT\")\n",
    "\n",
    "# ---------------------------\n",
    "# B) SIMPLE ALIGNMENT FILTER\n",
    "# ---------------------------\n",
    "ALIGN_METHOD = \"inter\"  # alternatives: \"inter\", \"mwmf\", \"itermax\", \"union\"\n",
    "\n",
    "WORD = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+(?:[-'][A-Za-zÀ-ÖØ-öø-ÿ]+)*|\\d+\", re.UNICODE)\n",
    "PT_STOPWORDS = {\n",
    "    \"a\",\"à\",\"ao\",\"aos\",\"as\",\"o\",\"os\",\"um\",\"uma\",\"de\",\"da\",\"das\",\"do\",\"dos\",\"em\",\"no\",\"na\",\"nos\",\"nas\",\n",
    "    \"e\",\"ou\",\"que\",\"se\",\"por\",\"para\",\"com\",\"como\",\"mais\",\"mas\",\"não\",\"sim\",\"já\",\"sua\",\"seu\",\"suas\",\"seus\",\n",
    "    \"eu\",\"tu\",\"ele\",\"ela\",\"nós\",\"vós\",\"eles\",\"elas\",\"me\",\"te\",\"lhe\",\"nos\",\"vos\",\"lhes\",\"isso\",\"isto\",\"aquilo\"\n",
    "}\n",
    "\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    return WORD.findall(s or \"\")\n",
    "\n",
    "def ali_tokenize(s: str) -> List[str]:\n",
    "    return tokenize(s)\n",
    "\n",
    "def is_content_token(tok: str) -> bool:\n",
    "    t = (tok or \"\").lower()\n",
    "    return (t not in PT_STOPWORDS) and (len(t) > 1)\n",
    "\n",
    "try:\n",
    "    from simalign import SentenceAligner\n",
    "    _ALIGNER = SentenceAligner(model=\"xlmr\", token_type=\"word\", matching_methods=\"a\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"SimAlign required. Install:\\n\"\n",
    "        \"  pip install simalign torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "def _raw_pairs(l_tokens, r_tokens, method: str = ALIGN_METHOD):\n",
    "    \"\"\"\n",
    "    Ask SimAlign for alignments, but never crash.\n",
    "    - Catches SimAlign internal errors (IndexError, ValueError, etc.)\n",
    "    - Handles missing keys ('itermax'/'inter'/'mwmf') and falls back.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = _ALIGNER.get_word_aligns(l_tokens, r_tokens)  # dict-like\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(out, dict):\n",
    "        return []\n",
    "\n",
    "    # normalize keys robustly\n",
    "    keys = {k.lower(): k for k in out.keys()}\n",
    "    def get(name: str):\n",
    "        return out.get(keys.get(name.lower()), [])\n",
    "\n",
    "    if method.lower() == \"union\":\n",
    "        return list({*get(\"inter\"), *get(\"itermax\"), *get(\"mwmf\")})\n",
    "\n",
    "    pairs = get(method)\n",
    "    if pairs:\n",
    "        return pairs\n",
    "\n",
    "    # graceful fallbacks\n",
    "    for alt in (\"itermax\", \"inter\", \"mwmf\"):\n",
    "        pairs = get(alt)\n",
    "        if pairs:\n",
    "            return pairs\n",
    "    return []\n",
    "\n",
    "\n",
    "def _form_norm(t: str) -> str:\n",
    "    import unicodedata as ud\n",
    "    return ud.normalize(\"NFC\", t or \"\").casefold()\n",
    "\n",
    "def _type_sim_from_pairs(L_tokens, R_tokens, pairs):\n",
    "    Ln = [_form_norm(t) for t in L_tokens]\n",
    "    Rn = [_form_norm(t) for t in R_tokens]\n",
    "    union = set(Ln) | set(Rn)\n",
    "    if not union: return 1.0\n",
    "    aligned = set()\n",
    "    for i, j in pairs:\n",
    "        if 0 <= i < len(Ln): aligned.add(Ln[i])\n",
    "        if 0 <= j < len(Rn): aligned.add(Rn[j])\n",
    "    return len(aligned) / len(union)\n",
    "\n",
    "def _smooth_small_gaps(covered: set, n_tokens: int, max_gap: int = 1):\n",
    "    C = set(covered); i = 0\n",
    "    while i < n_tokens:\n",
    "        if i not in C:\n",
    "            j = i\n",
    "            while j < n_tokens and j not in C: j += 1\n",
    "            gap = j - i\n",
    "            if 0 < gap <= max_gap and i > 0 and j < n_tokens:\n",
    "                for k in range(i, j): C.add(k)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return C\n",
    "\n",
    "def _interior_uncovered(tokens: List[str], covered: set) -> List[Tuple[int,int]]:\n",
    "    runs, cur = [], []\n",
    "    for i in range(len(tokens)):\n",
    "        if i not in covered: cur.append(i)\n",
    "        elif cur: runs.append((cur[0], cur[-1])); cur=[]\n",
    "    if cur: runs.append((cur[0], cur[-1]))\n",
    "    n = len(tokens)\n",
    "    return [(i0,i1) for (i0,i1) in runs if i0>0 and i1<n-1]\n",
    "\n",
    "def _content_count(tokens: List[str]) -> int:\n",
    "    return sum(1 for t in tokens if is_content_token(t))\n",
    "\n",
    "def _type_jaccard(L_tokens, R_tokens) -> float:\n",
    "    Ln = {_form_norm(t) for t in L_tokens}\n",
    "    Rn = {_form_norm(t) for t in R_tokens}\n",
    "    U = Ln | Rn\n",
    "    return (len(Ln & Rn) / len(U)) if U else 1.0\n",
    "\n",
    "def new_sim(br: str, pt: str, method: str = ALIGN_METHOD) -> float:\n",
    "    L = ali_tokenize(br or \"\"); R = ali_tokenize(pt or \"\")\n",
    "    if not L and not R: return 1.0\n",
    "    if not L or not R:  return 0.0\n",
    "    try:\n",
    "        pairs = _raw_pairs(L, R, method=method)\n",
    "    except Exception:\n",
    "        pairs = []\n",
    "    if not pairs:\n",
    "        return _type_jaccard(L, R)\n",
    "    return _type_sim_from_pairs(L, R, pairs)\n",
    "\n",
    "\n",
    "def alignment_quality_features(\n",
    "    br_prev: str, br_here: str, br_next: str,\n",
    "    pt_prev: str, pt_here: str, pt_next: str,\n",
    "    *, use_window: bool = True, sim_fn=None\n",
    ") -> dict:\n",
    "    if sim_fn is None:\n",
    "        sim_fn = new_sim\n",
    "\n",
    "    br_toks = ali_tokenize(br_here); pt_toks = ali_tokenize(pt_here)\n",
    "    if use_window:\n",
    "        pt_win = ali_tokenize(\" \".join(x for x in [pt_prev, pt_here, pt_next] if x))\n",
    "        br_pairs = _raw_pairs(br_toks, pt_win, method=ALIGN_METHOD)\n",
    "        br_cov = {i for i,_ in br_pairs}\n",
    "        br_win = ali_tokenize(\" \".join(x for x in [br_prev, br_here, br_next] if x))\n",
    "        pt_pairs = _raw_pairs(pt_toks, br_win, method=ALIGN_METHOD)\n",
    "        pt_cov = {i for i,_ in pt_pairs}\n",
    "    else:\n",
    "        pairs  = _raw_pairs(br_toks, pt_toks, method=ALIGN_METHOD)\n",
    "        br_cov = {i for i,_ in pairs}\n",
    "        pt_cov = {j for _,j in pairs}\n",
    "\n",
    "    br_cov = _smooth_small_gaps(br_cov, len(br_toks), 1)\n",
    "    pt_cov = _smooth_small_gaps(pt_cov, len(pt_toks), 1)\n",
    "\n",
    "    br_len = len(br_toks); pt_len = len(pt_toks)\n",
    "    br_cov_ratio = len(br_cov) / max(1, br_len)\n",
    "    pt_cov_ratio = len(pt_cov) / max(1, pt_len)\n",
    "    cov_min = min(br_cov_ratio, pt_cov_ratio)\n",
    "    cov_gap = abs(br_cov_ratio - pt_cov_ratio)\n",
    "\n",
    "    br_int_spans = _interior_uncovered(br_toks, br_cov)\n",
    "    pt_int_spans = _interior_uncovered(pt_toks, pt_cov)\n",
    "\n",
    "    def _content_in_runs(tokens, runs):\n",
    "        return sum(1 for i0,i1 in runs for t in tokens[i0:i1+1] if is_content_token(t))\n",
    "\n",
    "    br_ct = _content_count(br_toks)\n",
    "    pt_ct = _content_count(pt_toks)\n",
    "    br_int_ratio = _content_in_runs(br_toks, br_int_spans) / max(1, br_ct)\n",
    "    pt_int_ratio = _content_in_runs(pt_toks, pt_int_spans) / max(1, pt_ct)\n",
    "\n",
    "    base_sim = float(new_sim(br_here, pt_here, method=ALIGN_METHOD))\n",
    "\n",
    "    return {\n",
    "        \"br_cov\": br_cov_ratio, \"pt_cov\": pt_cov_ratio,\n",
    "        \"cov_min\": cov_min, \"cov_gap\": cov_gap,\n",
    "        \"br_int_content_ratio\": br_int_ratio, \"pt_int_content_ratio\": pt_int_ratio,\n",
    "        \"br_content_total\": br_ct, \"pt_content_total\": pt_ct,\n",
    "        \"br_len\": br_len, \"pt_len\": pt_len,\n",
    "        \"base_sim\": base_sim\n",
    "    }\n",
    "\n",
    "def alignment_similarity_only_flag(\n",
    "    feats: dict,\n",
    "    *,\n",
    "    min_sim_ok: float | None = None,\n",
    "    length_mode: str = \"total\",\n",
    "    base_min_sim: float = 0.30,\n",
    "    long_min_sim: float = 0.70,\n",
    "    short_len: int = 8,\n",
    "    long_len: int = 28,\n",
    "    **kwargs,\n",
    ") -> Tuple[bool,str,float,int]:\n",
    "    sim = float(feats[\"base_sim\"])\n",
    "\n",
    "    if min_sim_ok is not None:\n",
    "        thr = float(min_sim_ok)\n",
    "        L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "    else:\n",
    "        if length_mode == \"total\":\n",
    "            L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "        elif length_mode == \"char\":\n",
    "            L = max(int(feats.get(\"br_len\", 0)), int(feats.get(\"pt_len\", 0)))\n",
    "        else:  # \"content\"\n",
    "            L = max(int(feats.get(\"br_content_total\", 0)), int(feats.get(\"pt_content_total\", 0)))\n",
    "\n",
    "        if L <= short_len:\n",
    "            thr = base_min_sim\n",
    "        elif L >= long_len:\n",
    "            thr = long_min_sim\n",
    "        else:\n",
    "            t = (L - short_len) / max(1, (long_len - short_len))\n",
    "            thr = base_min_sim + t * (long_min_sim - base_min_sim)\n",
    "\n",
    "    activate = sim < thr\n",
    "    reason = (f\"low_similarity@simple(thr={thr:.2f},L={L})\" if activate else \"ok@simple\")\n",
    "    return activate, reason, thr, L\n",
    "\n",
    "# ---------------------------\n",
    "# C) Simple filter across the corpus\n",
    "# ---------------------------\n",
    "def build_simple_filter_flags_chunked(\n",
    "    *,\n",
    "    db_path,\n",
    "    chunk_size: int = 50_000,\n",
    "    use_window: bool = True,\n",
    "    thresholds: dict | None = None,\n",
    "):\n",
    "    if thresholds is None:\n",
    "        thresholds = dict(min_sim_ok=None, length_mode=\"total\",\n",
    "                          base_min_sim=0.30, long_min_sim=0.70,\n",
    "                          short_len=8, long_len=28)\n",
    "\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        _ensure_simple_filter_table(con)\n",
    "        mn, mx = con.execute(\"SELECT MIN(line_no), MAX(line_no) FROM opus_moses\").fetchone()\n",
    "        if mn is None or mx is None:\n",
    "            print(\"[simple-filter] opus_moses empty; nothing to do.\")\n",
    "            return\n",
    "        mn, mx = int(mn), int(mx)\n",
    "\n",
    "        cur = mn\n",
    "        total_flagged = 0\n",
    "        while cur <= mx:\n",
    "            hi = min(cur + chunk_size - 1, mx)\n",
    "\n",
    "            df = con.execute(\"\"\"\n",
    "                SELECT line_no, pair_id, sent_pt_br, sent_pt_pt\n",
    "                FROM opus_moses\n",
    "                WHERE line_no BETWEEN ? AND ?\n",
    "                ORDER BY line_no\n",
    "            \"\"\", [max(mn, cur-1), min(mx, hi+1)]).df()\n",
    "\n",
    "            rows = []\n",
    "            n = len(df)\n",
    "            for idx in range(n):\n",
    "                line_no = int(df.line_no.iloc[idx])\n",
    "                if line_no < cur or line_no > hi:\n",
    "                    continue\n",
    "                br_prev = df.sent_pt_br.iloc[idx-1] if idx > 0 else \"\"\n",
    "                br_here = df.sent_pt_br.iloc[idx]\n",
    "                br_next = df.sent_pt_br.iloc[idx+1] if idx+1 < n else \"\"\n",
    "                pt_prev = df.sent_pt_pt.iloc[idx-1] if idx > 0 else \"\"\n",
    "                pt_here = df.sent_pt_pt.iloc[idx]\n",
    "                pt_next = df.sent_pt_pt.iloc[idx+1] if idx+1 < n else \"\"\n",
    "\n",
    "                feats = alignment_quality_features(\n",
    "                    br_prev, br_here, br_next,\n",
    "                    pt_prev, pt_here, pt_next,\n",
    "                    use_window=use_window, sim_fn=new_sim\n",
    "                )\n",
    "                activate, reason, thr, L = alignment_similarity_only_flag(feats, **thresholds)\n",
    "                if activate:\n",
    "                    rows.append({\n",
    "                        \"line_no\": line_no,\n",
    "                        \"pair_id\": _as_int_or_none(df.pair_id.iloc[idx]),\n",
    "                        \"reason\": reason,\n",
    "                        \"base_sim\": float(feats[\"base_sim\"]),\n",
    "                        \"thr\": float(thr),\n",
    "                        \"L\": int(L),\n",
    "                    })\n",
    "\n",
    "            if rows:\n",
    "                con.register(\"flags\", pd.DataFrame(rows))\n",
    "                con.execute(\"\"\"\n",
    "                    INSERT INTO opus_filter_simple (line_no, pair_id, reason, base_sim, thr, L)\n",
    "                    SELECT line_no, pair_id, reason, base_sim, thr, L FROM flags\n",
    "                    ON CONFLICT(line_no) DO UPDATE SET\n",
    "                        pair_id = EXCLUDED.pair_id,\n",
    "                        reason  = EXCLUDED.reason,\n",
    "                        base_sim= EXCLUDED.base_sim,\n",
    "                        thr     = EXCLUDED.thr,\n",
    "                        L       = EXCLUDED.L\n",
    "                \"\"\")\n",
    "                con.unregister(\"flags\")\n",
    "                total_flagged += len(rows)\n",
    "\n",
    "            print(f\"[simple-filter] [{cur}..{hi}] flagged={len(rows)} (cum={total_flagged})\")\n",
    "            cur = hi + 1\n",
    "\n",
    "        print(f\"[simple-filter] DONE. total flagged lines: {total_flagged}\")\n",
    "\n",
    "def apply_simple_filter_ctas_swap(*, db_path, drop_flags=False):\n",
    "    with duckdb.connect(str(db_path)) as con:\n",
    "        con.execute(\"BEGIN\")\n",
    "        try:\n",
    "            con.execute(\"DROP TABLE IF EXISTS opus_moses_new\")\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE opus_moses_new AS\n",
    "                SELECT o.*\n",
    "                FROM opus_moses o\n",
    "                LEFT JOIN opus_filter_simple f USING (line_no)\n",
    "                WHERE f.line_no IS NULL\n",
    "                ORDER BY o.line_no\n",
    "            \"\"\")\n",
    "            con.execute(\"DROP TABLE opus_moses\")\n",
    "            con.execute(\"ALTER TABLE opus_moses_new RENAME TO opus_moses\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_line_pk ON opus_moses(line_no)\")\n",
    "            con.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS opus_moses_pair_uq  ON opus_moses(pair_id)\")\n",
    "            con.execute(\"COMMIT\")\n",
    "        except:\n",
    "            con.execute(\"ROLLBACK\"); raise\n",
    "\n",
    "        con.execute(\"FORCE CHECKPOINT\")\n",
    "        kept = con.execute(\"SELECT COUNT(*) FROM opus_moses\").fetchone()[0]\n",
    "        removed = con.execute(\"SELECT COUNT(*) FROM opus_filter_simple\").fetchone()[0]\n",
    "        print(f\"[simple-filter] applied. kept={kept:,} removed={removed:,}\")\n",
    "        if drop_flags:\n",
    "            con.execute(\"DROP TABLE opus_filter_simple\")\n",
    "            con.execute(\"FORCE CHECKPOINT\")\n",
    "\n",
    "# ---------------------------\n",
    "# D) One-call convenience runner (with simple reset)\n",
    "# ---------------------------\n",
    "def run_opus_pipeline_simple(\n",
    "    *,\n",
    "    db_path,\n",
    "    block_size: int = 50_000,\n",
    "    neighbor_overlap: int = 3,\n",
    "    neighbor_margin: float = 0.04,\n",
    "    neighbor_max_clause_chars: int = 60,\n",
    "    neighbor_max_iters: int = 5,\n",
    "    dedup_chunk_size: int = 50_000,\n",
    "    delete_on_trigger: bool = True,\n",
    "    filter_chunk_size: int = 50_000,\n",
    "    filter_use_window: bool = True,\n",
    "    filter_thresholds: dict | None = None,\n",
    "    apply_filter: bool = True,\n",
    "    reset: bool = False,          # <<< simple switch\n",
    "):\n",
    "    # reset planning progress + clear flags if asked\n",
    "    if reset:\n",
    "        with duckdb.connect(str(db_path)) as con:\n",
    "            _ensure_opus_ops_tables(con)\n",
    "            _ensure_simple_filter_table(con)\n",
    "            con.execute(\"DELETE FROM opus_ops_update\")\n",
    "            con.execute(\"DELETE FROM opus_ops_delete\")\n",
    "            con.execute(\"DELETE FROM opus_ops_progress\")\n",
    "            con.execute(\"INSERT INTO opus_ops_progress VALUES (0)\")\n",
    "            con.execute(\"DELETE FROM opus_filter_simple\")\n",
    "        print(\"[pipeline] reset: cleared ops progress and flags.\")\n",
    "\n",
    "    # 1) plan + swap (merge lines, clean)\n",
    "    plan_ops_over_corpus(block_size=block_size, reset=False, db_path=db_path)\n",
    "    apply_ops_ctas_swap(db_path=db_path)\n",
    "\n",
    "    # 2) neighbor moves (assumes your function exists)\n",
    "    apply_neighbor_moves_corpus_inplace(\n",
    "        block_size=block_size,\n",
    "        overlap=neighbor_overlap,\n",
    "        margin=neighbor_margin,\n",
    "        max_clause_chars=neighbor_max_clause_chars,\n",
    "        max_iters=neighbor_max_iters,\n",
    "    )\n",
    "\n",
    "    # 3) repeated-prefix cleaner (assumes your function exists)\n",
    "    run_repeated_prefix_cleaner_chunked(\n",
    "        db_path=db_path,\n",
    "        prev_window=6,\n",
    "        min_prefix_tokens=6,\n",
    "        coverage_thresh=0.92,\n",
    "        require_both=False,\n",
    "        collapse_adjacent_dups=True,\n",
    "        delete_on_trigger=delete_on_trigger,\n",
    "        delete_if_empty_only=False,\n",
    "        chunk_size=dedup_chunk_size,\n",
    "        start_line=None, end_line=None,\n",
    "        apply_changes=True,\n",
    "        print_updates=False,\n",
    "        trace_lines=None\n",
    "    )\n",
    "\n",
    "    # 4) simple alignment filter → write flags\n",
    "    build_simple_filter_flags_chunked(\n",
    "        db_path=db_path,\n",
    "        chunk_size=filter_chunk_size,\n",
    "        use_window=filter_use_window,\n",
    "        thresholds=filter_thresholds\n",
    "    )\n",
    "\n",
    "    # 5) apply filter (CTAS swap) if requested\n",
    "    if apply_filter:\n",
    "        apply_simple_filter_ctas_swap(db_path=db_path, drop_flags=False)\n",
    "\n",
    "    print(\"[pipeline] complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcfc856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a55afb47a44bd6aa1811b3fde97355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ffa538632456aa7d1597adaaca32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1..50000] updates=0 deletes=0\n",
      "[50001..100000] updates=0 deletes=0\n",
      "[100001..150000] updates=0 deletes=0\n",
      "[150001..200000] updates=0 deletes=0\n",
      "[200001..250000] updates=0 deletes=0\n",
      "[250001..300000] updates=0 deletes=0\n",
      "[300001..350000] updates=0 deletes=0\n",
      "[350001..400000] updates=0 deletes=0\n",
      "[400001..450000] updates=0 deletes=0\n",
      "[450001..500000] updates=0 deletes=0\n",
      "[500001..550000] updates=0 deletes=0\n",
      "[550001..600000] updates=0 deletes=0\n",
      "[600001..650000] updates=0 deletes=0\n",
      "[650001..700000] updates=0 deletes=0\n",
      "[700001..750000] updates=0 deletes=0\n",
      "[750001..800000] updates=0 deletes=0\n",
      "[800001..850000] updates=0 deletes=0\n",
      "[850001..900000] updates=0 deletes=0\n",
      "[900001..950000] updates=0 deletes=0\n",
      "[950001..1000000] updates=0 deletes=0\n",
      "[1000001..1050000] updates=0 deletes=0\n",
      "[1050001..1100000] updates=0 deletes=0\n",
      "[1100001..1150000] updates=0 deletes=0\n",
      "[1150001..1200000] updates=0 deletes=0\n",
      "[1200001..1250000] updates=0 deletes=0\n",
      "[1250001..1300000] updates=0 deletes=0\n",
      "[1300001..1350000] updates=0 deletes=0\n",
      "[1350001..1400000] updates=0 deletes=0\n",
      "[1400001..1450000] updates=0 deletes=0\n",
      "[1450001..1500000] updates=0 deletes=0\n",
      "[1500001..1550000] updates=0 deletes=0\n",
      "[1550001..1600000] updates=0 deletes=0\n",
      "[1600001..1650000] updates=0 deletes=0\n",
      "[1650001..1700000] updates=0 deletes=0\n",
      "[1700001..1750000] updates=0 deletes=0\n",
      "[1750001..1800000] updates=0 deletes=0\n",
      "[1800001..1850000] updates=0 deletes=0\n",
      "[1850001..1900000] updates=0 deletes=0\n",
      "[1900001..1950000] updates=0 deletes=0\n",
      "[1950001..2000000] updates=0 deletes=0\n",
      "[2000001..2050000] updates=0 deletes=0\n",
      "[2050001..2100000] updates=0 deletes=0\n",
      "[2100001..2150000] updates=0 deletes=0\n",
      "[2150001..2200000] updates=0 deletes=0\n",
      "[2200001..2250000] updates=0 deletes=0\n",
      "[2250001..2300000] updates=0 deletes=0\n",
      "[2300001..2350000] updates=0 deletes=0\n",
      "[2350001..2400000] updates=0 deletes=0\n",
      "[2400001..2450000] updates=0 deletes=0\n",
      "[2450001..2500000] updates=0 deletes=0\n",
      "[2500001..2550000] updates=0 deletes=0\n",
      "[2550001..2600000] updates=0 deletes=0\n",
      "[2600001..2650000] updates=0 deletes=0\n",
      "[2650001..2700000] updates=0 deletes=0\n",
      "[2700001..2750000] updates=0 deletes=0\n",
      "[2750001..2800000] updates=0 deletes=0\n",
      "[2800001..2850000] updates=0 deletes=0\n",
      "[2850001..2900000] updates=0 deletes=0\n",
      "[2900001..2950000] updates=0 deletes=0\n",
      "[2950001..3000000] updates=0 deletes=0\n",
      "[3000001..3050000] updates=0 deletes=0\n",
      "[3050001..3100000] updates=0 deletes=0\n",
      "[3100001..3150000] updates=0 deletes=0\n",
      "[3150001..3200000] updates=0 deletes=0\n",
      "[3200001..3250000] updates=0 deletes=0\n",
      "[3250001..3300000] updates=0 deletes=0\n",
      "[3300001..3350000] updates=0 deletes=0\n",
      "[3350001..3400000] updates=0 deletes=0\n",
      "[3400001..3450000] updates=0 deletes=0\n",
      "[3450001..3500000] updates=0 deletes=0\n",
      "[3500001..3550000] updates=0 deletes=0\n",
      "[3550001..3600000] updates=0 deletes=0\n",
      "[3600001..3650000] updates=0 deletes=0\n",
      "[3650001..3700000] updates=0 deletes=0\n",
      "[3700001..3750000] updates=0 deletes=0\n",
      "[3750001..3800000] updates=0 deletes=0\n",
      "[3800001..3850000] updates=0 deletes=0\n",
      "[3850001..3900000] updates=0 deletes=0\n",
      "[3900001..3950000] updates=0 deletes=0\n",
      "[3950001..4000000] updates=0 deletes=0\n",
      "[4000001..4050000] updates=0 deletes=0\n",
      "[4050001..4100000] updates=0 deletes=0\n",
      "[4100001..4150000] updates=0 deletes=0\n",
      "[4150001..4200000] updates=0 deletes=0\n",
      "[4200001..4250000] updates=0 deletes=0\n",
      "[4250001..4300000] updates=0 deletes=0\n",
      "[4300001..4350000] updates=0 deletes=0\n",
      "[4350001..4400000] updates=0 deletes=0\n",
      "[4400001..4450000] updates=0 deletes=0\n",
      "[4450001..4500000] updates=0 deletes=0\n",
      "[4500001..4550000] updates=0 deletes=0\n",
      "[4550001..4600000] updates=0 deletes=0\n",
      "[4600001..4650000] updates=0 deletes=0\n",
      "[4650001..4700000] updates=0 deletes=0\n",
      "[4700001..4750000] updates=0 deletes=0\n",
      "[4750001..4800000] updates=0 deletes=0\n",
      "[4800001..4850000] updates=0 deletes=0\n",
      "[4850001..4900000] updates=0 deletes=0\n",
      "[4900001..4950000] updates=0 deletes=0\n",
      "[4950001..5000000] updates=0 deletes=0\n",
      "[5000001..5050000] updates=0 deletes=0\n",
      "[5050001..5100000] updates=0 deletes=0\n",
      "[5100001..5150000] updates=0 deletes=0\n",
      "[5150001..5200000] updates=0 deletes=0\n",
      "[5200001..5250000] updates=0 deletes=0\n",
      "[5250001..5300000] updates=0 deletes=0\n",
      "[5300001..5350000] updates=0 deletes=0\n",
      "[5350001..5400000] updates=0 deletes=0\n",
      "[5400001..5450000] updates=0 deletes=0\n",
      "[5450001..5500000] updates=0 deletes=0\n",
      "[5500001..5550000] updates=0 deletes=0\n",
      "[5550001..5600000] updates=0 deletes=0\n",
      "[5600001..5650000] updates=0 deletes=0\n",
      "[5650001..5700000] updates=0 deletes=0\n",
      "[5700001..5750000] updates=0 deletes=0\n",
      "[5750001..5800000] updates=0 deletes=0\n",
      "[5800001..5850000] updates=0 deletes=0\n",
      "[5850001..5900000] updates=0 deletes=0\n",
      "[5900001..5950000] updates=0 deletes=0\n",
      "[5950001..6000000] updates=0 deletes=0\n",
      "[6000001..6050000] updates=0 deletes=0\n",
      "[6050001..6100000] updates=0 deletes=0\n",
      "[6100001..6150000] updates=0 deletes=0\n",
      "[6150001..6200000] updates=0 deletes=0\n",
      "[6200001..6250000] updates=0 deletes=0\n",
      "[6250001..6300000] updates=0 deletes=0\n",
      "[6300001..6350000] updates=0 deletes=0\n",
      "[6350001..6400000] updates=0 deletes=0\n",
      "[6400001..6450000] updates=0 deletes=0\n",
      "[6450001..6500000] updates=0 deletes=0\n",
      "[6500001..6550000] updates=0 deletes=0\n",
      "[6550001..6600000] updates=0 deletes=0\n",
      "[6600001..6650000] updates=0 deletes=0\n",
      "[6650001..6700000] updates=0 deletes=0\n",
      "[6700001..6750000] updates=0 deletes=0\n",
      "[6750001..6800000] updates=0 deletes=0\n",
      "[6800001..6850000] updates=0 deletes=0\n",
      "[6850001..6900000] updates=0 deletes=0\n",
      "[6900001..6950000] updates=0 deletes=0\n",
      "[6950001..7000000] updates=0 deletes=0\n",
      "[7000001..7050000] updates=0 deletes=0\n",
      "[7050001..7100000] updates=0 deletes=0\n",
      "[7100001..7150000] updates=0 deletes=0\n",
      "[7150001..7200000] updates=0 deletes=0\n",
      "[7200001..7250000] updates=0 deletes=0\n",
      "[7250001..7300000] updates=0 deletes=0\n",
      "[7300001..7350000] updates=0 deletes=0\n",
      "[7350001..7400000] updates=0 deletes=0\n",
      "[7400001..7450000] updates=0 deletes=0\n",
      "[7450001..7500000] updates=0 deletes=0\n",
      "[7500001..7550000] updates=0 deletes=0\n",
      "[7550001..7600000] updates=0 deletes=0\n",
      "[7600001..7650000] updates=0 deletes=0\n",
      "[7650001..7700000] updates=0 deletes=0\n",
      "[7700001..7750000] updates=0 deletes=0\n",
      "[7750001..7800000] updates=0 deletes=0\n",
      "[7800001..7850000] updates=0 deletes=0\n",
      "[7850001..7900000] updates=0 deletes=0\n",
      "[7900001..7950000] updates=0 deletes=0\n",
      "[7950001..8000000] updates=0 deletes=0\n",
      "[8000001..8050000] updates=0 deletes=0\n",
      "[8050001..8100000] updates=0 deletes=0\n",
      "[8100001..8150000] updates=0 deletes=0\n",
      "[8150001..8200000] updates=0 deletes=0\n",
      "[8200001..8250000] updates=0 deletes=0\n",
      "[8250001..8300000] updates=0 deletes=0\n",
      "[8300001..8350000] updates=0 deletes=0\n",
      "[8350001..8400000] updates=0 deletes=0\n",
      "[8400001..8450000] updates=0 deletes=0\n",
      "[8450001..8500000] updates=0 deletes=0\n",
      "[8500001..8550000] updates=0 deletes=0\n",
      "[8550001..8600000] updates=0 deletes=0\n",
      "[8600001..8650000] updates=0 deletes=0\n",
      "[8650001..8700000] updates=0 deletes=0\n",
      "[8700001..8750000] updates=0 deletes=0\n",
      "[8750001..8800000] updates=0 deletes=0\n",
      "[8800001..8850000] updates=0 deletes=0\n",
      "[8850001..8900000] updates=0 deletes=0\n",
      "[8900001..8950000] updates=0 deletes=0\n",
      "[8950001..9000000] updates=0 deletes=0\n",
      "[9000001..9050000] updates=0 deletes=0\n",
      "[9050001..9100000] updates=0 deletes=0\n",
      "[9100001..9150000] updates=0 deletes=0\n",
      "[9150001..9200000] updates=0 deletes=0\n",
      "[9200001..9250000] updates=0 deletes=0\n",
      "[9250001..9300000] updates=0 deletes=0\n",
      "[9300001..9350000] updates=0 deletes=0\n",
      "[9350001..9400000] updates=0 deletes=0\n",
      "[9400001..9450000] updates=0 deletes=0\n",
      "[9450001..9500000] updates=0 deletes=0\n",
      "[9500001..9550000] updates=0 deletes=0\n",
      "[9550001..9600000] updates=0 deletes=0\n",
      "[9600001..9650000] updates=0 deletes=0\n",
      "[9650001..9700000] updates=0 deletes=0\n",
      "[9700001..9750000] updates=0 deletes=0\n",
      "[9750001..9800000] updates=0 deletes=0\n",
      "[9800001..9850000] updates=0 deletes=0\n",
      "[9850001..9900000] updates=0 deletes=0\n",
      "[9900001..9950000] updates=0 deletes=0\n",
      "[9950001..10000000] updates=0 deletes=0\n",
      "[10000001..10050000] updates=0 deletes=0\n",
      "[10050001..10100000] updates=0 deletes=0\n",
      "[10100001..10150000] updates=0 deletes=0\n",
      "[10150001..10200000] updates=0 deletes=0\n",
      "[10200001..10250000] updates=0 deletes=0\n",
      "[10250001..10300000] updates=0 deletes=0\n",
      "[10300001..10350000] updates=0 deletes=0\n",
      "[10350001..10400000] updates=0 deletes=1\n",
      "  Deleted rows (line_no, pair_id):\n",
      "    (10360006,10360007)\n",
      "[10400001..10450000] updates=0 deletes=0\n",
      "[10450001..10500000] updates=0 deletes=0\n",
      "[10500001..10550000] updates=0 deletes=0\n",
      "[10550001..10600000] updates=0 deletes=0\n",
      "[10600001..10650000] updates=0 deletes=0\n",
      "[10650001..10700000] updates=0 deletes=0\n",
      "[10700001..10750000] updates=0 deletes=0\n",
      "[10750001..10800000] updates=0 deletes=0\n",
      "[10800001..10850000] updates=0 deletes=0\n",
      "[10850001..10900000] updates=0 deletes=0\n",
      "[10900001..10950000] updates=0 deletes=0\n",
      "[10950001..11000000] updates=0 deletes=0\n",
      "[11000001..11050000] updates=0 deletes=0\n",
      "[11050001..11100000] updates=0 deletes=0\n",
      "[11100001..11150000] updates=0 deletes=0\n",
      "[11150001..11200000] updates=0 deletes=0\n",
      "[11200001..11250000] updates=0 deletes=0\n",
      "[11250001..11300000] updates=0 deletes=0\n",
      "[11300001..11350000] updates=0 deletes=0\n",
      "[11350001..11400000] updates=0 deletes=0\n",
      "[11400001..11450000] updates=0 deletes=0\n",
      "[11450001..11500000] updates=0 deletes=0\n",
      "[11500001..11550000] updates=0 deletes=0\n",
      "[11550001..11600000] updates=0 deletes=0\n",
      "[11600001..11650000] updates=0 deletes=0\n",
      "[11650001..11700000] updates=0 deletes=0\n",
      "[11700001..11750000] updates=0 deletes=0\n",
      "[11750001..11800000] updates=0 deletes=0\n",
      "[11800001..11850000] updates=0 deletes=0\n",
      "[11850001..11900000] updates=0 deletes=0\n",
      "[11900001..11950000] updates=0 deletes=0\n",
      "[11950001..12000000] updates=0 deletes=0\n",
      "[12000001..12050000] updates=0 deletes=0\n",
      "[12050001..12100000] updates=0 deletes=0\n",
      "[12100001..12150000] updates=0 deletes=0\n",
      "[12150001..12200000] updates=0 deletes=0\n",
      "[12200001..12250000] updates=0 deletes=0\n",
      "[12250001..12300000] updates=0 deletes=0\n",
      "[12300001..12350000] updates=0 deletes=0\n",
      "[12350001..12400000] updates=0 deletes=0\n",
      "[12400001..12450000] updates=0 deletes=0\n",
      "[12450001..12500000] updates=0 deletes=0\n",
      "[12500001..12550000] updates=0 deletes=0\n",
      "[12550001..12600000] updates=0 deletes=0\n",
      "[12600001..12650000] updates=0 deletes=0\n",
      "[12650001..12700000] updates=0 deletes=0\n",
      "[12700001..12750000] updates=0 deletes=0\n",
      "[12750001..12800000] updates=0 deletes=0\n",
      "[12800001..12850000] updates=0 deletes=0\n",
      "[12850001..12900000] updates=0 deletes=0\n",
      "[12900001..12950000] updates=0 deletes=0\n",
      "[12950001..13000000] updates=0 deletes=0\n",
      "[13000001..13050000] updates=0 deletes=0\n",
      "[13050001..13100000] updates=0 deletes=0\n",
      "[13100001..13150000] updates=0 deletes=0\n",
      "[13150001..13200000] updates=0 deletes=0\n",
      "[13200001..13250000] updates=0 deletes=0\n",
      "[13250001..13300000] updates=0 deletes=0\n",
      "[13300001..13350000] updates=0 deletes=0\n",
      "[13350001..13400000] updates=0 deletes=0\n",
      "[13400001..13450000] updates=0 deletes=0\n",
      "[13450001..13500000] updates=0 deletes=0\n",
      "[13500001..13550000] updates=0 deletes=0\n",
      "[13550001..13600000] updates=0 deletes=0\n",
      "[13600001..13650000] updates=0 deletes=0\n",
      "[13650001..13700000] updates=0 deletes=0\n",
      "[13700001..13750000] updates=0 deletes=0\n",
      "[13750001..13800000] updates=0 deletes=0\n",
      "[13800001..13850000] updates=0 deletes=0\n",
      "[13850001..13900000] updates=0 deletes=0\n",
      "[13900001..13950000] updates=0 deletes=0\n",
      "[13950001..14000000] updates=0 deletes=0\n",
      "[14000001..14050000] updates=0 deletes=0\n",
      "[14050001..14100000] updates=0 deletes=0\n",
      "[14100001..14150000] updates=0 deletes=0\n",
      "[14150001..14200000] updates=0 deletes=0\n",
      "[14200001..14250000] updates=0 deletes=0\n",
      "[14250001..14300000] updates=0 deletes=0\n",
      "[14300001..14350000] updates=0 deletes=0\n",
      "[14350001..14400000] updates=0 deletes=0\n",
      "[14400001..14450000] updates=0 deletes=0\n",
      "[14450001..14500000] updates=0 deletes=0\n",
      "[14500001..14550000] updates=0 deletes=0\n",
      "[14550001..14600000] updates=0 deletes=0\n",
      "[14600001..14650000] updates=0 deletes=0\n",
      "[14650001..14700000] updates=0 deletes=0\n",
      "[14700001..14750000] updates=0 deletes=0\n",
      "[14750001..14800000] updates=0 deletes=0\n",
      "[14800001..14850000] updates=0 deletes=0\n",
      "[14850001..14900000] updates=0 deletes=0\n",
      "[14900001..14950000] updates=0 deletes=0\n",
      "[14950001..15000000] updates=0 deletes=0\n",
      "[15000001..15050000] updates=0 deletes=0\n",
      "[15050001..15100000] updates=0 deletes=0\n",
      "[15100001..15150000] updates=0 deletes=0\n",
      "[15150001..15200000] updates=0 deletes=0\n",
      "[15200001..15250000] updates=0 deletes=0\n",
      "[15250001..15300000] updates=0 deletes=0\n",
      "[15300001..15350000] updates=0 deletes=0\n",
      "[15350001..15400000] updates=0 deletes=0\n",
      "[15400001..15450000] updates=0 deletes=0\n",
      "[15450001..15500000] updates=0 deletes=0\n",
      "[15500001..15550000] updates=0 deletes=0\n",
      "[15550001..15600000] updates=0 deletes=0\n",
      "[15600001..15650000] updates=0 deletes=0\n",
      "[15650001..15700000] updates=0 deletes=0\n",
      "[15700001..15750000] updates=0 deletes=0\n",
      "[15750001..15800000] updates=0 deletes=0\n",
      "[15800001..15850000] updates=0 deletes=0\n",
      "[15850001..15900000] updates=0 deletes=0\n",
      "[15900001..15950000] updates=0 deletes=0\n",
      "[15950001..16000000] updates=0 deletes=0\n",
      "[16000001..16031951] updates=0 deletes=0\n",
      "[simple-filter] [1..50000] flagged=2224 (cum=2224)\n",
      "[simple-filter] [50001..100000] flagged=2220 (cum=4444)\n",
      "[simple-filter] [100001..150000] flagged=2053 (cum=6497)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m DB_PATH = \u001b[33m\"\u001b[39m\u001b[33m../data/duckdb/subs.duckdb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mrun_opus_pipeline_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDB_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_use_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# prev/here/next context for SimAlign\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_thresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# tune these if you like\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_sim_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# None → length-adaptive threshold\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlength_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_min_sim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlong_min_sim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshort_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlong_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# set False to only write flags\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 606\u001b[39m, in \u001b[36mrun_opus_pipeline_simple\u001b[39m\u001b[34m(db_path, block_size, neighbor_overlap, neighbor_margin, neighbor_max_clause_chars, neighbor_max_iters, dedup_chunk_size, delete_on_trigger, filter_chunk_size, filter_use_window, filter_thresholds, apply_filter, reset)\u001b[39m\n\u001b[32m    589\u001b[39m run_repeated_prefix_cleaner_chunked(\n\u001b[32m    590\u001b[39m     db_path=db_path,\n\u001b[32m    591\u001b[39m     prev_window=\u001b[32m6\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    602\u001b[39m     trace_lines=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    603\u001b[39m )\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# 4) simple alignment filter → write flags\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m \u001b[43mbuild_simple_filter_flags_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_use_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_thresholds\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;66;03m# 5) apply filter (CTAS swap) if requested\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m apply_filter:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 446\u001b[39m, in \u001b[36mbuild_simple_filter_flags_chunked\u001b[39m\u001b[34m(db_path, chunk_size, use_window, thresholds)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m thresholds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    442\u001b[39m     thresholds = \u001b[38;5;28mdict\u001b[39m(min_sim_ok=\u001b[38;5;28;01mNone\u001b[39;00m, length_mode=\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    443\u001b[39m                       base_min_sim=\u001b[32m0.30\u001b[39m, long_min_sim=\u001b[32m0.70\u001b[39m,\n\u001b[32m    444\u001b[39m                       short_len=\u001b[32m8\u001b[39m, long_len=\u001b[32m28\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_ensure_simple_filter_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT MIN(line_no), MAX(line_no) FROM opus_moses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetchone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 479\u001b[39m, in \u001b[36mbuild_simple_filter_flags_chunked\u001b[39m\u001b[34m(db_path, chunk_size, use_window, thresholds)\u001b[39m\n\u001b[32m    476\u001b[39m pt_here = df.sent_pt_pt.iloc[idx]\n\u001b[32m    477\u001b[39m pt_next = df.sent_pt_pt.iloc[idx+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx+\u001b[32m1\u001b[39m < n \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m feats = \u001b[43malignment_quality_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbr_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbr_here\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbr_next\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_here\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_next\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_sim\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m activate, reason, thr, L = alignment_similarity_only_flag(feats, **thresholds)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m activate:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 360\u001b[39m, in \u001b[36malignment_quality_features\u001b[39m\u001b[34m(br_prev, br_here, br_next, pt_prev, pt_here, pt_next, use_window, sim_fn)\u001b[39m\n\u001b[32m    358\u001b[39m     pt_cov = {i \u001b[38;5;28;01mfor\u001b[39;00m i,_ \u001b[38;5;129;01min\u001b[39;00m pt_pairs}\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     pairs  = \u001b[43m_raw_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbr_toks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_toks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALIGN_METHOD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m     br_cov = {i \u001b[38;5;28;01mfor\u001b[39;00m i,_ \u001b[38;5;129;01min\u001b[39;00m pairs}\n\u001b[32m    362\u001b[39m     pt_cov = {j \u001b[38;5;28;01mfor\u001b[39;00m _,j \u001b[38;5;129;01min\u001b[39;00m pairs}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36m_raw_pairs\u001b[39m\u001b[34m(l_tokens, r_tokens, method)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03mAsk SimAlign for alignments, but never crash.\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03m- Catches SimAlign internal errors (IndexError, ValueError, etc.)\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[33;03m- Handles missing keys ('itermax'/'inter'/'mwmf') and falls back.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     out = \u001b[43m_ALIGNER\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_word_aligns\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_tokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dict-like\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/simalign/simalign.py:209\u001b[39m, in \u001b[36mSentenceAligner.get_word_aligns\u001b[39m\u001b[34m(self, src_sent, trg_sent)\u001b[39m\n\u001b[32m    206\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m i, wlist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(l2_tokens):\n\u001b[32m    207\u001b[39m \t\tl2_b2w_map += [i \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wlist]\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m vectors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_embed_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_sent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.cpu().detach().numpy()\n\u001b[32m    210\u001b[39m vectors = [vectors[i, :\u001b[38;5;28mlen\u001b[39m(bpe_lists[i])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m]]\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_type == \u001b[33m\"\u001b[39m\u001b[33mword\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/simalign/simalign.py:65\u001b[39m, in \u001b[36mEmbeddingLoader.get_embed_list\u001b[39m\u001b[34m(self, sent_batch)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     64\u001b[39m \tinputs = \u001b[38;5;28mself\u001b[39m.tokenizer(sent_batch, is_split_into_words=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43memb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer >= \u001b[38;5;28mlen\u001b[39m(hidden):\n\u001b[32m     67\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpecified to take embeddings from layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but model has only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hidden)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m layers.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:859\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    853\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    857\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    873\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:610\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    606\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    608\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:548\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    546\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/pytorch_utils.py:251\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:556\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:471\u001b[39m, in \u001b[36mXLMRobertaIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DB_PATH = \"../data/duckdb/subs.duckdb\"\n",
    "\n",
    "run_opus_pipeline_simple(\n",
    "    db_path=DB_PATH,\n",
    "    block_size=50_000,\n",
    "    filter_use_window=False,            # prev/here/next context for SimAlign\n",
    "    filter_thresholds=dict(            # tune these if you like\n",
    "        min_sim_ok=None,               # None → length-adaptive threshold\n",
    "        length_mode=\"total\",\n",
    "        base_min_sim=0.30,\n",
    "        long_min_sim=0.70,\n",
    "        short_len=8,\n",
    "        long_len=28,\n",
    "    ),\n",
    "    apply_filter=True                  # set False to only write flags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6410e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
