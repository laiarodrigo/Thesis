{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28d46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import duckdb, pathlib\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8e85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pathlib, pandas as pd\n",
    "\n",
    "PROJECT_DB_PATH = pathlib.Path(\"../data/duckdb/subs_project.duckdb\")\n",
    "SOURCE_DB_PATH  = pathlib.Path(\"../data/duckdb/subs.duckdb\")\n",
    "\n",
    "PROJECT_DB_STR = PROJECT_DB_PATH.as_posix()\n",
    "SOURCE_DB_STR  = SOURCE_DB_PATH.as_posix()\n",
    "\n",
    "def connect_project(read_only: bool = True) -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Connect to subs_project.duckdb and ensure the 'src' catalog is attached.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(PROJECT_DB_STR, read_only=read_only)\n",
    "\n",
    "    # Attach src only if it's not there yet\n",
    "    dbl = con.execute(\"PRAGMA database_list\").df()\n",
    "    if not (dbl[\"name\"] == \"src\").any():\n",
    "        # read-only attach is fine since we don't want to modify src\n",
    "        con.execute(f\"ATTACH '{SOURCE_DB_STR}' AS src\")\n",
    "\n",
    "    return con\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa01ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"   # your chosen model\n",
    "SEED = 123\n",
    "\n",
    "RUN_MODE = \"local_debug\"   # change to \"cluster_full\" on HLT\n",
    "\n",
    "if RUN_MODE == \"local_debug\":\n",
    "    # short + tiny run just to make sure everything works\n",
    "    MAX_LENGTH = 512            # shorter context to save memory\n",
    "    OPUS_LIMIT = 50_000         # or even 10_000 if needed\n",
    "    PER_DEVICE_BATCH = 1\n",
    "    GRAD_ACCUM = 1\n",
    "    MAX_STEPS = 200             # tiny training run\n",
    "    USE_CPU = False             # set True if GPU still OOM and you just want to test\n",
    "else:  # \"cluster_full\"\n",
    "    MAX_LENGTH = 2048           # your real context length\n",
    "    OPUS_LIMIT = None           # full OPUS\n",
    "    PER_DEVICE_BATCH = 4        # per GPU\n",
    "    GRAD_ACCUM = 8              # effective batch = 32 sequences\n",
    "    MAX_STEPS = 8000            # or whatever you and your advisor want\n",
    "    USE_CPU = False             # we want GPUs on HLT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pathlib, pandas as pd\n",
    "from typing import Iterator, Dict, Optional\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 180)\n",
    "\n",
    "def stream_from_view(\n",
    "    view_name: str,\n",
    "    batch_size: int = 10_000,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Stream rows from train_data or test_data (views in subs_project.duckdb).\n",
    "    \"\"\"\n",
    "    con = connect_project(read_only=True)   # <= important change\n",
    "    offset = 0\n",
    "\n",
    "    base_query = f\"\"\"\n",
    "        SELECT\n",
    "          dataset, source, bucket, theme, label,\n",
    "          text_pt_br, text_pt_pt,\n",
    "          ref_pt_pt_manual, ref_pt_pt_deepl\n",
    "        FROM {view_name}\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        if limit is not None:\n",
    "            remaining = limit - offset\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            cur_batch = min(batch_size, remaining)\n",
    "        else:\n",
    "            cur_batch = batch_size\n",
    "\n",
    "        batch = con.execute(base_query + f\" LIMIT {cur_batch} OFFSET {offset}\").df()\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield row.to_dict()\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_training_rows_from_project(\n",
    "    train_limit: Optional[int] = None,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    print(\"Streaming rows from train_data...\")\n",
    "    yield from stream_from_view(\n",
    "        view_name=\"train_data\",\n",
    "        batch_size=batch_size,\n",
    "        limit=train_limit,\n",
    "    )\n",
    "\n",
    "def stream_test_rows_from_project(\n",
    "    test_limit: Optional[int] = None,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    print(\"Streaming rows from test_data...\")\n",
    "    yield from stream_from_view(\n",
    "        view_name=\"test_data\",\n",
    "        batch_size=batch_size,\n",
    "        limit=test_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def find_ptbrvarid_table(con: duckdb.DuckDBPyConnection) -> str:\n",
    "    \"\"\"\n",
    "    Try to find the PtBrVarId base table in subs.duckdb.\n",
    "    Adjust this if your table name is different.\n",
    "    \"\"\"\n",
    "    cand_df = con.execute(\"\"\"\n",
    "        SELECT table_name, table_type\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema='main' AND lower(table_name) LIKE 'ptbrvarid%'\n",
    "    \"\"\").df()\n",
    "    if cand_df.empty:\n",
    "        raise RuntimeError(\"Could not find a PtBrVarId table in subs.duckdb (name like 'ptbrvarid%').\")\n",
    "    base = cand_df[cand_df.table_type == \"BASE TABLE\"]\n",
    "    if not base.empty:\n",
    "        return base.table_name.iloc[0]\n",
    "    return cand_df.table_name.iloc[0]\n",
    "\n",
    "\n",
    "def stream_opus(\n",
    "    db_path: str,\n",
    "    batch_size: int = 10_000,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Iterator[Dict]:\n",
    "    con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    base_query = \"\"\"\n",
    "        SELECT\n",
    "            sent_pt_br AS text_pt_br,\n",
    "            sent_pt_pt AS text_pt_pt\n",
    "        FROM opus_moses_filtered\n",
    "    \"\"\"\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        if limit is not None:\n",
    "            remaining = limit - offset\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            cur_batch = min(batch_size, remaining)\n",
    "        else:\n",
    "            cur_batch = batch_size\n",
    "\n",
    "        batch = con.execute(\n",
    "            base_query + f\" LIMIT {cur_batch} OFFSET {offset}\"\n",
    "        ).df()\n",
    "\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield {\n",
    "                \"dataset\": \"OpenSubs\",\n",
    "                \"source\": \"opus_moses_filtered\",\n",
    "                \"bucket\": \"n/a\",\n",
    "                \"theme\": \"n/a\",\n",
    "                \"label\": None,\n",
    "                \"text_pt_br\": row[\"text_pt_br\"],\n",
    "                \"text_pt_pt\": row[\"text_pt_pt\"],\n",
    "                \"ref_pt_pt_manual\": None,\n",
    "                \"ref_pt_pt_deepl\": None,\n",
    "            }\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def stream_ptbrvarid_split(\n",
    "    db_path: str,\n",
    "    split_kinds=(\"train\", \"validation\"),\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Stream PtBrVarId rows for given splits from subs.duckdb.\n",
    "    - train instance: split_kinds = ('train', 'validation')\n",
    "    - test instance : split_kinds = ('test',)\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    ptbr_table = find_ptbrvarid_table(con)\n",
    "    print(f\"Using PtBrVarId table: {ptbr_table} | splits: {split_kinds}\")\n",
    "\n",
    "    split_list = \",\".join(f\"'{s}'\" for s in split_kinds)\n",
    "    base_query = f\"\"\"\n",
    "        SELECT\n",
    "            dataset,\n",
    "            domain,\n",
    "            split,\n",
    "            label,\n",
    "            text_pt_br,\n",
    "            text_pt_pt\n",
    "        FROM {ptbr_table}\n",
    "        WHERE dataset = 'PtBrVId'\n",
    "          AND lower(split) IN ({split_list})\n",
    "    \"\"\"\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        batch = con.execute(\n",
    "            base_query + f\" LIMIT {batch_size} OFFSET {offset}\"\n",
    "        ).df()\n",
    "\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield {\n",
    "                \"dataset\": \"PtBrVarId\",\n",
    "                \"source\": \"liaad/PtBrVId\",\n",
    "                \"bucket\": row.get(\"domain\", \"n/a\") or \"n/a\",\n",
    "                \"theme\": \"n/a\",\n",
    "                \"label\": row[\"label\"],       # 'pt-BR' or 'pt-PT'\n",
    "                \"text_pt_br\": row[\"text_pt_br\"],\n",
    "                \"text_pt_pt\": row[\"text_pt_pt\"],\n",
    "                \"ref_pt_pt_manual\": None,\n",
    "                \"ref_pt_pt_deepl\": None,\n",
    "            }\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_ptbrvarid_train(db_path: str, batch_size: int = 10_000) -> Iterator[Dict]:\n",
    "    return stream_ptbrvarid_split(db_path, split_kinds=(\"train\", \"validation\"), batch_size=batch_size)\n",
    "\n",
    "def stream_ptbrvarid_test(db_path: str, batch_size: int = 10_000) -> Iterator[Dict]:\n",
    "    return stream_ptbrvarid_split(db_path, split_kinds=(\"test\",), batch_size=batch_size)\n",
    "\n",
    "def stream_frmt_dev(\n",
    "    db_path: str,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    base_query = \"\"\"\n",
    "        SELECT bucket, text_pt_br, text_pt_pt\n",
    "        FROM frmt_dev\n",
    "    \"\"\"\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        batch = con.execute(\n",
    "            base_query + f\" LIMIT {batch_size} OFFSET {offset}\"\n",
    "        ).df()\n",
    "\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield {\n",
    "                \"dataset\": \"FRMT\",\n",
    "                \"source\": \"google-research/frmt\",\n",
    "                \"bucket\": row.get(\"bucket\", \"n/a\") or \"n/a\",\n",
    "                \"theme\": \"n/a\",\n",
    "                \"label\": None,\n",
    "                \"text_pt_br\": row[\"text_pt_br\"],\n",
    "                \"text_pt_pt\": row[\"text_pt_pt\"],\n",
    "                \"ref_pt_pt_manual\": None,\n",
    "                \"ref_pt_pt_deepl\": None,\n",
    "            }\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_frmt_test(\n",
    "    db_path: str,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    base_query = \"\"\"\n",
    "        SELECT bucket, text_pt_br, text_pt_pt\n",
    "        FROM frmt_test\n",
    "    \"\"\"\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        batch = con.execute(\n",
    "            base_query + f\" LIMIT {batch_size} OFFSET {offset}\"\n",
    "        ).df()\n",
    "\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield {\n",
    "                \"dataset\": \"FRMT\",\n",
    "                \"source\": \"google-research/frmt\",\n",
    "                \"bucket\": row.get(\"bucket\", \"n/a\") or \"n/a\",\n",
    "                \"theme\": \"n/a\",\n",
    "                \"label\": None,\n",
    "                \"text_pt_br\": row[\"text_pt_br\"],\n",
    "                \"text_pt_pt\": row[\"text_pt_pt\"],\n",
    "                \"ref_pt_pt_manual\": None,\n",
    "                \"ref_pt_pt_deepl\": None,\n",
    "            }\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_gold_test(\n",
    "    db_path: str,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    con = duckdb.connect(db_path, read_only=True)\n",
    "\n",
    "    base_query = \"\"\"\n",
    "        SELECT bucket, theme, text_pt_br, ref_pt_pt_manual, ref_pt_pt_deepl\n",
    "        FROM gold_test\n",
    "    \"\"\"\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        batch = con.execute(\n",
    "            base_query + f\" LIMIT {batch_size} OFFSET {offset}\"\n",
    "        ).df()\n",
    "\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield {\n",
    "                \"dataset\": \"Gold\",\n",
    "                \"source\": \"joaosanches/golden_collection\",\n",
    "                \"bucket\": row.get(\"bucket\", \"n/a\") or \"n/a\",\n",
    "                \"theme\": row.get(\"theme\", \"n/a\") or \"n/a\",\n",
    "                \"label\": None,\n",
    "                \"text_pt_br\": row[\"text_pt_br\"],              # pt-BR\n",
    "                \"text_pt_pt\": row[\"ref_pt_pt_manual\"],        # we treat manual EP as pt-PT text\n",
    "                \"ref_pt_pt_manual\": row[\"ref_pt_pt_manual\"],\n",
    "                \"ref_pt_pt_deepl\": row[\"ref_pt_pt_deepl\"],\n",
    "            }\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_training_rows(\n",
    "    opus_limit: Optional[int] = None,\n",
    "    opus_batch: int = 10_000,\n",
    "    ptbr_batch: int = 10_000,\n",
    "    frmt_batch: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Train instance:\n",
    "      - OPUS (translation + classification)\n",
    "      - PtBrVarId train+validation (classification only, since no parallel)\n",
    "      - FRMT dev (translation + classification when both sides exist)\n",
    "    \"\"\"\n",
    "    src_path = SOURCE_DB_PATH.as_posix()\n",
    "    proj_path = PROJECT_DB_PATH.as_posix()\n",
    "\n",
    "    print(\"Streaming OPUS/OpenSubs (train)...\")\n",
    "    for row in stream_opus(src_path, batch_size=opus_batch, limit=opus_limit):\n",
    "        yield row\n",
    "\n",
    "    print(\"Streaming PtBrVarId train+val (train)...\")\n",
    "    for row in stream_ptbrvarid_train(src_path, batch_size=ptbr_batch):\n",
    "        yield row\n",
    "\n",
    "    print(\"Streaming FRMT dev (train)...\")\n",
    "    for row in stream_frmt_dev(proj_path, batch_size=frmt_batch):\n",
    "        yield row\n",
    "\n",
    "\n",
    "def stream_test_rows(\n",
    "    ptbr_batch: int = 10_000,\n",
    "    frmt_batch: int = 10_000,\n",
    "    gold_batch: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Test instance:\n",
    "      - PtBrVarId test\n",
    "      - FRMT test\n",
    "      - Gold Collection\n",
    "    \"\"\"\n",
    "    src_path = SOURCE_DB_PATH.as_posix()\n",
    "    proj_path = PROJECT_DB_PATH.as_posix()\n",
    "\n",
    "    print(\"Streaming PtBrVarId test (eval)...\")\n",
    "    for row in stream_ptbrvarid_test(src_path, batch_size=ptbr_batch):\n",
    "        yield row\n",
    "\n",
    "    print(\"Streaming FRMT test (eval)...\")\n",
    "    for row in stream_frmt_test(proj_path, batch_size=frmt_batch):\n",
    "        yield row\n",
    "\n",
    "    print(\"Streaming Gold Collection (eval)...\")\n",
    "    for row in stream_gold_test(proj_path, batch_size=gold_batch):\n",
    "        yield row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6343eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming rows from test_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9522592474e34e378a5f8e8c9bfb85a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import IterableDataset, Dataset\n",
    "\n",
    "def duckdb_row_generator():\n",
    "    # train_limit can be your old OPUS_LIMIT-like knob; now it's \"max training rows\"\n",
    "    yield from stream_training_rows_from_project(\n",
    "        train_limit=OPUS_LIMIT,    # OPUS_LIMIT now means \"limit total rows\" (or None for full)\n",
    "        batch_size=10_000,\n",
    "    )\n",
    "\n",
    "raw_train = IterableDataset.from_generator(duckdb_row_generator)\n",
    "\n",
    "# Eval set: small, in memory\n",
    "eval_rows = []\n",
    "for i, row in enumerate(stream_test_rows_from_project(test_limit=10_000, batch_size=10_000)):\n",
    "    eval_rows.append(row)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "eval_ds = Dataset.from_pandas(eval_df, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d6bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "SEED = 123\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and not USE_CPU else torch.float32,\n",
    "    device_map=\"auto\" if not USE_CPU else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "if USE_CPU:\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "SYSTEM_TRANSLATION = (\n",
    "    \"És um assistente especialista em português europeu e português do Brasil. \"\n",
    "    \"A tua tarefa é converter frases entre as duas variantes, mantendo o significado, o registo e um estilo natural.\"\n",
    ")\n",
    "\n",
    "SYSTEM_CLASSIFICATION = (\n",
    "    \"És um linguista especializado em português europeu e português do Brasil. \"\n",
    "    \"A tua tarefa é identificar a variante de português de uma frase.\"\n",
    ")\n",
    "\n",
    "USER_TRANSL_BR2PT = (\n",
    "    \"Converte o seguinte texto de português do Brasil para português europeu, \"\n",
    "    \"mantendo o sentido e soando natural em português europeu.\\n\\n\"\n",
    "    \"Texto: {source}\"\n",
    ")\n",
    "\n",
    "USER_TRANSL_PT2BR = (\n",
    "    \"Converte o seguinte texto de português europeu para português do Brasil, \"\n",
    "    \"mantendo o sentido e soando natural em português do Brasil.\\n\\n\"\n",
    "    \"Texto: {source}\"\n",
    ")\n",
    "\n",
    "USER_CLASSIFICATION = (\n",
    "    \"Qual é a variante de português desta frase? \"\n",
    "    \"Responde apenas com \\\"Português do Brasil\\\" ou \\\"Português Europeu\\\".\\n\\n\"\n",
    "    \"Frase: {source}\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_examples_from_row(row: Dict):\n",
    "    \"\"\"\n",
    "    Return a *list* of examples for this row.\n",
    "    Each example is a dict: {\"task\": ..., \"source\": ..., \"target\": ...}\n",
    "\n",
    "    - OPUS (dataset == 'OpenSubs'):\n",
    "        * Always has both texts -> translation + classification for each side.\n",
    "    - PtBrVarId:\n",
    "        * Only classification, using the explicit 'label' column.\n",
    "        * No translation (no rows with two texts).\n",
    "    - FRMT:\n",
    "        * If both text_pt_br and text_pt_pt exist -> translation + classification (both sides).\n",
    "        * Otherwise -> classification only for whichever side exists.\n",
    "    - Gold:\n",
    "        * We treat text_pt_br as pt-BR, ref_pt_pt_manual as pt-PT.\n",
    "        * If both exist -> translation (BR->PT) + classification for both sides.\n",
    "        * Otherwise -> classification only for available side(s).\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "\n",
    "    dataset = row[\"dataset\"]\n",
    "    src_br  = row[\"text_pt_br\"]\n",
    "    src_pt  = row[\"text_pt_pt\"]   # for Gold, this is ref_pt_pt_manual\n",
    "    label   = row[\"label\"]\n",
    "\n",
    "    # --- 1) OPUS: translation + classification from both columns ---\n",
    "    if dataset == \"OpenSubs\":\n",
    "        if src_br and src_pt:\n",
    "            # translation\n",
    "            examples.append({\n",
    "                \"task\": \"translate_br2pt\",\n",
    "                \"source\": src_br,\n",
    "                \"target\": src_pt,\n",
    "            })\n",
    "            # classification BR\n",
    "            examples.append({\n",
    "                \"task\": \"classify\",\n",
    "                \"source\": src_br,\n",
    "                \"target\": \"pt-BR\",\n",
    "            })\n",
    "            # classification PT\n",
    "            examples.append({\n",
    "                \"task\": \"classify\",\n",
    "                \"source\": src_pt,\n",
    "                \"target\": \"pt-PT\",\n",
    "            })\n",
    "        return examples\n",
    "\n",
    "    # --- 2) PtBrVarId: classification only ---\n",
    "    if dataset == \"PtBrVarId\":\n",
    "        if label in (\"pt-BR\", \"pt-PT\"):\n",
    "            # choose whichever text exists (you said only one per row)\n",
    "            text = src_br if src_br else src_pt\n",
    "            if text:\n",
    "                examples.append({\n",
    "                    \"task\": \"classify\",\n",
    "                    \"source\": text,\n",
    "                    \"target\": label,\n",
    "                })\n",
    "        return examples\n",
    "\n",
    "    # --- 3) FRMT / Gold / others: generic rule ---\n",
    "\n",
    "    has_br = bool(src_br)\n",
    "    has_pt = bool(src_pt)\n",
    "\n",
    "    # a) translation if both sides exist\n",
    "    if has_br and has_pt:\n",
    "        examples.append({\n",
    "            \"task\": \"translate_br2pt\",\n",
    "            \"source\": src_br,\n",
    "            \"target\": src_pt,\n",
    "        })\n",
    "\n",
    "    # b) classification for all available texts\n",
    "    if has_br:\n",
    "        examples.append({\n",
    "            \"task\": \"classify\",\n",
    "            \"source\": src_br,\n",
    "            \"target\": \"pt-BR\",\n",
    "        })\n",
    "    if has_pt:\n",
    "        examples.append({\n",
    "            \"task\": \"classify\",\n",
    "            \"source\": src_pt,\n",
    "            \"target\": \"pt-PT\",\n",
    "        })\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    examples = []\n",
    "    for i in range(len(batch[\"dataset\"])):\n",
    "        row = {k: batch[k][i] for k in batch}\n",
    "        row_examples = build_examples_from_row(row)  # or build_example_dict(s)\n",
    "        examples.extend(row_examples)\n",
    "\n",
    "    if not examples:\n",
    "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    texts = []\n",
    "    for ex in examples:\n",
    "        task   = ex[\"task\"]\n",
    "        source = ex[\"source\"]\n",
    "        target = ex[\"target\"]\n",
    "\n",
    "        if task == \"translate_br2pt\":\n",
    "            system_prompt = SYSTEM_TRANSLATION\n",
    "            user_content  = USER_TRANSL_BR2PT.format(source=source)\n",
    "        elif task == \"translate_pt2br\":\n",
    "            system_prompt = SYSTEM_TRANSLATION\n",
    "            user_content  = USER_TRANSL_PT2BR.format(source=source)\n",
    "        elif task == \"classify\":\n",
    "            system_prompt = SYSTEM_CLASSIFICATION\n",
    "            user_content  = USER_CLASSIFICATION.format(source=source)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\",    \"content\": system_prompt},\n",
    "            {\"role\": \"user\",      \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": target},\n",
    "        ]\n",
    "        text = tok.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    tokenized = tok(\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",   # or padding=\"longest\", but max_length is simpler\n",
    "    )\n",
    "\n",
    "    # Labels = input_ids, with padding kept (simple, works fine)\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    # (optional but nicer) mask pad positions so they don't contribute to loss:\n",
    "    pad_id = tok.pad_token_id\n",
    "    labels = [\n",
    "        [(tid if tid != pad_id else -100) for tid in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a015fed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515dbd3f22e8482ca0321a5a6e8f8af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = raw_train.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"dataset\", \"source\", \"bucket\", \"theme\",\n",
    "        \"label\", \"text_pt_br\", \"text_pt_pt\",\n",
    "        \"ref_pt_pt_manual\", \"ref_pt_pt_deepl\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "eval_ds = Dataset.from_pandas(eval_df, preserve_index=False)\n",
    "\n",
    "eval_tokenized = eval_ds.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"dataset\", \"source\", \"bucket\", \"theme\",\n",
    "        \"label\", \"text_pt_br\", \"text_pt_pt\",\n",
    "        \"ref_pt_pt_manual\", \"ref_pt_pt_deepl\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7748a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 616,235,008 || trainable%: 3.2756\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf971d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming OPUS/OpenSubs (train)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m      9\u001b[39m training_args = TrainingArguments(\n\u001b[32m     10\u001b[39m     output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./qwen3-0_6b-ptbr-ptpt-lora-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_MODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     remove_unused_columns=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m trainer = Trainer(\n\u001b[32m     43\u001b[39m     model=model,\n\u001b[32m     44\u001b[39m     args=training_args,\n\u001b[32m     45\u001b[39m     train_dataset=train_tokenized,\n\u001b[32m     46\u001b[39m     eval_dataset=eval_tokenized,\n\u001b[32m     47\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m trainer.evaluate()\n\u001b[32m     52\u001b[39m trainer.save_model(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./qwen3-0_6b-ptbr-ptpt-lora-adapter-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_MODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/accelerate/data_loader.py:866\u001b[39m, in \u001b[36mDataLoaderDispatcher.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28mself\u001b[39m._stop_iteration = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    865\u001b[39m first_batch = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m next_batch, next_batch_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    867\u001b[39m batch_index = \u001b[32m0\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_iteration:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/accelerate/data_loader.py:820\u001b[39m, in \u001b[36mDataLoaderDispatcher._fetch_batches\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.state.num_processes):\n\u001b[32m    819\u001b[39m         \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         batches.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    822\u001b[39m     batch = concatenate(batches, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         data.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:2538\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2535\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2538\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:1252\u001b[39m, in \u001b[36mMappedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter.format_row(pa_table)\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:1431\u001b[39m, in \u001b[36mMappedExamplesIterable._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m   1426\u001b[39m     outputs = (\n\u001b[32m   1427\u001b[39m         (key, transformed_example)\n\u001b[32m   1428\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, transformed_batch \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[32m   1429\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(transformed_batch)\n\u001b[32m   1430\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_examples_since_previous_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:1426\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1424\u001b[39m outputs = iter_outputs()\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m-> \u001b[39m\u001b[32m1426\u001b[39m     outputs = \u001b[43m(\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1428\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, transformed_example \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m   1432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:1416\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_outputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m   1415\u001b[39m         \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state_example_idx\u001b[39m\u001b[33m\"\u001b[39m] = current_idx\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict:\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/datasets/iterable_dataset.py:1346\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.apply_function\u001b[39m\u001b[34m(key_example, indices)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   1345\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(key_example, indices)\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m processed_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(key_example, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mpreprocess_batch\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    180\u001b[39m     text = tok.apply_chat_template(\n\u001b[32m    181\u001b[39m         messages,\n\u001b[32m    182\u001b[39m         tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    183\u001b[39m         add_generation_prompt=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m     texts.append(text)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m tokenized = \u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# or padding=\"longest\", but max_length is simpler\u001b[39;49;00m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Labels = input_ids, with padding kept (simple, works fine)\u001b[39;00m\n\u001b[32m    195\u001b[39m labels = tokenized[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2938\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2936\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2937\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2938\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2939\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2940\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3026\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3021\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3022\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3023\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3024\u001b[39m         )\n\u001b[32m   3025\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3044\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3045\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3048\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   3049\u001b[39m         text=text,\n\u001b[32m   3050\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3068\u001b[39m         **kwargs,\n\u001b[32m   3069\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3227\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3217\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3218\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3219\u001b[39m     padding=padding,\n\u001b[32m   3220\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3224\u001b[39m     **kwargs,\n\u001b[32m   3225\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3229\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3245\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:566\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    553\u001b[39m encodings = \u001b[38;5;28mself\u001b[39m._tokenizer.encode_batch(\n\u001b[32m    554\u001b[39m     batch_text_or_text_pairs,\n\u001b[32m    555\u001b[39m     add_special_tokens=add_special_tokens,\n\u001b[32m    556\u001b[39m     is_pretokenized=is_split_into_words,\n\u001b[32m    557\u001b[39m )\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    565\u001b[39m tokens_and_encodings = [\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    577\u001b[39m ]\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[32m    585\u001b[39m sanitized_tokens = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Thesis/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:349\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._convert_encoding\u001b[39m\u001b[34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[39m\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    348\u001b[39m         encoding_dict[\u001b[33m\"\u001b[39m\u001b[33moffset_mapping\u001b[39m\u001b[33m\"\u001b[39m].append(e.offsets)\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_length:\n\u001b[32m    350\u001b[39m         encoding_dict[\u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mlen\u001b[39m(e.ids))\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoding_dict, encodings\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./qwen3-0_6b-ptbr-ptpt-lora-{RUN_MODE}\",\n",
    "\n",
    "    # core training\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    max_steps=MAX_STEPS,\n",
    "\n",
    "    # logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    # evaluation\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "\n",
    "    # saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "\n",
    "    # precision & device\n",
    "    bf16=torch.cuda.is_available() and not USE_CPU,\n",
    "    fp16=False,\n",
    "    use_cpu=USE_CPU,\n",
    "\n",
    "    # important for our custom preprocess_batch\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.save_model(f\"./qwen3-0_6b-ptbr-ptpt-lora-adapter-{RUN_MODE}\")\n",
    "tok.save_pretrained(f\"./qwen3-0_6b-ptbr-ptpt-lora-adapter-tokenizer-{RUN_MODE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d4118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
