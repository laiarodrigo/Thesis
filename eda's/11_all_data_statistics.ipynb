{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917517e1",
   "metadata": {},
   "source": [
    "**// IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc4e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pathlib, pandas as pd, re, numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "PROJECT_DB_PATH = pathlib.Path(\"../data/duckdb/subs_project.duckdb\")\n",
    "SOURCE_DB_PATH  = pathlib.Path(\"../data/duckdb/subs.duckdb\")\n",
    "\n",
    "PROJECT_DB_STR = PROJECT_DB_PATH.as_posix()\n",
    "SOURCE_DB_STR  = SOURCE_DB_PATH.as_posix()\n",
    "\n",
    "def connect_project(read_only: bool = True) -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Connect to subs_project.duckdb and ensure the 'src' catalog is attached.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(PROJECT_DB_STR, read_only=read_only)\n",
    "\n",
    "    # Attach src only if it's not there yet\n",
    "    dbl = con.execute(\"PRAGMA database_list\").df()\n",
    "    if not (dbl[\"name\"] == \"src\").any():\n",
    "        # read-only attach is fine since we don't want to modify src\n",
    "        con.execute(f\"ATTACH '{SOURCE_DB_STR}' AS src\")\n",
    "\n",
    "    return con\n",
    "\n",
    "TRAIN_TABLE  = 'train_data'      # <- use the VIEW directly\n",
    "TEST_TABLE   = 'test_data'       # <- use the VIEW directly\n",
    "\n",
    "N_RATIO_SAMPLE   = 500_000\n",
    "N_SCATTER_SAMPLE = 50_000\n",
    "N_TEXT_SAMPLE    = 10_000\n",
    "SAMPLES = 1000\n",
    "\n",
    "con = connect_project(read_only=True)\n",
    "\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "con.execute(\"PRAGMA preserve_insertion_order=false;\")\n",
    "# optionally, to avoid DuckDB grabbing too much RAM:\n",
    "# con.execute(\"PRAGMA memory_limit='3GB';\")\n",
    "\n",
    "def both_sides(where_extra: str = \"\") -> str:\n",
    "    extra = f\" AND {where_extra}\" if where_extra else \"\"\n",
    "    return f\"\"\"\n",
    "      SELECT text_pt_br, text_pt_pt FROM {TRAIN_TABLE}\n",
    "      WHERE text_pt_br IS NOT NULL AND text_pt_pt IS NOT NULL{extra}\n",
    "      UNION ALL\n",
    "      SELECT text_pt_br, text_pt_pt FROM {TEST_TABLE}\n",
    "      WHERE text_pt_br IS NOT NULL AND text_pt_pt IS NOT NULL{extra}\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26642730",
   "metadata": {},
   "source": [
    "**// CONFIGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0090b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pathlib, pandas as pd\n",
    "from typing import Iterator, Dict, Optional\n",
    "\n",
    "def stream_from_view(\n",
    "    view_name: str,\n",
    "    batch_size: int = 10_000,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Stream rows from train_data or test_data (views in subs_project.duckdb).\n",
    "    \"\"\"\n",
    "    con = connect_project(read_only=True)   # <= important change\n",
    "    offset = 0\n",
    "\n",
    "    base_query = f\"\"\"\n",
    "        SELECT\n",
    "          dataset, source, bucket, theme, label,\n",
    "          text_pt_br, text_pt_pt,\n",
    "          ref_pt_pt_manual, ref_pt_pt_deepl\n",
    "        FROM {view_name}\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        if limit is not None:\n",
    "            remaining = limit - offset\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            cur_batch = min(batch_size, remaining)\n",
    "        else:\n",
    "            cur_batch = batch_size\n",
    "\n",
    "        batch = con.execute(base_query + f\" LIMIT {cur_batch} OFFSET {offset}\").df()\n",
    "        if batch.empty:\n",
    "            break\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            yield row.to_dict()\n",
    "\n",
    "        offset += len(batch)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def stream_training_rows_from_project(\n",
    "    train_limit: Optional[int] = None,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    print(\"Streaming rows from train_data...\")\n",
    "    yield from stream_from_view(\n",
    "        view_name=\"train_data\",\n",
    "        batch_size=batch_size,\n",
    "        limit=train_limit,\n",
    "    )\n",
    "\n",
    "def stream_test_rows_from_project(\n",
    "    test_limit: Optional[int] = None,\n",
    "    batch_size: int = 10_000,\n",
    ") -> Iterator[Dict]:\n",
    "    print(\"Streaming rows from test_data...\")\n",
    "    yield from stream_from_view(\n",
    "        view_name=\"test_data\",\n",
    "        batch_size=batch_size,\n",
    "        limit=test_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "# # -------- Per-dataset counts (do each split separately to avoid huge unions) --------\n",
    "# train_ds = con.execute(f\"\"\"\n",
    "#     SELECT dataset, COUNT(*) AS n\n",
    "#     FROM {TRAIN_TABLE}\n",
    "#     GROUP BY dataset\n",
    "# \"\"\").fetchdf()\n",
    "\n",
    "# test_ds = con.execute(f\"\"\"\n",
    "#     SELECT dataset, COUNT(*) AS n\n",
    "#     FROM {TEST_TABLE}\n",
    "#     GROUP BY dataset\n",
    "# \"\"\").fetchdf()\n",
    "\n",
    "# per_dataset = (\n",
    "#     pd.concat([train_ds.assign(split=\"train\"), test_ds.assign(split=\"test\")], ignore_index=True)\n",
    "#       .groupby(\"dataset\", as_index=False)[\"n\"].sum()\n",
    "#       .sort_values(\"n\", ascending=False)\n",
    "# )\n",
    "\n",
    "# # -------- Totals & diagnostics (per split, then sum in pandas) --------\n",
    "# train_tot = con.execute(f\"\"\"\n",
    "#     SELECT\n",
    "#       COUNT(*) AS total_rows,\n",
    "#       SUM(dataset IS NULL) AS null_dataset,\n",
    "#       SUM(COALESCE(TRIM(text_pt_br),'')='' AND COALESCE(TRIM(text_pt_pt),'')='') AS both_texts_empty_or_null\n",
    "#     FROM {TRAIN_TABLE}\n",
    "# \"\"\").fetchdf().iloc[0]\n",
    "\n",
    "# test_tot = con.execute(f\"\"\"\n",
    "#     SELECT\n",
    "#       COUNT(*) AS total_rows,\n",
    "#       SUM(dataset IS NULL) AS null_dataset,\n",
    "#       SUM(COALESCE(TRIM(text_pt_br),'')='' AND COALESCE(TRIM(text_pt_pt),'')='') AS both_texts_empty_or_null\n",
    "#     FROM {TEST_TABLE}\n",
    "# \"\"\").fetchdf().iloc[0]\n",
    "\n",
    "# totals = pd.DataFrame([{\n",
    "#     \"total_rows\": int(train_tot[\"total_rows\"] + test_tot[\"total_rows\"]),\n",
    "#     \"rows_with_null_dataset\": int(train_tot[\"null_dataset\"] + test_tot[\"null_dataset\"]),\n",
    "#     \"rows_with_both_texts_empty_or_null\": int(train_tot[\"both_texts_empty_or_null\"] + test_tot[\"both_texts_empty_or_null\"]),\n",
    "# }])\n",
    "\n",
    "# per_split = pd.DataFrame([\n",
    "#     {\"split\":\"train\",\n",
    "#      \"total_rows\": int(train_tot[\"total_rows\"]),\n",
    "#      \"rows_with_null_dataset\": int(train_tot[\"null_dataset\"]),\n",
    "#      \"rows_with_both_texts_empty_or_null\": int(train_tot[\"both_texts_empty_or_null\"])},\n",
    "#     {\"split\":\"test\",\n",
    "#      \"total_rows\": int(test_tot[\"total_rows\"]),\n",
    "#      \"rows_with_null_dataset\": int(test_tot[\"null_dataset\"]),\n",
    "#      \"rows_with_both_texts_empty_or_null\": int(test_tot[\"both_texts_empty_or_null\"])},\n",
    "# ])\n",
    "\n",
    "# print(\"=== Rows per dataset (combined) ===\"); display(per_dataset)\n",
    "# print(\"\\n=== Totals & null/empty diagnostics (combined) ===\"); display(totals)\n",
    "# print(\"\\n=== Per-split diagnostics (optional) ===\"); display(per_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b4205",
   "metadata": {},
   "source": [
    "**// MAIN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415fd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(con.execute(\"SELECT COUNT(*) FROM train_data\").fetchone()[0])\n",
    "# print(con.execute(\"SELECT COUNT(*) FROM test_data\").fetchone()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cbce04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming rows from train_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e006da32054cc995e938e6f8b8b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming rows from test_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd3e176c19141d39ed449eaa4d60626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Counter({'total_rows': 1000}),\n",
       " 'test': Counter({'total_rows': 1000,\n",
       "          'null_br': 817,\n",
       "          'empty_br': 817,\n",
       "          'null_pt': 183,\n",
       "          'empty_pt': 183})}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_basic_counts_from_view_stream(\n",
    "    train_limit=None,\n",
    "    test_limit=None,\n",
    "):\n",
    "    stats = {\n",
    "        \"train\": Counter(),\n",
    "        \"test\": Counter(),\n",
    "    }\n",
    "\n",
    "    # TRAIN\n",
    "    n_seen = 0\n",
    "    for row in stream_training_rows_from_project(\n",
    "        train_limit=train_limit,\n",
    "        batch_size=10_000,\n",
    "    ):\n",
    "        split = \"train\"\n",
    "        br = row[\"text_pt_br\"]\n",
    "        pt = row[\"text_pt_pt\"]\n",
    "\n",
    "        stats[split][\"total_rows\"] += 1\n",
    "        if br is None:\n",
    "            stats[split][\"null_br\"] += 1\n",
    "        if pt is None:\n",
    "            stats[split][\"null_pt\"] += 1\n",
    "\n",
    "        if (br or \"\").strip() == \"\":\n",
    "            stats[split][\"empty_br\"] += 1\n",
    "        if (pt or \"\").strip() == \"\":\n",
    "            stats[split][\"empty_pt\"] += 1\n",
    "\n",
    "        if br is None and pt is None:\n",
    "            stats[split][\"both_null\"] += 1\n",
    "\n",
    "        if (br is None or str(br).strip() == \"\") and (pt is None or str(pt).strip() == \"\"):\n",
    "            stats[split][\"both_empty_or_null\"] += 1\n",
    "\n",
    "        n_seen += 1\n",
    "        if train_limit is not None and n_seen >= train_limit:\n",
    "            break\n",
    "\n",
    "    # TEST\n",
    "    n_seen = 0\n",
    "    for row in stream_test_rows_from_project(\n",
    "        test_limit=test_limit,\n",
    "        batch_size=10_000,\n",
    "    ):\n",
    "        split = \"test\"\n",
    "        br = row[\"text_pt_br\"]\n",
    "        pt = row[\"text_pt_pt\"]\n",
    "\n",
    "        stats[split][\"total_rows\"] += 1\n",
    "        if br is None:\n",
    "            stats[split][\"null_br\"] += 1\n",
    "        if pt is None:\n",
    "            stats[split][\"null_pt\"] += 1\n",
    "\n",
    "        if (br or \"\").strip() == \"\":\n",
    "            stats[split][\"empty_br\"] += 1\n",
    "        if (pt or \"\").strip() == \"\":\n",
    "            stats[split][\"empty_pt\"] += 1\n",
    "\n",
    "        if br is None and pt is None:\n",
    "            stats[split][\"both_null\"] += 1\n",
    "\n",
    "        if (br is None or str(br).strip() == \"\") and (pt is None or str(pt).strip() == \"\"):\n",
    "            stats[split][\"both_empty_or_null\"] += 1\n",
    "\n",
    "        n_seen += 1\n",
    "        if test_limit is not None and n_seen >= test_limit:\n",
    "            break\n",
    "\n",
    "    return stats\n",
    "\n",
    "stats = compute_basic_counts_from_view_stream(train_limit=SAMPLES, test_limit=SAMPLES)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming rows from train_data...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    " \n",
    "def reservoir_sample_pairs(\n",
    "    n_target: int = 100_000,\n",
    "    include_test: bool = True,\n",
    ") -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Uniform sample of sentence pairs (br, pt) from train_data (+ optionally test_data).\n",
    "    Only keeps rows where both sides are non-empty.\n",
    "    \"\"\"\n",
    "    reservoir: list[tuple[str, str]] = []\n",
    "    t = 0  # number of eligible pairs seen so far\n",
    "\n",
    "    # TRAIN\n",
    "    for row in stream_training_rows_from_project(\n",
    "        train_limit=None,\n",
    "        batch_size=10_000,\n",
    "    ):\n",
    "        br = row[\"text_pt_br\"]\n",
    "        pt = row[\"text_pt_pt\"]\n",
    "        if not br or not pt:\n",
    "            continue\n",
    "        t += 1\n",
    "        pair = (br, pt)\n",
    "        if len(reservoir) < n_target:\n",
    "            reservoir.append(pair)\n",
    "        else:\n",
    "            j = random.randint(0, t - 1)\n",
    "            if j < n_target:\n",
    "                reservoir[j] = pair\n",
    "\n",
    "    # TEST (optional)\n",
    "    if include_test:\n",
    "        for row in stream_test_rows_from_project(\n",
    "            test_limit=None,\n",
    "            batch_size=10_000,\n",
    "        ):\n",
    "            br = row[\"text_pt_br\"]\n",
    "            pt = row[\"text_pt_pt\"]\n",
    "            if not br or not pt:\n",
    "                continue\n",
    "            t += 1\n",
    "            pair = (br, pt)\n",
    "            if len(reservoir) < n_target:\n",
    "                reservoir.append(pair)\n",
    "            else:\n",
    "                j = random.randint(0, t - 1)\n",
    "                if j < n_target:\n",
    "                    reservoir[j] = pair\n",
    "\n",
    "    print(f\"Reservoir sample size: {len(reservoir)} from {t} eligible pairs\")\n",
    "    return reservoir\n",
    "\n",
    "N_SAMPLE_PAIRS = 100_000  # you can reduce if it's slow\n",
    "pairs = reservoir_sample_pairs(N_SAMPLE_PAIRS, include_test=True)\n",
    "\n",
    "# Split into side lists\n",
    "br_texts = [p[0] for p in pairs]\n",
    "pt_texts = [p[1] for p in pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d77766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "def count_qwen_tokens(texts):\n",
    "    enc = tok(\n",
    "        texts,\n",
    "        add_special_tokens=False,\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "    )\n",
    "    return [len(ids) for ids in enc[\"input_ids\"]]\n",
    "\n",
    "br_token_lens = count_qwen_tokens(br_texts)\n",
    "pt_token_lens = count_qwen_tokens(pt_texts)\n",
    "\n",
    "def summarize_lengths(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    return {\n",
    "        \"side\": name,\n",
    "        \"n\": int(arr.size),\n",
    "        \"mean\": float(arr.mean()),\n",
    "        \"median\": float(np.median(arr)),\n",
    "        \"p90\": float(np.quantile(arr, 0.90)),\n",
    "        \"p95\": float(np.quantile(arr, 0.95)),\n",
    "        \"p99\": float(np.quantile(arr, 0.99)),\n",
    "        \"max\": int(arr.max()),\n",
    "    }\n",
    "\n",
    "qwen_token_stats = pd.DataFrame([\n",
    "    summarize_lengths(\"pt_br\", br_token_lens),\n",
    "    summarize_lengths(\"pt_pt\", pt_token_lens),\n",
    "])\n",
    "\n",
    "print(\"=== Qwen-token stats on raw sentences (sample) ===\")\n",
    "display(qwen_token_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf2c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LENGTH STATS (chars) — full dataset via SQL ===\n"
     ]
    }
   ],
   "source": [
    "len_br = np.array([len(s) for s in br_texts])\n",
    "len_pt = np.array([len(s) for s in pt_texts])\n",
    "\n",
    "length_stats = pd.DataFrame([\n",
    "    summarize_lengths(\"len_br_chars\", len_br),\n",
    "    summarize_lengths(\"len_pt_chars\", len_pt),\n",
    "])\n",
    "\n",
    "print(\"\\n=== LENGTH STATS (chars) — sampled pairs ===\")\n",
    "display(length_stats)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(len_br, bins=100)\n",
    "plt.title(\"PT-BR length distribution (sampled train+test)\")\n",
    "plt.xlabel(\"Characters\"); plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
