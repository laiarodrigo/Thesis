study:
  name: encdec_translation_lora
  storage: sqlite:///hpo/encoder_decoder/translation_optuna.db
  direction: minimize
  n_trials: 16
  seed: 123
  metric: eval_loss

base_config: configs/encoder_decoder/translation_lora.yaml
runner_script: scripts/encoder_decoder/train_encdec_lora.py
output_root: outputs/hpo/encoder_decoder/translation
generated_config_dir: hpo/encoder_decoder/generated_translation

fixed_overrides:
  training.logging_steps: 50
  training.eval_strategy: steps
  training.eval_steps: 1000
  training.save_steps: 1000
  training.save_total_limit: 1
  training.early_stopping_patience: 6
  training.load_best_model_at_end: true
  training.metric_for_best_model: eval_loss
  training.greater_is_better: false
  training.do_eval: true

search_space:
  training.learning_rate:
    type: float
    low: 3.0e-5
    high: 3.0e-4
    log: true
  lora.r:
    type: categorical
    choices: [8, 16, 32]
  lora.alpha:
    type: categorical
    choices: [16, 32, 64]
  lora.dropout:
    type: categorical
    choices: [0.0, 0.05, 0.1]
  training.per_device_train_batch_size:
    type: categorical
    choices: [4, 8, 16]
  training.gradient_accumulation_steps:
    type: categorical
    choices: [2, 4, 8]
  training.warmup_steps:
    type: categorical
    choices: [100, 200, 500]
  training.max_steps:
    type: categorical
    choices: [20000, 40000, 60000]
