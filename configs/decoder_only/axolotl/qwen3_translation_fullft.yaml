base_model: Qwen/Qwen3-0.6B
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true
chat_template: tokenizer_default

load_in_8bit: false
load_in_4bit: false

datasets:
  - path: data/decoder_only/qwen3_translation/train.jsonl
    ds_type: json
    type: chat_template
    field_messages: messages
    roles_to_train:
      - assistant

test_datasets:
  - path: data/decoder_only/qwen3_translation/valid.jsonl
    ds_type: json
    type: chat_template
    field_messages: messages
    roles_to_train:
      - assistant

dataset_prepared_path: data/decoder_only/qwen3_translation/prepared_fullft
output_dir: outputs/decoder_only/qwen3_translation/fullft

sequence_len: 128
sample_packing: false
pad_to_sequence_len: true

micro_batch_size: 1
gradient_accumulation_steps: 80
learning_rate: 2.0e-5
weight_decay: 0.01
warmup_steps: 200
max_steps: 3000
eval_batch_size: 1
eval_on_start: false

gradient_checkpointing: true
bf16: true
fp16: false
tf32: false
eval_causal_lm_metrics: []

logging_steps: 50
eval_steps: 1000000
save_steps: 200
save_total_limit: 2

load_best_model_at_end: false
metric_for_best_model: eval_loss
greater_is_better: false
