seed: 123

model:
  # Replace with your target encoder-decoder model, e.g. T5Gemma2.
  base_model: google/flan-t5-base
  trust_remote_code: true
  max_source_length: 512
  max_target_length: 128

dataset:
  train_path: data/encoder_decoder/translation_train.jsonl
  valid_path: data/encoder_decoder/translation_valid.jsonl

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: none
  target_modules:
    - q
    - v

training:
  output_dir: outputs/encoder_decoder/flan_t5_base_translation_lora
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 200
  max_steps: 100000
  logging_steps: 50
  eval_strategy: steps
  eval_steps: 2000
  save_steps: 5000
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  do_eval: true
  bf16: true
  fp16: false
  gradient_checkpointing: true
  dataloader_num_workers: 8
